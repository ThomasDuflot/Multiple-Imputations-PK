library(lixoftConnectors)
initializeLixoftConnectors(software = "monolix")
library(mrgsolve)
library(MASS) 
library(dplyr) 
library(ggplot2)
library(missForest)
library(rMIDAS)
library(dplyr)
library(mice)
library(FactoMineR)
library(ggcorrplot)
library(tidyr)
library(ggsci)
library(ggpubr)
library(Amelia)
library(xgboost)
library(caret)
library(PFIM)
library(magick)
library(VIM)
library(stringr)
library(ggh4x)
library(purrr)
library(flextable)
library(officer)
library(colorBlindness)
library(haven)
library(copula)
library(fitdistrplus)
library(corrplot)
library(ggcorrplot)
library(tools)

setwd("C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1")

my_theme <- theme_minimal(base_size = 16) +
  theme(
    axis.title = element_text(size = 18, color = "black"),      # Axis labels size and color
    axis.text = element_text(size = 16, color = "black"),       # Axis ticks size and color
    axis.line = element_line(linewidth = 1.5),                  # Axis lines size
    legend.title = element_text(size = 18, color = "black"),    # Legend title size and color
    legend.text = element_text(size = 16, color = "black")      # Legend text size and color
  )



set.seed(123)
url <- "https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_BIOPRO.xpt"
download.file(url, destfile = "DEMO_I.XPT", mode = "wb")
# Read the .XPT file
nhanes_data_BIO <- read_xpt("DEMO_I.XPT")
# View first few rows
head(nhanes_data_BIO)

url <- "https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_DEMO.xpt"
download.file(url, destfile = "DEMO_I.XPT", mode = "wb")
# Read the .XPT file
nhanes_data_DEM <- read_xpt("DEMO_I.XPT")
# View first few rows
head(nhanes_data_DEM)

url <- "https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/P_BMX.xpt"
download.file(url, destfile = "DEMO_I.XPT", mode = "wb")
# Read the .XPT file
nhanes_data_EXAM <- read_xpt("DEMO_I.XPT")
# View first few rows
head(nhanes_data_EXAM)



# List of data frames to merge
dfs <- list(nhanes_data_BIO, nhanes_data_DEM, nhanes_data_EXAM)

# Merge all data frames in the list by SEQN
merged_data <- Reduce(function(x, y) merge(x, y, by = "SEQN", all.x = TRUE), dfs)

# Compute eGFR using CKD-EPI with a race adjustment factor (1.159 for non-Hispanic Blacks)
merged_data$eGFR_CKD_EPI <- with(merged_data, ifelse(RIAGENDR == 1,
  ifelse(LBXSCR <= 0.9,
         141 * (LBXSCR / 0.9)^(-0.411) * (0.993^RIDAGEYR) * ifelse(RIDRETH1 == 3, 1.159, 1),
         141 * (LBXSCR / 0.9)^(-1.209) * (0.993^RIDAGEYR) * ifelse(RIDRETH1 == 3, 1.159, 1)
  ),
  ifelse(LBXSCR <= 0.7,
         144 * (LBXSCR / 0.7)^(-0.329) * (0.993^RIDAGEYR) * ifelse(RIDRETH1 == 3, 1.159, 1),
         144 * (LBXSCR / 0.7)^(-1.209) * (0.993^RIDAGEYR) * ifelse(RIDRETH1 == 3, 1.159, 1)
  )
))

# Compute Body Surface Area (BSA) using the DuBois formula
merged_data$BSA <- with(merged_data, 0.007184 * BMXWT^0.425 * BMXHT^0.725)

# Adjust the CKD-EPI eGFR to obtain the absolute value (in mL/min)
merged_data$eGFR_abs <- with(merged_data, eGFR_CKD_EPI * (BSA / 1.73))


# Filter the merged dataset:
final_df <- merged_data %>%
  # Keep only participants aged 18 or older
  filter(RIDAGEYR >= 18) %>%
  # Remove rows with missing values in any of the selected variables:
  filter(!is.na(RIDAGEYR),
         !is.na(BMXHT),
         !is.na(BMXWT),
         !is.na(LBDSALSI),
         !is.na(eGFR_abs)) %>%
  # Select only the desired columns:
  select(RIDAGEYR, BMXHT, LBDSALSI, eGFR_abs, BMXWT) %>%
  # Rename the columns as requested:
  rename(Age = RIDAGEYR,
         Height = BMXHT,
         Alb = LBDSALSI,
         CrCl = eGFR_abs,
         WT = BMXWT)

# Check the first few rows of the new dataframe
head(final_df)

var_order <- c("Age", "Height", "Alb", "CrCl", "WT")

fit_multiple_dists <- function(x, dist_vec) {
  fits <- list()
  for(d in dist_vec) {
    res <- tryCatch({
      fitdist(x, d)
    }, error = function(e) NULL) # Handle errors gracefully
    fits[[d]] <- res
  }
  return(fits)
}

best_fits <- list()

# Define candidate distributions for each variable
dist_choices <- list(
  Age    = NULL,                # Empirical
  WT     = c("lnorm", "norm", "gamma"),
  CrCl   = c("lnorm", "norm", "gamma"),
  Alb    = c("lnorm", "norm", "gamma"),
  Height = c("lnorm", "norm", "gamma")
)

for (var in var_order) {
  cat("\n===== Variable:", var, "=====\n")
  
  x <- final_df[[var]]

  if (is.null(dist_choices[[var]])) {
    cat("Using empirical approach for", var, "\n")
    best_fits[[var]] <- NULL  # Store as NULL for later handling
  } else {
    candidate_fits <- fit_multiple_dists(x, dist_choices[[var]])
    
    best_aic  <- Inf
    best_dist <- NULL
    
    for (d in names(candidate_fits)) {
      fit_obj <- candidate_fits[[d]]
      if (!is.null(fit_obj)) {
        cat("  Distribution:", d, "| AIC:", round(fit_obj$aic, 2), "| BIC:", round(fit_obj$bic, 2), "\n")
        if (fit_obj$aic < best_aic) {
          best_aic  <- fit_obj$aic
          best_dist <- fit_obj
        }
      }
    }
    
    best_fits[[var]] <- best_dist
    if (!is.null(best_dist)) {
      cat("  --> Best fit for", var, "is", best_dist$distname, "with AIC =", round(best_dist$aic, 2), "\n")
    }
  }
}

data_mat <- as.matrix(final_df[, var_order])
u_data <- pobs(data_mat)

norm_cop <- normalCopula(dim = length(var_order), dispstr = "un")
fit_norm <- fitCopula(norm_cop, u_data, method = "mpl")
summary(fit_norm)


n_sim <- 100
sim_u <- rCopula(n_sim, fit_norm@copula)

simulate_from_fit <- function(u, dist_fit, var_name) {
  if (is.null(dist_fit)) {
    return(quantile(final_df[[var_name]], u, type = 8)) # Empirical for Age
  }
  
  dist_name <- dist_fit$distname
  est <- dist_fit$estimate
  
  if (dist_name == "norm") {
    return(qnorm(u, mean = est["mean"], sd = est["sd"]))
    
  } else if (dist_name == "lnorm") {
    return(qlnorm(u, meanlog = est["meanlog"], sdlog = est["sdlog"]))
    
  } else if (dist_name == "gamma") {
    return(qgamma(u, shape = est["shape"], rate = est["rate"]))
    
  } else {
    stop(paste("Distribution", dist_name, "not handled"))
  }
}

simulated_list <- list()

for (i in seq_along(var_order)) {
  var_name <- var_order[i]
  u_col    <- sim_u[, i]
  dist_fit <- best_fits[[var_name]]
  
  sim_values <- simulate_from_fit(u_col, dist_fit, var_name)
  simulated_list[[var_name]] <- sim_values
}

simulated_df <- as.data.frame(simulated_list)
head(simulated_df)


real_data <- final_df[var_order] %>%
  mutate(Type = "Real")

sim_data <- simulated_df %>%
  mutate(Type = "Simulated")

combined_data <- bind_rows(real_data, sim_data)

plot_var <- function(var) {
  ggplot(combined_data, aes_string(x = var, fill = "Type")) +
    geom_density(alpha = 0.5) +
    theme_minimal() +
    ggtitle(paste("Distribution of", var, "Real vs Simulated"))
}

plot_var <- function(var) {
  ggplot(combined_data, aes_string(x = var, fill = "Type")) +
    geom_density(alpha = 0.5) +
    my_theme +
    ggtitle(paste("Distribution of", var, "Real vs Simulated"))
}

# Generate plots
plot_age    <- plot_var("Age")
plot_wt     <- plot_var("WT")
plot_crcl   <- plot_var("CrCl")
plot_alb    <- plot_var("Alb")
plot_height <- plot_var("Height")

# Compute correlation matrices for real and simulated data
cor_matrix_real <- cor(real_data[, var_order], use = "pairwise.complete.obs")
cor_matrix_sim  <- cor(simulated_df[, var_order], use = "pairwise.complete.obs")

# Correlation plot for real data
cor_plot_real <- ggcorrplot(cor_matrix_real, 
                            type = "lower", lab = TRUE, 
                            hc.order = FALSE, # Ensure order is fixed
                            lab_size = 3, 
                            colors = c("blue", "white", "red")) + 
  ggtitle("Correlation Matrix: Real Data") +
  theme_bw(base_size = 11)

# Correlation plot for simulated data
cor_plot_sim <- ggcorrplot(cor_matrix_sim, 
                           type = "lower", lab = TRUE, 
                           hc.order = FALSE, # Ensure order is fixed
                           lab_size = 3, 
                           colors = c("blue", "white", "red")) + 
  ggtitle("Correlation Matrix: Simulated Data") +
  theme_bw(base_size = 11)

# Create an empty plot using gridExtra
empty_plot <- grid.rect(gp = gpar(col = "white"))


combined_plot <- ggarrange(
  plot_age, plot_wt, 
  plot_crcl, plot_alb, 
  plot_height, empty_plot,cor_plot_real, cor_plot_sim,
  nrow = 4, ncol = 2,
  common.legend = TRUE, legend = "right"
)


png("NHANES_Copula.png", width = 18, height = 12, units = "in", res = 900) 
print(combined_plot)  # Ensure the plot is printed inside png()
dev.off()


sim_data<-simulated_df
# Add binary variables for PPI and UM
sim_data$PPI <- rbinom(n = 100, size = 1, prob = 0.3)
sim_data$UM <- rbinom(n = 100, size = 1, prob = 0.5)
sim_data$ID <- 1:100

# Optional: Check correlations in the simulated data
cor(sim_data)





modelEquations = list(

  outcomes = list( "RespPK"),

  equations = list(  "RespPK" = "dose_RespPK/V * ka/(ka - Cl/V) * (exp(-Cl/V * t) - exp(-ka * t))"))

# model parameters
modelParameters = list(
  ModelParameter( name = "V",    distribution = LogNormal( mu = 10, omega = 0.30) ),
  ModelParameter( name = "Cl",   distribution = LogNormal( mu = 1, omega = 0.15) ),
  ModelParameter( name = "ka",   distribution = LogNormal( mu = 1, omega = 0.5) ))


# error Model
errorModelRespPK = Combined1( outcome = "RespPK", sigmaInter = 0.3, sigmaSlope = 0.15 )
modelError = list( errorModelRespPK )

## sampling times
samplingTimesRespPK = SamplingTimes( outcome = "RespPK", samplings = c(0.25, 0.5, 4, 11, 12)   )

# arm
administrationRespPK = Administration( outcome = "RespPK", dose = c( 100 ))

arm1 = Arm( name = "BrasTest1",
            size = 100,
            administrations = list( administrationRespPK ),
            samplingTimes   = list( samplingTimesRespPK  ))



design1 = Design( name = "design1", arms = list( arm1 ))



evaluationPop = Evaluation( name = "",
                            modelEquations = modelEquations,
                            modelParameters = modelParameters,
                            modelError = modelError,
                            outcomes = list( "RespPK"),
                            designs = list( design1 ),
                            fim = "population",
                            odeSolverParameters = list( atol = 1e-8, rtol = 1e-8 ) )

evaluationPop = run( evaluationPop )


plotOptions = list( unitTime=c("hour"), unitResponses= c("mcg/mL"))
outputPath = "C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1"
outputFile = "EvaluationPopFIMR1v4.html"
PFIM::Report( evaluationPop, outputPath, outputFile, plotOptions )


# constraints
administrationConstraintsRespPK = AdministrationConstraints( outcome = "RespPK", doses = c( 100) )
		
samplingConstraintsRespPK  = SamplingTimeConstraints( outcome = "RespPK",
                                                      initialSamplings = c(0.25,0.5,0.75,1,1.5,2,3:12),
                                                      numberOfsamplingsOptimisable = 5 )


arm1 = Arm( name = "BrasTest1",
            size = 100,
            administrations = list( administrationRespPK ),
            samplingTimes   = list( samplingTimesRespPK  ),
            administrationsConstraints = list( administrationConstraintsRespPK ),
            samplingTimesConstraints = list( samplingConstraintsRespPK ) )



design1 = Design( name = "design1", arms = list( arm1 ))


# --------------------------------------
# Optimization

# optimize the Fisher Information Matrix for the PopulationFIM
optimizationPopFIM = Optimization( name = "PK_analytic_populationFIM",
                             modelEquations = modelEquations,
                             modelParameters = modelParameters,
                             modelError = modelError,
                             optimizer = "FedorovWynnAlgorithm",
                             optimizerParameters = list( elementaryProtocols = list(c(0.25,0.5,0.75,1,1.5,2,3:12)),
                             numberOfSubjects = c(100),
                             proportionsOfSubjects = c(1),
                             showProcess = T),
                             designs = list( design1 ),
                             fim = "population",
                             outcomes = list( "RespPK" ) )

optimizationPopFIM = run( optimizationPopFIM )


# plots
plotOptions = list( unitTime=c("hour"), unitResponses= c("mcg/mL"))
outputPath = "C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1"
outputFile = "OptimizationPopFIMR1.html"
PFIM::Report( optimizationPopFIM, outputPath, outputFile, plotOptions )




### Part from L. FAYETTE (IAME) ###
rootDir = getwd()
currentFolder = file.path(rootDir)


#

USER_DEFINED_FUNCT = FALSE # Model evaluation in PFIM does not rely on a user defined function
PARRALEL_FIM = FALSE # No parallel computation in PFIM

pfimCovariatePath = file.path(rootDir, "PFIM_6_1_beta_cov")
devtools::load_all(pfimCovariatePath)


currentFolder = file.path(rootDir, "R1")



# Create folder for PFIM results
pfimResultFolder = "PFIM_results_6_1"
pfimResultGeneralPath = file.path(currentFolder, pfimResultFolder)
dir.create(pfimResultGeneralPath)


################################################################################
# ... function for saving tabs and figures

write_tab_latex <- function(my_tab, savetitle, path){
  
  my_tab_path = paste0(path, "/", savetitle, "_latex.tex")
  nCol = ncol(my_tab)
  stringNcol = paste(rep("c", times = nCol), collapse = " ")
  
  sink(my_tab_path)
  cat("\\begin{table}[] 
\\centering
\\rowcolors{2}{lightgray!40}{}
\\caption{}
")
  cat(paste0("\\begin{tabular}{", stringNcol, "}"))
  cat("\\rowcolor{lightgray}")
  
  sink()
  
  write.table(my_tab, file = my_tab_path,
              row.names = FALSE,
              quote = FALSE,
              sep = " & ",
              eol = " \\\\ \n",
              append = TRUE)
  
  sink(my_tab_path, append = TRUE)
  
  
  cat("\\end{tabular}
      \\label{tab:}
      \\end{table}")
  
  sink()
  
  closeAllConnections()
  
  
}

save_my_fig <- function(plot, fig_file_name, save_path, 
                        width = 4, height = 4){
  png_name = paste0(fig_file_name, ".png")
  png(file.path(save_path, png_name), width = width, height = height, units = 'in', res = 900) 
  print(plot)
  dev.off()
  
}

my_theme = theme_bw(base_size = 11) +
  theme( 
    legend.key.size = unit(1,"lines"),
    legend.margin=margin(t = 0, r = 0, b = 0, l = 0, "cm"), 
    legend.position="right", 
    plot.margin = margin(t = 0, r = 0, b = 0, l = 0, "cm"),
    legend.box.margin=margin(-1,0,0,0, "cm"),
    legend.title=element_text(size=rel(1)),
    legend.text=element_text(size=rel(0.9)), 
    axis.text.y=element_text(size=rel(0.8)),
    axis.text.x=element_text(size=rel(0.8)),
    axis.title=element_text(size=rel(1)),
    strip.text.x = element_text(size = rel(0.9)) #facet
  ) 

################################################################################
# --- PFIM Set Up

# --------------------------------------
# model equations
# --------------------------------------
modelEquations = list(
  
  outcomes = list( "RespPK" ),
  
  equations = list("RespPK" = "dose_RespPK/V * ka/(ka - Cl/V) * (exp(-Cl/V * t) - exp(-ka * t))") )

# --------------------------------------
# model parameters
# --------------------------------------

mu_Cl = 1
mu_V = 10
mu_ka = 1

WT_VC = 1 # Weight on Vc ()
CrCl_CL = 0.8 # Creatinine Clearance on CL
PPI_KA = log(0.5) # Effect of Proton Pump Inhibitor on KA
UM_CL = log(2) # Effect of UM on CL

modelParameters = list(
  ModelParameter( name = "Cl",   distribution = LogNormal( mu = mu_Cl, omega = 0.15 ) ),
  ModelParameter( name = "V",    distribution = LogNormal( mu = mu_V, omega = 0.30 ),),
  ModelParameter( name = "ka",   distribution = LogNormal( mu = mu_ka, omega = 0.50 ) ),
  
  ModelParameter( name = "beta_V_WT",
                  distribution = Normal( mu = WT_VC , omega = 0 ), fixedOmega = TRUE, covariateEffect = TRUE),
  ModelParameter( name = "beta_Cl_CLCR",
                  distribution = Normal( mu = CrCl_CL , omega = 0 ), fixedOmega = TRUE, covariateEffect = TRUE ),
  ModelParameter( name = "beta_Cl_UM",
                  distribution = Normal( mu = UM_CL , omega = 0 ), fixedOmega = TRUE, covariateEffect = TRUE ),
  ModelParameter( name = "beta_ka_PPI",
                  distribution = Normal( mu = PPI_KA , omega = 0 ), fixedOmega = TRUE, covariateEffect = TRUE )
  
)


# --------------------------------------
# model covariate relationship
# --------------------------------------
modelRelationships = list(
  ModelRelationship( name = "beta_V_WT", param = "V", cov = "WT", relationship = "Exp" ),
  ModelRelationship( name = "beta_Cl_CLCR", param = "Cl", cov = "CLCR", relationship = "Exp" ),
  ModelRelationship( name = "beta_Cl_UM", param = "Cl", cov = "UM", relationship = "Exp" ),
  ModelRelationship( name = "beta_ka_PPI", param = "ka", cov = "PPI", relationship = "Exp")
)

# --------------------------------------
# error model
# --------------------------------------
errorModelRespPK = Combined1( outcome = "RespPK", sigmaInter = 0.30 , sigmaSlope = 0.15  )
modelError = list( errorModelRespPK )

# --------------------------------------
# Arm
# --------------------------------------

## administration & multi-doses
administrationRespPK = Administration( outcome = "RespPK",
                                       timeDose = c(0),
                                       dose = c( 100 ) )

## sampling times
samplingTimesRespPK = SamplingTimes( outcome = "RespPK",
                                     samplings =  	c(0.5, 4,5, 11, 12)   )

## arms
arm1 = Arm( name = "BrasTest",
            size = 100,
            administrations = list( administrationRespPK ) ,
            samplingTimes   = list( samplingTimesRespPK ) )

# --------------------------------------
# Covariates
# --------------------------------------

median_CLCR = median(sim_data$CrCl)
median_WT = median(sim_data$WT)


head(sim_data)

PFIM_covariates.df = sim_data %>%
  dplyr::select(WT, CrCl, PPI, UM) %>%
  mutate(WT = log(WT/median_WT),
         CLCR = log(CrCl/median_CLCR))

PFIM_covariates.df = PFIM_covariates.df %>% 
  dplyr::select(CLCR, WT, PPI, UM)


logtCLCR_ref = 0
logtCLCR_P90 = log(quantile(sim_data$CrCl, probs = c(0.90))/median_CLCR)
logtCLCR_P10 = log(quantile(sim_data$CrCl, probs = c(0.10))/median_CLCR)

logtWT_ref = 0
logtWT_P90 = log(quantile(sim_data$WT, probs = c(0.90))/median_WT)
logtWT_P10 = log(quantile(sim_data$WT, probs = c(0.90))/median_WT)


list_CovariateForRatio = list(
  #...Continuous
  "CLCR" = CovariateForRatio(cov = "CLCR",
                             relationship = "Exp",
                             ref = logtCLCR_ref,
                             values = c(logtCLCR_P90, logtCLCR_P10),
                             names = c("P90", "P10")
  ),
  "WT"= CovariateForRatio(cov = "WT",
                          relationship = "Exp",
                          ref = logtWT_ref,
                          values = c(logtWT_P90, logtWT_P10),
                          names = c("P90", "P10")
  ),
  
  #...Discrete
  "PPI" = CovariateForRatio(cov = "PPI",
                            relationship = "Exp",
                            ref = 0,
                            values = c(1),
                            names = c("PPI")
  ),
  "UM" = CovariateForRatio(cov = "UM",
                           relationship = "Exp",
                           ref = 0,
                           values = c(1),
                           names = c("UM")
  )
)

CovariateOrder = c( "CLCR", "WT", "PPI", "UM" )
RatioOrder = c( "CLCR on Cl, P10", "CLCR on Cl, P90",
                "WT on V, P10", "WT on V, P90",
                "PPI on ka", "UM on Cl" )

MC = nrow(PFIM_covariates.df)

#CovariatePop
CovariatePopType = "sampled"
covariatePop = CovariatePop(type = CovariatePopType,
                            data = PFIM_covariates.df,
                            MC = MC)

# --------------------------------------
# Design
# --------------------------------------
design1 = Design( name = "design1",
                  arms = list( arm1 ),
                  covariates = covariatePop )

################################################################################
# --- Run Evaluation

# --------------------------------------
# Evaluation
# --------------------------------------
evaluation.Data = Evaluation( name = "Rouen - Covariates integration: data",
                              modelEquations = modelEquations,
                              modelParameters = modelParameters,
                              modelRelationships = modelRelationships,
                              modelError = modelError,
                              outcomes = list( "RespPK"),
                              designs = list( design1 ),
                              fim = "population" )

evaluation.Data = run( evaluation.Data )

show( evaluation.Data )

saveEvaluationFIMpath = file.path(pfimResultGeneralPath, "evaluationFIM_Data.RData4")
save(evaluation.Data, file = saveEvaluationFIMpath)

################################################################################
#--- Important constants

alphalevel.signif = 0.05
alphalevel.relev = 0.1

ratioSup = 1.25
ratioInf = 0.80

desiredPowerSignif = 0.8
desiredPowerRelev = 0.8

# --------------------------------------
# Power computation
# --------------------------------------

powerSignif = powerSignificance(evaluation.Data, alphalevel.signif)
powerSignif.df = data.frame(Parameter = names(powerSignif),
                            PowerSignif = as.numeric(powerSignif))

save(powerSignif.df, file = file.path(pfimResultGeneralPath, paste0("powerSignif.df", ".RData")))

powerSignif.df_Latex = powerSignif.df %>%
  rowwise() %>%
  mutate(Parameter = gsub("beta, ", "\\beta_{", names_to_latex(Parameter), fixed=TRUE)) %>%
  rename("P_{Signif}" = PowerSignif)

savetitle = "PowerSignificance"
write_tab_latex(powerSignif.df_Latex, savetitle, pfimResultGeneralPath)


CI_ratio_Latex = list_ciRatioToDftoPlot(ciRatio(evaluation.Data, alphalevel.relev,  list_CovariateForRatio), LaTex = T)
savetitle = "CIratios"
write_tab_latex(CI_ratio_Latex, savetitle, pfimResultGeneralPath)


powerRelevance = powerRelevanceRatio(evaluation.Data, alphalevel.relev, ratioInf, ratioSup, list_CovariateForRatio )
powerRelevance.df = list_CovariateForRatioToDftoPlot(powerRelevance)

save(powerRelevance.df, file = file.path(pfimResultGeneralPath, paste0("powerSignif", ".RData")))


powerRelevance.df_Latex = list_CovariateForRatioToDftoPlot(powerRelevance, LaTex = TRUE) %>%
  rowwise() %>%
  mutate(Ratio = ifelse(!grepl(Names, Covariate, fixed=TRUE),
                        paste0(Covariate, ", (", Names, ")"),
                        Covariate) ) %>%
  mutate(PRelev = round(PRelev, 2)) %>%
  arrange(Ratio) %>%
  dplyr::select(Ratio, PRelev) %>%
  rename("P_{relev}" = PRelev)

savetitle = "PowerRelevance"
write_tab_latex(powerRelevance.df_Latex, savetitle, pfimResultGeneralPath)



# --------------------------------------
# Number of subjects needed
# --------------------------------------
sujSgnif = subjectsPowerSignificance(evaluation.Data, alphalevel.signif, desiredPowerSignif)

sujSgnif.df = data.frame(Parameter = names(sujSgnif),
                         N = as.vector(sujSgnif))
save(sujSgnif.df, file = file.path(pfimResultGeneralPath, paste0("NneededSignificance", ".RData")))

sujSgnif.df_Latex = sujSgnif.df %>%
  rowwise() %>%
  mutate(
    Parameter = names_to_latex(Parameter),
    N = ceiling(N)
  )

savetitle = "NneededSignificance"
write_tab_latex(sujSgnif.df_Latex, savetitle, pfimResultGeneralPath)


sujRelev = subjectsPowerRelevance(evaluation.Data, alphalevel.relev,
                                  desiredPowerRelev,
                                  ratioInf, ratioSup,
                                  list_CovariateForRatio)

sujRelev.df = data.frame(Ratio = names(sujRelev),
                         N = as.vector(unlist(sujRelev)))
save(sujRelev.df, file = file.path(pfimResultGeneralPath, paste0("NneededRelevance", ".RData")))
sujRelev.df_Latex = sujRelev.df %>%
  rowwise() %>%
  mutate(
    Ratio = paste0("$", gsub(", ", "$, $", Ratio, fixed=TRUE), "$"),
    Ratio = gsub("\\$P10\\$", "(P10)", Ratio),  # Properly escaping $
    Ratio = gsub("\\$P90\\$", "(P90)", Ratio),  # Properly escaping $
    N = ceiling(N)
  )  %>%
  arrange(Ratio)

savetitle = "NneededRelevance"
write_tab_latex(sujRelev.df_Latex, savetitle, pfimResultGeneralPath)

CI_andPower_andN = left_join(left_join(CI_ratio_Latex, powerRelevance.df_Latex),
                             sujRelev.df_Latex)
savetitle = "Relevance_CI_Power_N"
write_tab_latex(CI_andPower_andN, savetitle, pfimResultGeneralPath)

################################################################################
# --- Plot Power as a function of N

N_list = seq(1, 200, 1) # N to test
N_init = 100 # N used in evaluation

q.1_alpha2_Significance = qnorm(p= 1-alphalevel.signif/2, 0, 1)
q.1_alpha2_CIratio = qnorm(p= 1-alphalevel.relev/2, 0, 1)

nCov = 4
df_for_power_Ninit =  tableRes(evaluation.Data)[1:nCov, ] %>%
  mutate(Name = gsub("\u03bc_", "", Parameter)) %>%
  rename(
    Values = Value
  ) %>%
  dplyr::select(-Parameter, -RSE)

df_for_power_Ninit$Covariate = c("CLCR", "UM", "PPI", "WT")
df_for_power_Ninit$Parameter = c("Cl", "Cl", "ka", "V")

#---Significance
powerSignificance_Ninit = df_for_power_Ninit %>%
  rowwise() %>%
  mutate(statTest = Values/SE,
         PSignificance = round(1 - pnorm(q.1_alpha2_Significance - statTest , mean = 0, sd = 1) + pnorm(-q.1_alpha2_Significance - statTest , mean = 0, sd = 1),2)
  )


powerEquivalenceRatio_Empirical <- function(df, alphalevel, ratioInf, ratioSup, list_CovariateForRatio) {
  
  list_PRelev = list()
  
  q.1_alpha2 = qnorm(1-alphalevel/2, mean = 0, sd = 1) 
  
  
  values.covariates = df$Values
  SE.covariates = df$SE
  
  covariates = df$Covariate
  relationships = df$Name
  parameters = df$Parameter
  
  Nrelations = length(relationships)
  
  for(r in 1:Nrelations){
    covEffect = values.covariates[r]
    se = SE.covariates[r]
    
    covForRatio = list_CovariateForRatio[[covariates[r]]]
    
    nameRelationShip = paste0(covariates[r], " on ", parameters[r])
    
    list_PRelev[[nameRelationShip]] = covForRatio
    list_PRelev[[nameRelationShip]]@cov = nameRelationShip
    
    ref.cov = covForRatio@ref
    cov.values = covForRatio@values
    
    nCov.values = length(cov.values)
    
    cov.PRelev = rep(NA, times = nCov.values)
    
    
    for(c in 1:nCov.values){
      if(nCov.values==1){
        Zi = cov.values
      }else{
        Zi = cov.values[c]
      }
      
      Zi_Z_ref = Zi - ref.cov
      
      if(Zi_Z_ref>0){
        Bsup = log(ratioSup)/Zi_Z_ref
        Binf = log(ratioInf)/Zi_Z_ref
      }else{
        Bsup = log(ratioInf)/Zi_Z_ref
        Binf = log(ratioSup)/Zi_Z_ref
      }
      
      
      PRelev = 1 - pnorm( q.1_alpha2 - (covEffect - Bsup)/se ,  mean = 0, sd = 1) +  pnorm(-q.1_alpha2 - (covEffect-Binf)/se,  mean = 0, sd = 1) 
      
      
      cov.PRelev[c] = PRelev
      
    }
    
    
    
    list_PRelev[[nameRelationShip]]@PRelev = cov.PRelev
    
  }
  
  return(list_PRelev)
  
}


#---equivalence
PowerEquivalenceRatio_Ninit = list_CovariateForRatioToDftoPlot(powerEquivalenceRatio_Empirical(df_for_power_Ninit, alphalevel.relev,  ratioInf, ratioSup, list_CovariateForRatio))
PowerEquivalenceRatio_Ninit = PowerEquivalenceRatio_Ninit %>%
  rename(PRelevance = PRelev)


allSignificance_df = data.frame(matrix(data = 0, ncol = 4, nrow=0))
colnames(allSignificance_df) = c("Covariate", "PSignificance", "N")

allEquivalence_df = data.frame(matrix(data = 0, ncol = 5, nrow=0))
colnames(allEquivalence_df) = c("Covariate", "Names", "PRelevance",  "N")

for(N in N_list){
  
  # df for power N
  df_for_power_N = df_for_power_Ninit %>% mutate(
    SE = sqrt(N_init/N)*SE
  )
  
  # Power Significance test
  powerSignificance_N = df_for_power_N %>%
    rowwise() %>%
    mutate(statTest = Values/SE,
           PSignificance = round(1 - pnorm(q.1_alpha2_Significance - statTest , mean = 0, sd = 1) + pnorm(-q.1_alpha2_Significance - statTest , mean = 0, sd = 1),2)
    )
  
  powerSignificance_N$N = N
  
  powerSignificance_N_bind = powerSignificance_N %>%
    dplyr::select(Covariate, PSignificance, N)
  
  allSignificance_df = rbind(allSignificance_df, powerSignificance_N_bind)
  
  # Power Relevance test
  PowerEquivalenceRatio_DF = list_CovariateForRatioToDftoPlot(powerEquivalenceRatio_Empirical(df_for_power_N, alphalevel.relev,  ratioInf, ratioSup, list_CovariateForRatio))
  
  PowerEquivalenceRatio_DF$N = N
  
  allEquivalence_df = rbind(allEquivalence_df, PowerEquivalenceRatio_DF)
  
}

head(allSignificance_df)
allSignificance_df$Covariate = factor(allSignificance_df$Covariate, levels = CovariateOrder)

head(allEquivalence_df)

allEquivalence_df = allEquivalence_df %>%
  rowwise() %>%
  mutate(Ratio = ifelse(!grepl(Names, Covariate, fixed=TRUE),
                        paste0(Covariate, ", ", Names),
                        Covariate) ) %>%
  mutate(Ratio = factor(Ratio, levels = RatioOrder)) %>%
  arrange(Ratio)


# --------------------------------------
# Power plots
# --------------------------------------

Covariate.colors = SteppedSequential5Steps[c(1, 21, 16, 11)]
Ratio.colors = SteppedSequential5Steps[c(1, 5, 21, 25, 16, 11)]

PowerSignif.plot = ggplot(allSignificance_df[allSignificance_df$N<100,],
                          aes(x = N, y = PSignificance, color = Covariate)) +
  geom_line() +
  scale_color_manual(values = Covariate.colors) +
  my_theme +
  labs(x = "N", y = "P_{signif}")

PowerRelev.plot = ggplot(allEquivalence_df, aes(x = N, y = PRelev, color = Ratio)) +
  geom_line() +
  scale_color_manual(values = Ratio.colors) +
  my_theme +
  labs(x = "N", y = "P_{relev}")



#...Save Plots
fig_file_name = "PowerSignificance"
save_my_fig(PowerSignif.plot, fig_file_name, pfimResultGeneralPath, width = 8, height = 4)


fig_file_name = "PowerRelevance"
save_my_fig(PowerRelev.plot, fig_file_name, pfimResultGeneralPath, width = 8, height = 4)


################################################################################
# --- Optimisation 

# arm
newAdministrationRespPK = Administration( outcome = "RespPK", dose = c( 100 ))


## sampling times
newSamplingTimesRespPK = SamplingTimes( outcome = "RespPK", 
                                        samplings = c( 0.25, 0.5, 4, 11, 12 ))




# constraints
administrationConstraintsRespPK = AdministrationConstraints( outcome = "RespPK", doses = c( 100) )

samplingConstraintsRespPK  = SamplingTimeConstraints( outcome = "RespPK",
                                                      initialSamplings = c(0.25,0.5,0.75,1,1.5,2,3:12),
                                                      numberOfsamplingsOptimisable = 5 )

armConstr = Arm( name = "BrasConstr",
                 size = 100,
                 administrations = list( newAdministrationRespPK ) ,
                 samplingTimes   = list( newSamplingTimesRespPK ), 
                 administrationsConstraints = list( administrationConstraintsRespPK ),
                 samplingTimesConstraints = list( samplingConstraintsRespPK ))


designConstr = Design( name = "design1", 
                       arms = list( armConstr ), 
                       covariates = covariatePop )

#...Multiplicative algorithm
optimization.Mult = Optimization( name = "Opti_Rouen",
                                  modelEquations = modelEquations,
                                  modelParameters = modelParameters,
                                  modelRelationships = modelRelationships,
                                  modelError = modelError,
                                  
                                  
                                  optimizer = "MultiplicativeAlgorithm",
                                  optimizerParameters = list( lambda = 0.99,
                                                              numberOfIterations = 1000,
                                                              weightThreshold = 0.01,
                                                              delta = 1e-04, showProcess = T ),
                                  
                                  designs = list( designConstr ),
                                  fim = "population",
                                  outcomes = list( "RespPK" = "RespPK" ),
                                  odeSolverParameters = list( atol = 1e-8, rtol = 1e-8 ) )

deb.Mult= Sys.time()
optimization.Mult = run( optimization.Mult )
fin.Mult = Sys.time()

difftime(fin.Mult, deb.Mult) #Time difference of 1.006751 days


optimization.Mult@optimizationResults

saveOptiMultFIMpath = file.path(pfimResultGeneralPath, "optiMultFIM_Data.RData4")
save(optimization.Mult, file = saveOptiMultFIMpath)

### End of L. FAYETTE PFIM 6.1 custom ###




my_theme <- theme_minimal(base_size = 16) +
  theme(
    axis.title = element_text(size = 18, color = "black"),      # Axis labels size and color
    axis.text = element_text(size = 16, color = "black"),       # Axis ticks size and color
    axis.line = element_line(linewidth = 1.5),                  # Axis lines size
    legend.title = element_text(size = 18, color = "black"),    # Legend title size and color
    legend.text = element_text(size = 16, color = "black")      # Legend text size and color
  )



# Define the PK model
code <- '

$PROB

$PARAM @annotated
TVCL : 1  : 1  Clearance (L.h-1)
TVVC : 10  : 2  Central volume (L)
TVKA : 1  : 3  Absorption constant (L)


$PARAM @annotated @covariates

WT : 70 : Weight (kg)
CrCl : 90 : Creatinine clearance (mL/min/1.73m²)
PPI : 0 : PPI intake or not
UM : 0 : Ultrarapid metabolizer
WT_VC : 1 : Weight on Vc ()
CrCl_CL : 0.8 : Creatinine Clearance on CL
PPI_KA : 0.5 : Effect of Proton Pump Inhibitor on KA
UM_CL : 2 : Effect of UM on CL

$OMEGA @block
0.0225// CL
0 0.09 // VC
0 0 0.25 // KA


$SIGMA 
0.0225 // err prop
0.09 //  err additive


$CMT @annotated
GUT : Depot compartment [ADM]
CENTRAL : Central compartment (mg/L) [OBS]



$TABLE
double DV  = (CENTRAL / VC)+ (EPS(2)+(CENTRAL / VC)*EPS(1)) ;


$MAIN
double CL = TVCL * pow((CrCl/90),CrCl_CL) * pow((UM_CL),UM) *exp(ETA(1))    ;
double VC = TVVC * pow((WT/70),WT_VC)*exp(ETA(2))  ;
double KA = TVKA * pow((PPI_KA),PPI)*exp(ETA(3))   ;
double K10 = CL/VC ;


$ODE

dxdt_GUT        = -KA*GUT ;
dxdt_CENTRAL    = KA*GUT - K10*CENTRAL ;

$CAPTURE DV CL VC KA'

modbsv <- mrgsolve::mcode("optim", code, atol=1e-8, rtol=1e-8,maxsteps=5000)



ev1 <- ev(amt = 100, cmt = 1)
out <- 
  modbsv %>% 
  ev(ev1) %>%
  idata_set(sim_data) %>%
  mrgsim(delta=0.05, end=12, obsonly=TRUE)

out

png("mrgsolve output R1.png", width = 24, height = 9, units = 'in', res = 900)

plot(out)

# Close the graphics device
dev.off()




# Define the time séquence (from 0 to 12 by 0.05)
time_grid <- seq(0, 12, by = 0.05)

# Expand sim_data to create a new dataframe with time grid
expanded_sim_data <- sim_data[rep(1:nrow(sim_data), each = length(time_grid)), ]
expanded_sim_data$time <- rep(time_grid, nrow(sim_data))

# Initialize amt, evid, and cmt columns
expanded_sim_data$amt <- 0
expanded_sim_data$evid <- 0
expanded_sim_data$cmt <- 0

# Define dosing regimen: 100 mg dosing_times <- 0
dosing_times <- 0


# Apply the dosing regimen for each subject
for (i in 1:nrow(sim_data)) {
  # Get rows corresponding to the current subject
  subject_rows <- expanded_sim_data$ID == sim_data$ID[i]
  
  # Assign amt, evid, and cmt for the dosing times
  expanded_sim_data$amt[subject_rows & expanded_sim_data$time %in% dosing_times] <- 100
  expanded_sim_data$evid[subject_rows & expanded_sim_data$time %in% dosing_times] <- 1
  expanded_sim_data$cmt[subject_rows & expanded_sim_data$time %in% dosing_times] <- 1
}

# Check the expanded dataframe
head(expanded_sim_data)

# Merge expanded_sim_data and out based on 'ID' and 'time'
merged_data <- merge(expanded_sim_data, as.data.frame(out), by = c("ID", "time"))

# Check the merged data
head(merged_data)


# Filter merged_data to keep only rows where 'time' is (0.25, 0.5, 4, 11, 12)
filtered_data <- merged_data[merged_data$time %in% c(0.25, 0.5, 4, 11, 12), ]

# Check the filtered data
head(filtered_data)

# Step 4: Append the duplicated rows back to the original dataframe
final_data <- filtered_data

# Step 5: Sort the final_data by ID and time to maintain order (optional)
final_data <- final_data[order(final_data$ID, final_data$time), ]

final_data_mice <-final_data
final_data_mice$PPI<-as.factor(as.character(final_data_mice$PPI))
final_data_mice$UM<-as.factor(as.character(final_data_mice$UM))








# Define the columns you want to use for imputation
selected_columns <- c(1:9, 15)  # Columns: ID, time, 3:9, and DV (column 14)




# Create a list to store dataframes with different percentages of missing data
missing_data_list <- list()

# Define the percentages of missing data to generate (5%, 20%, 50% and 75%)
percentages <- c(5, 20, 50,75)

# Get the unique IDs and the number of individuals
unique_ids <- unique(final_data$ID)
n_individuals <- length(unique_ids)

# Step 1: Create missing data for WT, CrCl, and PPI
introduce_missing_data <- function(df, variable, percentage) {
  n_missing <- round(n_individuals * percentage / 100)
  individuals_to_missing <- sample(unique_ids, n_missing, replace = FALSE)
  df[df$ID %in% individuals_to_missing, variable] <- NA
  return(df)
}

# Loop over each percentage and generate the corresponding dataframe with missing data
for (perc in percentages) {
  data_with_missing <- final_data_mice
  data_with_missing <- introduce_missing_data(data_with_missing, "CrCl", perc)
  data_with_missing <- introduce_missing_data(data_with_missing, "WT", perc)
  data_with_missing <- introduce_missing_data(data_with_missing, "PPI", perc)
  data_with_missing <- introduce_missing_data(data_with_missing, "UM", perc)
  missing_data_list[[paste0("missing_", perc, "perc")]] <- data_with_missing
}



# Define your list of missing data percentages
missing_percentages <- c("5%", "20%", "50%", "75%")

# Loop over each dataframe in missing_data_list and save each plot as PNG
for (i in 1:length(missing_data_list)) {
  # Subset the data to keep only the variables of interest
  selected_data <- missing_data_list[[i]][, c("UM", "CrCl", "PPI", "WT")]
  
  # Create the PNG filename, removing special characters like %
  png_filename <- paste0("Missing_Data_", gsub("%", "", missing_percentages[i]), ".png")
  
  # Save the plot as PNG with a larger width to provide more space
  png(png_filename, width = 1600, height = 700)
  
  # Increase the top margin to make space for the title
  par(mar = c(5, 5, 10, 5))  # Increase top margin to 10 for title space
  
  # Generate the aggr plot
  aggr(selected_data, numbers = TRUE, sortVars = TRUE, 
       prop = TRUE, cex.axis = 2, cex.lab = 2, cex.numbers = 2, gap = 5, 
       labels = c("UM", "CrCl", "PPI", "WT"), combined = FALSE, 
       col = c("skyblue", "red"), border = "black")
  
  # Add the title using mtext(), positioning it above the plot
  mtext(paste("Missing Data -", missing_percentages[i]), side = 3, line = 8.5, cex = 2, font = 2)
  
  # Close the PNG device
  dev.off()
}


# Load the 4 PNG images in the correct order
img1 <- image_read("Missing_Data_5.png")
img2 <- image_read("Missing_Data_20.png")
img3 <- image_read("Missing_Data_50.png")
img4 <- image_read("Missing_Data_75.png")

# Combine the 4 images into a 2x2 layout in the correct order
combined_img <- image_append(c(
  image_append(c(img1, img2), stack = TRUE), 
  image_append(c(img3, img4), stack = TRUE)
))

# Display the combined image
print(combined_img)

# Optionally, save the combined image as a PNG
image_write(combined_img, path = "Combined_Missing_Data_Final.png")

# Initialize an empty list to store the missing summary for each dataset
missing_summary_list <- list()

# Loop through each dataset in missing_data_list
for (i in seq_along(missing_data_list)) {
  
  # Get the current dataset
  data_with_missing <- missing_data_list[[i]]
  
  # Count the number of missing covariates per row (UM, CrCl, PPI, WT)
  data_with_missing$Missing_Count <- rowSums(is.na(data_with_missing[, c("UM", "CrCl", "PPI", "WT")]))
  
  # Compute the percentage of rows for each level of missing covariates (0 to 4)
  missing_summary <- data_with_missing %>%
    group_by(Missing_Count) %>%
    summarise(Percentage = n() / nrow(data_with_missing) * 100) %>%
    ungroup()
  
  # Store in the list with the name of the dataset
  missing_summary_list[[names(missing_data_list)[i]]] <- missing_summary
}

# Combine all summaries into one dataframe for better visualization
missing_summary_combined <- bind_rows(missing_summary_list, .id = "Dataset")

# Print the missing summary
print(missing_summary_combined)


# Load required libraries
library(flextable)
library(officer)
library(dplyr)

# Ensure Missing_Count is a factor for correct ordering in the table
missing_summary_combined <- missing_summary_combined %>%
  mutate(Missing_Count = factor(Missing_Count, levels = 0:4, labels = c("0", "1", "2", "3", "4")))

# Create a flextable for the missing covariates summary
missing_summary_ft <- flextable(missing_summary_combined) %>%
  theme_vanilla() %>%
  autofit() %>%
  set_caption("Summary of Simultaneous Missing Covariates") %>%
  set_header_labels(
    Dataset = "Dataset",
    Missing_Count = "Number of Missing Covariates",
    Percentage = "Percentage of Rows (%)"
  )

# Define the Word document
doc <- read_docx() %>%
  body_add_par("Missing Covariates Summary", style = "heading 1") %>%
  body_add_flextable(missing_summary_ft)

# Export to a Word file
output_file <- "Missing_Covariates_Summary.docx"
print(doc, target = output_file)

# Confirm the export
cat("Word document saved as:", output_file)






# Define the columns you want to use for imputation
selected_columns <- c(1:9, 15)  # Columns: ID, time, 3:9, and DV (column 14)


# Initialize the list for storing imputed data
mice_imputed_data_list <- list()

# Define the imputation methods:
# pmm for WT and CrCl (continuous), logreg for PPI and UM (binomial),
mice_imputation_methods <- c("", "","","","","pmm","pmm", "logreg","logreg","")

# Loop through each dataframe with missing data for imputation
for (i in 1:length(missing_data_list)) {
  
  # Subset the data to the relevant columns (including all potential predictors)
  mice_df_subset <- missing_data_list[[i]][, selected_columns]

  # Create a predictorMatrix to handle clustering by ID
  mice_predictorMatrix <- make.predictorMatrix(mice_df_subset)
  
  # Specify that ID should be used for clustering but not as a predictor
  mice_predictorMatrix[, "ID"] <- -2  # Cluster by ID
  mice_predictorMatrix["ID", ] <- 0   # ID shouldn't be a predictor
  
  # Perform the imputation using all variables as predictors
  # Use DV and time as predictors along with WT, CrCl, PPI, etc.
  mice_imputed_data <- mice(
    mice_df_subset, 
    m = 5,  # Perform multiple imputations
    maxit = 20,
    method = mice_imputation_methods,  # Imputation methods for each variable
    predictorMatrix = mice_predictorMatrix, 
    seed = 500
  )
  
  # Store the completed (imputed) dataset in the list
  mice_imputed_data_list[[paste0("mice_imputed_data_", i)]] <- mice::complete(mice_imputed_data)
  
  # Adjust imputed data to enforce consistency within each ID
  for (id in unique(mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$ID)) {
    idx <- mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$ID == id  # Get all rows for the specific ID
    
    # Calculate the mean for WT and CrCl, and the most frequent value for PPI
    mean_WT <- mean(mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$WT[idx], na.rm = TRUE)
    mean_CrCl <- mean(mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$CrCl[idx], na.rm = TRUE)
    most_frequent_PPI <- as.numeric(names(sort(table(mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$PPI[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1
    most_frequent_UM <- as.numeric(names(sort(table(mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$UM[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1
    # Replace all values within that ID with the calculated values
    mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$WT[idx] <- mean_WT
    mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$CrCl[idx] <- mean_CrCl
    mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$PPI[idx] <- most_frequent_PPI
    mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$UM[idx] <- most_frequent_UM
  }
}

# Set your working directory to the folder containing the files

setwd("C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1")
write.csv(final_data,"final_data.csv")


# Function to read files with a specific prefix into a list
read_imputed_data <- function(prefix) {
  # List all files that match the prefix and have an underscore followed by a number
  files <- list.files(pattern = paste0("^", prefix, "_\\d+\\.csv$"))
  
  # Sort the files to ensure they are in the correct order
  files <- sort(files)
  
  # Read each file and store it in a list
  data_list <- lapply(files, read.csv)
  
  # Optionally, name the list elements for easier reference
  names(data_list) <- paste0(prefix, "_", 1:length(data_list))
  
  return(data_list)
}






# Create a vector for the missing data percentages
missing_percentages <- c("5%", "20%", "50%", "75%")

# Initialize lists to store results
WT_errors <- list()
CrCl_errors <- list()
PPI_concordance <- list()
UM_concordance <- list()

# Loop through the missing and imputed datasets
for (i in 1:length(missing_data_list)) {
  
  # Extract missing rows from the current missing_data_list
  missing_WT <- missing_data_list[[i]] %>% filter(is.na(WT))
  missing_CrCl <- missing_data_list[[i]] %>% filter(is.na(CrCl))
  missing_PPI <- missing_data_list[[i]] %>% filter(is.na(PPI))
  missing_UM <- missing_data_list[[i]] %>% filter(is.na(UM))
 
  # Extract corresponding imputed data
  imputed_WT <- mice_imputed_data_list[[i]] %>% filter(ID %in% missing_WT$ID)
  imputed_CrCl <- mice_imputed_data_list[[i]] %>% filter(ID %in% missing_CrCl$ID)
  imputed_PPI <- mice_imputed_data_list[[i]] %>% filter(ID %in% missing_PPI$ID)
  imputed_UM <- mice_imputed_data_list[[i]] %>% filter(ID %in% missing_UM$ID)  
  # Match imputed with original data and compute errors for WT
  WT_error <- imputed_WT %>%
    inner_join(final_data %>% select(ID, WT), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%  # Keep only first row of each ID
    mutate(WT_imputed = WT.x, WT_original = WT.y,
           AE_WT = abs(WT_imputed - WT_original),
           PE_WT = (WT_imputed - WT_original) / WT_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "Mice")
  
  # Match imputed with original data and compute errors for CrCl
  CrCl_error <- imputed_CrCl %>%
    inner_join(final_data %>% select(ID, CrCl), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(CrCl_imputed = CrCl.x, CrCl_original = CrCl.y,
           AE_CrCl = abs(CrCl_imputed - CrCl_original),
           PE_CrCl = (CrCl_imputed - CrCl_original) / CrCl_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "Mice")
  
  # Match imputed with original data and compute concordance for PPI
  PPI_conc <- imputed_PPI %>%
    inner_join(final_data %>% select(ID, PPI), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(PPI_imputed = PPI.x, PPI_original = PPI.y,
           ConcordancePPI = PPI_imputed == PPI_original,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "Mice")
  UM_conc <- imputed_UM %>%
    inner_join(final_data %>% select(ID, UM), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(UM_imputed = UM.x, UM_original = UM.y,
           ConcordanceUM = UM_imputed == UM_original,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "Mice")  


  # Store the results in the lists
  WT_errors[[i]] <- WT_error
  CrCl_errors[[i]] <- CrCl_error
  PPI_concordance[[i]] <- PPI_conc
  UM_concordance[[i]] <- UM_conc
}

# Combine the results into single dataframes for WT, CrCl, and PPI
WT_errors_combined <- bind_rows(WT_errors)
CrCl_errors_combined <- bind_rows(CrCl_errors)
PPI_concordance_combined <- bind_rows(PPI_concordance)
UM_concordance_combined <- bind_rows(UM_concordance)

# Combine the dataframes using cbind without joining by ID
mice_combined_data <- cbind(
  WT_errors_combined %>% select(ID, AE_WT, PE_WT, Missing_Values, Imputation_Method),
  CrCl_errors_combined %>% select(AE_CrCl, PE_CrCl),
  PPI_concordance_combined %>% select(ConcordancePPI),
  UM_concordance_combined %>% select(ConcordanceUM)
)

# View the combined dataframe
mice_combined_data









MF_imputed_data_list <- list()  # Initialize an empty list to store the results

for (i in seq_along(missing_data_list)) {
  # Perform the imputation with missForest
  MF_imputed_result <- missForest(missing_data_list[[i]][, selected_columns], 
                                  maxiter = 50, 
                                  ntree = 300, 
                                  replace = TRUE, 
                                  variablewise = TRUE)
  
  # Extract the imputed data (ximp contains the completed dataset)
  MF_imputed_data <- MF_imputed_result[[1]]
  
  # Store the imputed data in the list
  MF_imputed_data_list[[paste0("MF_imputed_data_", i)]] <- MF_imputed_data
  
  # Adjust imputed data to enforce consistency within each ID
  for (id in unique(MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$ID)) {
    idx <- MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$ID == id  # Get all rows for the specific ID
    
    # Calculate the mean for WT and CrCl, and the most frequent value for PPI
    mean_WT <- mean(MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$WT[idx], na.rm = TRUE)
    mean_CrCl <- mean(MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$CrCl[idx], na.rm = TRUE)
    most_frequent_PPI <- as.numeric(names(sort(table(MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$PPI[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1
    most_frequent_UM <- as.numeric(names(sort(table(MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$UM[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1

    # Replace all values within that ID with the calculated values
    MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$WT[idx] <- mean_WT
    MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$CrCl[idx] <- mean_CrCl
    MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$PPI[idx] <- most_frequent_PPI
    MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$UM[idx] <- most_frequent_UM
  }
}




# Create a vector for the missing data percentages
missing_percentages <- c("5%", "20%", "50%", "75%")

# Initialize lists to store results
WT_errors <- list()
CrCl_errors <- list()
PPI_concordance <- list()
UM_concordance <- list()

# Loop through the missing and imputed datasets
for (i in 1:length(missing_data_list)) {
  
  # Extract missing rows from the current missing_data_list
  missing_WT <- missing_data_list[[i]] %>% filter(is.na(WT))
  missing_CrCl <- missing_data_list[[i]] %>% filter(is.na(CrCl))
  missing_PPI <- missing_data_list[[i]] %>% filter(is.na(PPI))
  missing_UM <- missing_data_list[[i]] %>% filter(is.na(UM))
 
  # Extract corresponding imputed data
  imputed_WT <- MF_imputed_data_list[[i]] %>% filter(ID %in% missing_WT$ID)
  imputed_CrCl <- MF_imputed_data_list[[i]] %>% filter(ID %in% missing_CrCl$ID)
  imputed_PPI <- MF_imputed_data_list[[i]] %>% filter(ID %in% missing_PPI$ID)
  imputed_UM <- MF_imputed_data_list[[i]] %>% filter(ID %in% missing_UM$ID)  

  
  # Match imputed with original data and compute errors for WT
  WT_error <- imputed_WT %>%
    inner_join(final_data %>% select(ID, WT), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%  # Keep only first row of each ID
    mutate(WT_imputed = WT.x, WT_original = WT.y,
           AE_WT = abs(WT_imputed - WT_original),
           PE_WT = (WT_imputed - WT_original) / WT_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MF")
  
  # Match imputed with original data and compute errors for CrCl
  CrCl_error <- imputed_CrCl %>%
    inner_join(final_data %>% select(ID, CrCl), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(CrCl_imputed = CrCl.x, CrCl_original = CrCl.y,
           AE_CrCl = abs(CrCl_imputed - CrCl_original),
           PE_CrCl = (CrCl_imputed - CrCl_original) / CrCl_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MF")
  
  # Match imputed with original data and compute concordance for PPI
  PPI_conc <- imputed_PPI %>%
    inner_join(final_data %>% select(ID, PPI), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(PPI_imputed = PPI.x, PPI_original = PPI.y,
           ConcordancePPI = PPI_imputed == PPI_original,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MF")
  UM_conc <- imputed_UM %>%
    inner_join(final_data %>% select(ID, UM), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(UM_imputed = UM.x, UM_original = UM.y,
           ConcordanceUM = UM_imputed == UM_original,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MF")
  
  # Store the results in the lists
  WT_errors[[i]] <- WT_error
  CrCl_errors[[i]] <- CrCl_error
  PPI_concordance[[i]] <- PPI_conc
  UM_concordance[[i]] <- UM_conc
}

# Combine the results into single dataframes for WT, CrCl, and PPI
WT_errors_combined <- bind_rows(WT_errors)
CrCl_errors_combined <- bind_rows(CrCl_errors)
PPI_concordance_combined <- bind_rows(PPI_concordance)
UM_concordance_combined <- bind_rows(UM_concordance)

# Combine the dataframes using cbind without joining by ID
MF_combined_data <- cbind(
  WT_errors_combined %>% select(ID, AE_WT, PE_WT, Missing_Values, Imputation_Method),
  CrCl_errors_combined %>% select(AE_CrCl, PE_CrCl),
  PPI_concordance_combined %>% select(ConcordancePPI),
  UM_concordance_combined %>% select(ConcordanceUM)
)

# View the combined dataframe
MF_combined_data




# Initialize the list to store imputed datasets
MIDAS_imputed_data_list <- list()

# Define binary column
MIDAS_bin <- c('PPI','UM')  # PPI and UM are binary

# Import numpy from Python (via reticulate package if necessary)
np <- import("numpy")

# Loop over each dataset with missing data
for(i in seq_along(missing_data_list)) {
  
  # Replace NAs with NA for the current data frame (rMIDAS handles NA internally)
  missing_data_list[[i]] <- as.data.frame(lapply(missing_data_list[[i]], function(x) {
    ifelse(is.na(x), NA, x)  # Convert NAs for rMIDAS
  }))
  
  # Apply rMIDAS preprocessing steps
  # Ensure DV, time, Height, Alb, etc., are included as predictors
  MIDAS_conv <- rMIDAS::convert(missing_data_list[[i]][, selected_columns], 
                        bin_cols = MIDAS_bin, 
                        minmax_scale = TRUE)  # Normalize for neural network
  
  # Train the rMIDAS model
  MIDAS_train <- rMIDAS::train(MIDAS_conv,
                       training_epochs = 50,           # Number of training epochs
                       layer_structure = c(128,64,32),  # Hidden layers
                       input_drop = 0.5,              # Dropout to prevent overfitting
                       seed = 89)                      # Set a seed for reproducibility
  
  # Perform imputation with the trained model (m=1 for single imputation)
  MIDAS_complete <- rMIDAS::complete(MIDAS_train, m = 1)
  
  # Extract the imputed data
  MIDAS_complete_df <- MIDAS_complete[[1]]

  # Adjust imputed data for consistency within each ID
  for (id in unique(MIDAS_complete_df$ID)) {
    idx <- MIDAS_complete_df$ID == id  # Get all rows for the specific ID
    
    # Calculate mean for WT and CrCl, and the most frequent value for PPI
    mean_WT <- mean(MIDAS_complete_df$WT[idx], na.rm = TRUE)
    mean_CrCl <- mean(MIDAS_complete_df$CrCl[idx], na.rm = TRUE)
    most_frequent_PPI <- as.numeric(names(sort(table(MIDAS_complete_df$PPI[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1
    most_frequent_UM <- as.numeric(names(sort(table(MIDAS_complete_df$UM[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1

    # Replace all values within that ID with the calculated values
    MIDAS_complete_df$WT[idx] <- mean_WT
    MIDAS_complete_df$CrCl[idx] <- mean_CrCl
    MIDAS_complete_df$PPI[idx] <- most_frequent_PPI
    MIDAS_complete_df$UM[idx] <- most_frequent_UM
  }
  
  # Store the adjusted completed data in the list
  MIDAS_imputed_data_list[[i]] <- MIDAS_complete_df
}


# Loop through each data frame in the list
for (i in seq_along(MIDAS_imputed_data_list)) {
  #MIDAS_imputed_data_list[[i]]$ID <- as.integer(MIDAS_imputed_data_list[[i]]$ID)   # Convert ID to integer
  MIDAS_imputed_data_list[[i]]$PPI <- as.numeric(as.character(MIDAS_imputed_data_list[[i]]$PPI)) - 1  # Convert PPI to numeric and reduce by 1
  MIDAS_imputed_data_list[[i]]$UM <- as.numeric(as.character(MIDAS_imputed_data_list[[i]]$UM)) - 1  # Convert UM to numeric and reduce by 1
}






# Create a vector for the missing data percentages
missing_percentages <- c("5%", "20%", "50%", "75%")

# Initialize lists to store results
WT_errors <- list()
CrCl_errors <- list()
PPI_concordance <- list()
UM_concordance <- list()

# Loop through the missing and imputed datasets
for (i in 1:length(missing_data_list)) {
  
  # Extract missing rows from the current missing_data_list
  missing_data <- missing_data_list[[i]]
  
  # Get indices of rows with NA values for WT, CrCl, and PPI
  na_indices_WT <- which(is.na(missing_data$WT))
  na_indices_CrCl <- which(is.na(missing_data$CrCl))
  na_indices_PPI <- which(is.na(missing_data$PPI))
  na_indices_UM <- which(is.na(missing_data$UM))
  
  # Retrieve the corresponding imputed values using the indices
  imputed_WT <- MIDAS_imputed_data_list[[i]][na_indices_WT, ]
  imputed_CrCl <- MIDAS_imputed_data_list[[i]][na_indices_CrCl, ]
  imputed_PPI <- MIDAS_imputed_data_list[[i]][na_indices_PPI, ]
  imputed_UM <- MIDAS_imputed_data_list[[i]][na_indices_UM, ]
  
  # Ensure IDs are unique in the imputed data
  imputed_WT <- imputed_WT %>% distinct(ID, .keep_all = TRUE)
  imputed_CrCl <- imputed_CrCl %>% distinct(ID, .keep_all = TRUE)
  imputed_PPI <- imputed_PPI %>% distinct(ID, .keep_all = TRUE)
  imputed_UM <- imputed_UM %>% distinct(ID, .keep_all = TRUE)

  # Match imputed with original data and compute errors for WT
  WT_error <- imputed_WT %>%
    left_join(final_data %>% select(ID, WT), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(WT_imputed = WT.x, WT_original = WT.y,
           AE_WT = abs(WT_imputed - WT_original),
           PE_WT = (WT_imputed - WT_original) / WT_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MIDAS")
  
  # Match imputed with original data and compute errors for CrCl
  CrCl_error <- imputed_CrCl %>%
    left_join(final_data %>% select(ID, CrCl), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(CrCl_imputed = CrCl.x, CrCl_original = CrCl.y,
           AE_CrCl = abs(CrCl_imputed - CrCl_original),
           PE_CrCl = (CrCl_imputed - CrCl_original) / CrCl_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MIDAS")

  # Match imputed with original data and compute concordance for PPI
PPI_conc <- imputed_PPI %>%
  left_join(final_data %>% select(ID, PPI), by = "ID") %>%
  distinct(ID, .keep_all = TRUE) %>%
  mutate(PPI_imputed = PPI.x, PPI_original = PPI.y,  # Adjust this line based on the column names
         ConcordancePPI = PPI_imputed == PPI_original,
         Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
         Imputation_Method = "MIDAS")
UM_conc <- imputed_UM %>%
    left_join(final_data %>% select(ID, UM), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(UM_imputed = UM.x, UM_original = UM.y,
           ConcordanceUM = UM_imputed == UM_original,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MIDAS")

  # Store the results in the lists
  WT_errors[[i]] <- WT_error
  CrCl_errors[[i]] <- CrCl_error
  PPI_concordance[[i]] <- PPI_conc
  UM_concordance[[i]] <- UM_conc
}

# Combine the results into single dataframes for WT, CrCl, and PPI
WT_errors_combined <- bind_rows(WT_errors)
CrCl_errors_combined <- bind_rows(CrCl_errors)
PPI_concordance_combined <- bind_rows(PPI_concordance)
UM_concordance_combined <- bind_rows(UM_concordance)

# Combine the dataframes using cbind without joining by ID
MIDAS_combined_data <- cbind(
  WT_errors_combined %>% select(ID, AE_WT, PE_WT, Missing_Values, Imputation_Method),
  CrCl_errors_combined %>% select(AE_CrCl, PE_CrCl),
  PPI_concordance_combined %>% select(ConcordancePPI),
  UM_concordance_combined %>% select(ConcordanceUM)
)

# View the combined dataframe
MIDAS_combined_data




for (i in seq_along(missing_data_list)) {
  missing_data_list[[i]]$PPI <- as.numeric(as.character(missing_data_list[[i]]$PPI)) - 1  # Convert PPI to numeric and reduce by 1
  missing_data_list[[i]]$UM <- as.numeric(as.character(missing_data_list[[i]]$UM)) - 1  # Convert UM to numeric and reduce by 1
}







# Initialize an empty list to store the imputed datasets
amelia_imputed_data_list <- list()

for (i in seq_along(missing_data_list)) {
    # Perform the imputation with Amelia
    amelia_result <- amelia(missing_data_list[[i]][, selected_columns], 
                            m = 1,     # Generate 1 imputed datasets for more robust results
                            idvars = "ID",  # Treat 'ID' as an identifier
                            noms = c("PPI","UM"),   # Treat 'PPI' as a categorical variable
                            ts = "time",    # Specify time-series structure
                            tolerance = 1e-05,    # Stricter convergence criteria
                            empri = 0.01,    # Stricter convergence criteria
                            polytime = 3)   # Capture linear time effects
    
    # Extract the imputed data (taking the first imputed dataset here)
    amelia_imputed_data <- amelia_result$imputations[[1]]  # Extract one imputed dataset
    
    # Store the imputed data in the list
    amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]] <- amelia_imputed_data
    
    # (Optional: Post-processing to ensure consistency within each ID)
    for (id in unique(amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$ID)) {
        idx <- amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$ID == id
        
        # Calculate the mean for WT and CrCl, and the most frequent value for PPI
        mean_WT <- mean(amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$WT[idx], na.rm = TRUE)
        mean_CrCl <- mean(amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$CrCl[idx], na.rm = TRUE)
        most_frequent_PPI <- as.numeric(names(sort(table(amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$PPI[idx]), decreasing = TRUE)[1]))
        most_frequent_UM <- as.numeric(names(sort(table(amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$UM[idx]), decreasing = TRUE)[1]))
        
        # Replace all values within that ID with the calculated values
        amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$WT[idx] <- mean_WT
        amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$CrCl[idx] <- mean_CrCl
        amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$PPI[idx] <- most_frequent_PPI
        amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$UM[idx] <- most_frequent_UM
    }
}



# Loop through each data frame in the list
for (i in seq_along(amelia_imputed_data_list)) {
  #amelia_imputed_data_list[[i]]$ID <- as.integer(amelia_imputed_data_list[[i]]$ID)   # Convert ID to integer
  amelia_imputed_data_list[[i]]$PPI <- as.numeric(as.character(amelia_imputed_data_list[[i]]$PPI))   # Convert PPI to numeric and reduce by 1
  amelia_imputed_data_list[[i]]$UM <- as.numeric(as.character(amelia_imputed_data_list[[i]]$UM))   # Convert UM to numeric and reduce by 1
}


# Create a vector for the missing data percentages
missing_percentages <- c("5%", "20%", "50%", "75%")

# Initialize lists to store results
WT_errors <- list()
CrCl_errors <- list()
PPI_concordance <- list()
UM_concordance <- list()

# Loop through the missing and imputed datasets
for (i in 1:length(missing_data_list)) {
  
  # Extract missing rows from the current missing_data_list
  missing_data <- missing_data_list[[i]]
  
  # Get indices of rows with NA values for WT, CrCl, and PPI
  na_indices_WT <- which(is.na(missing_data$WT))
  na_indices_CrCl <- which(is.na(missing_data$CrCl))
  na_indices_PPI <- which(is.na(missing_data$PPI))
  na_indices_UM <- which(is.na(missing_data$UM))  

  # Retrieve the corresponding imputed values using the indices
  imputed_WT <- amelia_imputed_data_list[[i]][na_indices_WT, ]
  imputed_CrCl <- amelia_imputed_data_list[[i]][na_indices_CrCl, ]
  imputed_PPI <- amelia_imputed_data_list[[i]][na_indices_PPI, ]
  imputed_UM <- amelia_imputed_data_list[[i]][na_indices_UM, ]
  
  # Ensure IDs are unique in the imputed data
  imputed_WT <- imputed_WT %>% distinct(ID, .keep_all = TRUE)
  imputed_CrCl <- imputed_CrCl %>% distinct(ID, .keep_all = TRUE)
  imputed_PPI <- imputed_PPI %>% distinct(ID, .keep_all = TRUE)
  imputed_UM <- imputed_UM %>% distinct(ID, .keep_all = TRUE)

  # Match imputed with original data and compute errors for WT
  WT_error <- imputed_WT %>%
    left_join(final_data %>% select(ID, WT), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(WT_imputed = WT.x, WT_original = WT.y,
           AE_WT = abs(WT_imputed - WT_original),
           PE_WT = (WT_imputed - WT_original) / WT_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "amelia")
  
  # Match imputed with original data and compute errors for CrCl
  CrCl_error <- imputed_CrCl %>%
    left_join(final_data %>% select(ID, CrCl), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(CrCl_imputed = CrCl.x, CrCl_original = CrCl.y,
           AE_CrCl = abs(CrCl_imputed - CrCl_original),
           PE_CrCl = (CrCl_imputed - CrCl_original) / CrCl_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "amelia");

  # Match imputed with original data and compute concordance for PPI
PPI_conc <- imputed_PPI %>%
  left_join(final_data %>% select(ID, PPI), by = "ID") %>%
  distinct(ID, .keep_all = TRUE) %>%
  mutate(PPI_imputed = PPI.x, PPI_original = PPI.y,  # Adjust this line based on the column names
         ConcordancePPI = PPI_imputed == PPI_original,
         Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
         Imputation_Method = "amelia")
UM_conc <- imputed_UM %>%
  left_join(final_data %>% select(ID, UM), by = "ID") %>%
  distinct(ID, .keep_all = TRUE) %>%
  mutate(UM_imputed = UM.x, UM_original = UM.y,  # Adjust this line based on the column names
         ConcordanceUM = UM_imputed == UM_original,
         Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
         Imputation_Method = "amelia")

  # Store the results in the lists
  WT_errors[[i]] <- WT_error
  CrCl_errors[[i]] <- CrCl_error
  PPI_concordance[[i]] <- PPI_conc
  UM_concordance[[i]] <- UM_conc
}

# Combine the results into single dataframes for WT, CrCl, and PPI
WT_errors_combined <- bind_rows(WT_errors)
CrCl_errors_combined <- bind_rows(CrCl_errors)
PPI_concordance_combined <- bind_rows(PPI_concordance)
UM_concordance_combined <- bind_rows(UM_concordance)

# Combine the dataframes using cbind without joining by ID
amelia_combined_data <- cbind(
  WT_errors_combined %>% select(ID, AE_WT, PE_WT, Missing_Values, Imputation_Method),
  CrCl_errors_combined %>% select(AE_CrCl, PE_CrCl),
  PPI_concordance_combined %>% select(ConcordancePPI),
  UM_concordance_combined %>% select(ConcordanceUM)
)

# View the combined dataframe
amelia_combined_data










# Initialize the list to store imputed datasets
XGB_imputed_data_list <- list()

# Set parameters for XGBoost
params_continuous <- list(
  booster = "gbtree",
  objective = "reg:squarederror",  # For continuous variables (WT, CrCl)
  eval_metric = "rmse",
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

params_binary <- list(
  booster = "gbtree",
  objective = "binary:logistic",  # For binary variable (PPI)
  eval_metric = "logloss",
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Helper function to scale continuous variables between 0 and 1
scale_data <- function(data) {
  scaled_data <- as.data.frame(scale(data))
  attr(scaled_data, "scaled:center") <- attr(scale(data), "scaled:center")
  attr(scaled_data, "scaled:scale") <- attr(scale(data), "scaled:scale")
  return(scaled_data)
}

# Helper function to reverse scaling
reverse_scale <- function(scaled_data, original_data) {
  scaled_center <- attr(scaled_data, "scaled:center")
  scaled_scale <- attr(scaled_data, "scaled:scale")
  original_data <- (scaled_data * scaled_scale) + scaled_center
  return(original_data)
}

# Function to impute continuous variables using XGBoost
impute_xgboost_continuous <- function(data, target_column, params) {
  complete_cases <- !is.na(data[[target_column]])
  incomplete_cases <- is.na(data[[target_column]])
  
  # Split data into predictors (features) and target (response)
  x_train <- data[complete_cases, !(names(data) %in% c(target_column))]
  y_train <- data[complete_cases, target_column]
  
  x_test <- data[incomplete_cases, !(names(data) %in% c(target_column))]

  # Scale continuous data for training and testing
  x_train_scaled <- scale_data(x_train)
  x_test_scaled <- scale_data(x_test)
  
  # Convert to XGBoost matrix format
  dtrain <- xgb.DMatrix(data = as.matrix(x_train_scaled), label = y_train)
  dtest <- xgb.DMatrix(data = as.matrix(x_test_scaled))
  
  # Cross-validation to determine the best number of rounds
  cv <- xgb.cv(
    params = params, 
    data = dtrain, 
    nrounds = 1000, 
    nfold = 5, 
    early_stopping_rounds = 10, 
    print_every_n = 10
  )
  best_nrounds <- cv$best_iteration
  
  # Train the final model using best number of rounds
  model <- xgb.train(params = params, data = dtrain, nrounds = best_nrounds)
  
  # Predict missing values
  predictions <- predict(model, dtest)
  
  # Fill in the missing values with predictions
  data[incomplete_cases, target_column] <- predictions
  
  return(data)
}

# Function to impute binary variables using XGBoost
impute_xgboost_binary <- function(data, target_column, params) {
  complete_cases <- !is.na(data[[target_column]])
  incomplete_cases <- is.na(data[[target_column]])
  
  # Split data into predictors (features) and target (response)
  x_train <- data[complete_cases, !(names(data) %in% c(target_column))]
  y_train <- data[complete_cases, target_column]
  
  x_test <- data[incomplete_cases, !(names(data) %in% c(target_column))]
  
  # Convert to XGBoost matrix format
  dtrain <- xgb.DMatrix(data = as.matrix(x_train), label = y_train)
  dtest <- xgb.DMatrix(data = as.matrix(x_test))
  
  # Cross-validation to determine the best number of rounds
  cv <- xgb.cv(
    params = params, 
    data = dtrain, 
    nrounds = 1000, 
    nfold = 5, 
    early_stopping_rounds = 10, 
    print_every_n = 10
  )
  best_nrounds <- cv$best_iteration
  
  # Train the final model using best number of rounds
  model <- xgb.train(params = params, data = dtrain, nrounds = best_nrounds)
  
  # Predict missing values
  predictions <- predict(model, dtest)
  
  # Threshold predictions at 0.5 to determine class labels (binary classification)
  data[incomplete_cases, target_column] <- ifelse(predictions > 0.5, 1, 0)
  
  return(data)
}

# Loop through each dataset with missing data
for (i in seq_along(missing_data_list)) {
  
  # Make a copy of the dataset
  data <- missing_data_list[[i]][, selected_columns]
  
  # Impute continuous variables (WT and CrCl)
  data <- impute_xgboost_continuous(data, "WT", params_continuous)
  data <- impute_xgboost_continuous(data, "CrCl", params_continuous)
  
  # Impute binary variable (PPI)
  data <- impute_xgboost_binary(data, "PPI", params_binary)
  data <- impute_xgboost_binary(data, "UM", params_binary)  

  # Store the imputed dataset
  XGB_imputed_data_list[[paste0("XGB_imputed_data_", i)]] <- data
  
  # Optional: Post-process to ensure consistency within each ID (like in your original script)
  for (id in unique(data$ID)) {
    idx <- data$ID == id  # Get all rows for the specific ID
    
    # Calculate mean for WT and CrCl, and the most frequent value for PPI
    mean_WT <- mean(data$WT[idx], na.rm = TRUE)
    mean_CrCl <- mean(data$CrCl[idx], na.rm = TRUE)
    most_frequent_PPI <- as.numeric(names(sort(table(data$PPI[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1
    most_frequent_UM <- as.numeric(names(sort(table(data$UM[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1
    
    # Replace all values within that ID with the calculated values
    data$WT[idx] <- mean_WT
    data$CrCl[idx] <- mean_CrCl
    data$PPI[idx] <- most_frequent_PPI
    data$UM[idx] <- most_frequent_UM
  }
  
  # Store the adjusted data back into the list
  XGB_imputed_data_list[[paste0("XGB_imputed_data_", i)]] <- data
}



# Create a vector for the missing data percentages
missing_percentages <- c("5%", "20%", "50%", "75%")

# Initialize lists to store results
WT_errors <- list()
CrCl_errors <- list()
PPI_concordance <- list()
UM_concordance <- list()

# Loop through the missing and imputed datasets
for (i in 1:length(missing_data_list)) {
  
  # Extract missing rows from the current missing_data_list
  missing_data <- missing_data_list[[i]]
  
  # Get indices of rows with NA values for WT, CrCl, and PPI
  na_indices_WT <- which(is.na(missing_data$WT))
  na_indices_CrCl <- which(is.na(missing_data$CrCl))
  na_indices_PPI <- which(is.na(missing_data$PPI))
  na_indices_UM <- which(is.na(missing_data$UM))
  
  # Retrieve the corresponding imputed values using the indices
  imputed_WT <- XGB_imputed_data_list[[i]][na_indices_WT, ]
  imputed_CrCl <- XGB_imputed_data_list[[i]][na_indices_CrCl, ]
  imputed_PPI <- XGB_imputed_data_list[[i]][na_indices_PPI, ]
  imputed_UM <- XGB_imputed_data_list[[i]][na_indices_UM, ]
  
  # Ensure IDs are unique in the imputed data
  imputed_WT <- imputed_WT %>% distinct(ID, .keep_all = TRUE)
  imputed_CrCl <- imputed_CrCl %>% distinct(ID, .keep_all = TRUE)
  imputed_PPI <- imputed_PPI %>% distinct(ID, .keep_all = TRUE)
  imputed_UM <- imputed_UM %>% distinct(ID, .keep_all = TRUE)

  # Match imputed with original data and compute errors for WT
  WT_error <- imputed_WT %>%
    left_join(final_data %>% select(ID, WT), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(WT_imputed = WT.x, WT_original = WT.y,
           AE_WT = abs(WT_imputed - WT_original),
           PE_WT = (WT_imputed - WT_original) / WT_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "XGB")
  
  # Match imputed with original data and compute errors for CrCl
  CrCl_error <- imputed_CrCl %>%
    left_join(final_data %>% select(ID, CrCl), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(CrCl_imputed = CrCl.x, CrCl_original = CrCl.y,
           AE_CrCl = abs(CrCl_imputed - CrCl_original),
           PE_CrCl = (CrCl_imputed - CrCl_original) / CrCl_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "XGB");

  # Match imputed with original data and compute concordance for PPI
PPI_conc <- imputed_PPI %>%
  left_join(final_data %>% select(ID, PPI), by = "ID") %>%
  distinct(ID, .keep_all = TRUE) %>%
  mutate(PPI_imputed = PPI.x, PPI_original = PPI.y,  # Adjust this line based on the column names
         ConcordancePPI = PPI_imputed == PPI_original,
         Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
         Imputation_Method = "XGB")
UM_conc <- imputed_UM %>%
  left_join(final_data %>% select(ID, UM), by = "ID") %>%
  distinct(ID, .keep_all = TRUE) %>%
  mutate(UM_imputed = UM.x, UM_original = UM.y,  # Adjust this line based on the column names
         ConcordanceUM = UM_imputed == UM_original,
         Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
         Imputation_Method = "XGB")
;

  # Store the results in the lists
  WT_errors[[i]] <- WT_error
  CrCl_errors[[i]] <- CrCl_error
  PPI_concordance[[i]] <- PPI_conc
  UM_concordance[[i]] <- UM_conc
}

# Combine the results into single dataframes for WT, CrCl, and PPI
WT_errors_combined <- bind_rows(WT_errors)
CrCl_errors_combined <- bind_rows(CrCl_errors)
PPI_concordance_combined <- bind_rows(PPI_concordance)
UM_concordance_combined <- bind_rows(UM_concordance)

# Combine the dataframes using cbind without joining by ID
XGB_combined_data <- cbind(
  WT_errors_combined %>% select(ID, AE_WT, PE_WT, Missing_Values, Imputation_Method),
  CrCl_errors_combined %>% select(AE_CrCl, PE_CrCl),
  PPI_concordance_combined %>% select(ConcordancePPI),
  UM_concordance_combined %>% select(ConcordanceUM)
)

# View the combined dataframe
XGB_combined_data

# Define the columns of interest
columns_of_interest <- c("Height", "WT", "Alb", "CrCl", "Age", "PPI", "UM")

# Define the percentages of missing data
missing_percentages <- c("5% NA", "20% NA", "50% NA", "75% NA")

# Combine all datasets into a list
all_datasets <- list(
  sim_data,  # Original simulated data
  
  # MICE imputed datasets
  mice_imputed_data_list[[1]],
  mice_imputed_data_list[[2]],
  mice_imputed_data_list[[3]],
  mice_imputed_data_list[[4]],
  
  # AMELIA imputed datasets
  amelia_imputed_data_list[[1]],
  amelia_imputed_data_list[[2]],
  amelia_imputed_data_list[[3]],
  amelia_imputed_data_list[[4]],
  
  # MF imputed datasets
  MF_imputed_data_list[[1]],
  MF_imputed_data_list[[2]],
  MF_imputed_data_list[[3]],
  MF_imputed_data_list[[4]],
  
  # XGB imputed datasets
  XGB_imputed_data_list[[1]],
  XGB_imputed_data_list[[2]],
  XGB_imputed_data_list[[3]],
  XGB_imputed_data_list[[4]],
  
  # MIDAS imputed datasets
  MIDAS_imputed_data_list[[1]],
  MIDAS_imputed_data_list[[2]],
  MIDAS_imputed_data_list[[3]],
  MIDAS_imputed_data_list[[4]]
)



missing_percentages2 <- gsub(" NA", "", missing_percentages)


# Assign names to datasets
dataset_names <- c(
  "Original Data",
  
  # Mice datasets with missing percentages
  paste0("Mice Missing Percentage: ", missing_percentages2),
  
  # Amelia datasets
  paste0("Amelia Missing Percentage: ", missing_percentages2),
  
  # MF datasets
  paste0("MF Missing Percentage: ", missing_percentages2),
  
  # XGB datasets
  paste0("XGB Missing Percentage: ", missing_percentages2),
  
  # MIDAS datasets
  paste0("MIDAS Missing Percentage: ", missing_percentages2)
)



# Ensure that the number of datasets matches the number of names
if (length(all_datasets) != length(dataset_names)) {
  stop("The number of datasets and dataset names do not match.")
}

# Initialize an empty list to store plots
plot_list <- list()

# Loop over each dataset
for (i in seq_along(all_datasets)) {
  data <- all_datasets[[i]]
  
  # Check if all columns of interest are present in the dataset
  missing_cols <- setdiff(columns_of_interest, names(data))
  if (length(missing_cols) > 0) {
    warning(paste("Dataset", dataset_names[i], "is missing columns:", paste(missing_cols, collapse = ", ")))
    next
  }
  
  # Select columns of interest
  data_subset <- data[, columns_of_interest]
  
  # Convert all columns to numeric
  data_subset <- data_subset %>% mutate_all(~ as.numeric(as.character(.)))
  
  # Remove rows with NAs
  data_subset <- na.omit(data_subset)
  if (nrow(data_subset) == 0) {
    warning(paste("Dataset", dataset_names[i], "has no complete cases. Skipping."))
    next
  }
  
  # Compute correlation matrix
  corr_matrix <- cor(data_subset, use = "complete.obs", method = "pearson")
  
  # Generate the correlation plot using ggcorrplot with labels
  p <- ggcorrplot(corr_matrix,
                  type = "upper",
                  lab = TRUE,        # Include correlation coefficients
                  lab_size = 3,      # Adjust label size for readability
                  title = dataset_names[i],
                  show.legend = FALSE,
                  ggtheme = my_theme,
                  colors = c("blue", "white", "red"))  # Negative correlations in blue, positive in red
  
  # Adjust plot theme for readability
  p <- p + theme(
    plot.title = element_text(size = 10, hjust = 0.5),
    axis.text = element_text(size = 6)
  )
  
  # Add the plot to the list
  plot_list[[i]] <- p
}



plot_indices <- c(
  1, 6, 7, 8, 9,
  1, 10, 11, 12, 13,1, 2, 3, 4, 5,1, 18, 19, 20, 21, 1, 14, 15,
  16, 17
)


# Arrange plots using ggarrange
# Since we now have 21 datasets, we'll arrange them in 7 columns and 3 rows
combined_plot <- ggarrange(
  plotlist = plot_list[plot_indices],
  ncol = 5, nrow = 5,
  labels = NULL
)




# Save the combined plot to a PNG file using png() and dev.off()
# Set the output image size and resolution
png("Correlation plot.png", width = 16, height = 16, units = 'in', res = 900)

# Draw the combined plot
print(combined_plot)

# Close the graphics device
dev.off()





# Assuming you have the correlation matrices stored or you can compute them again
corr_matrices <- list()

# Compute correlation matrices for all datasets
for (i in seq_along(all_datasets)) {
  data <- all_datasets[[i]]
  data_subset <- data[, columns_of_interest]
  
  # Convert all columns to numeric
  data_subset <- data_subset %>% mutate_all(~ as.numeric(as.character(.)))
  
  # Remove rows with NAs
  data_subset <- na.omit(data_subset)
  
  # Compute correlation matrix
  corr_matrix <- cor(data_subset, use = "complete.obs", method = "pearson")
  
  # Store the correlation matrix
  corr_matrices[[i]] <- corr_matrix
}

# Assign names to the correlation matrices
names(corr_matrices) <- dataset_names




# Extract the original correlation matrix
corr_original <- corr_matrices[["Original Data"]]

# Initialize a data frame to store the results
frobenius_results <- data.frame(
  Dataset = character(),
  Frobenius_Norm = numeric(),
  stringsAsFactors = FALSE
)

# Loop through the imputed datasets
for (i in seq_along(corr_matrices)) {
  dataset_name <- names(corr_matrices)[i]
  
  # Skip the original dataset
  if (dataset_name == "Final Data") next
  
  corr_matrix <- corr_matrices[[i]]
  
  # Compute the Frobenius norm of the difference
  diff_matrix <- corr_matrix - corr_original
  frob_norm <- norm(diff_matrix, type = "F")
  
  # Store the result
  frobenius_results <- rbind(frobenius_results, data.frame(
    Dataset = dataset_name,
    Frobenius_Norm = frob_norm
  ))
}

# Display the results sorted by Frobenius Norm
frobenius_results <- frobenius_results %>% arrange(Frobenius_Norm)
print(frobenius_results)




# Initialize a data frame to store the results
rv_results <- data.frame(
    Dataset = character(),
    RV_Coefficient = numeric(),
    stringsAsFactors = FALSE
)

# Loop through the imputed datasets
for (i in seq_along(corr_matrices)) {
    dataset_name <- names(corr_matrices)[i]
    
    # Skip the original dataset
    if (dataset_name == "Final Data") next
    
    corr_matrix <- corr_matrices[[i]]
    
    # Compute the RV coefficient
    rv_obj <- coeffRV(corr_original, corr_matrix)
    
    # Extract the RV coefficient value
    rv_value <- rv_obj$rv
    
    # Store the result
    rv_results <- rbind(rv_results, data.frame(
        Dataset = dataset_name,
        RV_Coefficient = rv_value
    ))
}

# Display the results sorted by RV Coefficient (descending)
rv_results <- rv_results %>% arrange(desc(RV_Coefficient))
print(rv_results)


# Frobenius Norm Plot
plot_frobenius <- ggplot(frobenius_results, aes(x = reorder(Dataset, Frobenius_Norm), y = Frobenius_Norm)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(title = "Frobenius Norm of Correlation Matrix Differences",
       x = "Dataset",
       y = "Frobenius Norm") +
  my_theme


# RV Coefficient Plot
plot_rv <- ggplot(rv_results, aes(x = reorder(Dataset, -RV_Coefficient), y = RV_Coefficient)) +
  geom_bar(stat = "identity", fill = "salmon") +
  coord_flip() +
  labs(title = "RV Coefficient between Correlation Matrices",
       x = "Dataset",
       y = "RV Coefficient") +
  my_theme


combined_plot <- ggarrange(
  plot_frobenius,
  plot_rv,
  ncol = 1, nrow = 2,
  labels = c("A", "B")
)


png(filename = "Combined_Metrics_Corr_Plot.png",
    width = 10,    # Adjust width as needed
    height = 12,   # Adjust height as needed
    units = "in",
    res = 300)

# Draw the combined plot
print(combined_plot)

# Close the graphics device
dev.off()







# Define the imputation methods
methods <- c('Mice', 'Amelia', 'MF', 'XGB', 'MIDAS')

# Define the missing percentages
missing_percentages <- c('Missing Percentage: 0%', 'Missing Percentage: 5%', 'Missing Percentage: 20%', 'Missing Percentage: 50%', 'Missing Percentage: 75%')

# Extract 'final_data' from the list
final_data <- all_datasets[[1]]  # Assuming 'final_data' is the first in the list

# Replicate 'final_data' for each method
final_data_replicates <- lapply(methods, function(method) {
  df <- final_data
  df$Imputation_Method <- method
  df$Missing_Percentage <- 'Original Data'
  # Keep only the 'WT', 'CrCl', 'ID', 'Imputation_Method', 'Missing_Percentage' columns
  df <- df[, c('WT', 'CrCl', 'ID', 'Imputation_Method', 'Missing_Percentage')]
  return(df)
})

# Name the replicated data frames
names(final_data_replicates) <- methods

# Now, process each imputed data frame in 'all_datasets' (excluding 'final_data')
for (i in 2:length(all_datasets)) {  # Start from 2 to skip 'final_data'
  # Get the data frame
  df <- all_datasets[[i]]
  
  # Keep only the first row for each ID
  df <- df[!duplicated(df$ID), ]
  
  # Extract the method and missing percentage from the list index
  method_index <- ((i - 2) %/% 4) + 1  # Determine method index
  method <- methods[method_index]
  missing_pct <- missing_percentages[(i - 2) %% 4 + 2]  # Get missing percentage
  
  # Add the 'Imputation_Method' and 'Missing_Percentage' columns
  df$Imputation_Method <- method
  df$Missing_Percentage <- missing_pct
  
  # Keep only the 'WT', 'CrCl', 'ID', 'Imputation_Method', 'Missing_Percentage' columns
  df <- df[, c('WT', 'CrCl', 'ID', 'Imputation_Method', 'Missing_Percentage')]
  
  # Update the data frame in the list
  all_datasets[[i]] <- df
}

# Combine the replicated 'final_data' data frames with the imputed data frames
# Exclude the original 'final_data' from 'all_datasets' since we have its replicates
combined_datasets <- c(final_data_replicates, all_datasets[-1])

# Now, create the long-format data frame
long_df <- do.call(rbind, combined_datasets)

# Reset row names to avoid duplicates
rownames(long_df) <- NULL

# View the first few rows of the long-format data frame
head(long_df)
long_df$Missing_Percentage <- factor(long_df$Missing_Percentage,
  levels = c("Original Data", 
             "Missing Percentage: 5%", 
             "Missing Percentage: 20%", 
             "Missing Percentage: 50%", 
             "Missing Percentage: 75%")
)

# Define the plotting function
plot_histogram_with_gaussian_and_density_fixed <- function(data, variable, variable_name) {
    # Determine binwidth based on the variable
    binwidth_value <- ifelse(variable == "WT", 4, 5)  # Adjust as needed
    
    # Compute group statistics (mean and standard deviation)
    group_stats <- data %>%
        group_by(Imputation_Method, Missing_Percentage) %>%
        summarize(
            mean_value = mean(.data[[variable]], na.rm = TRUE),
            sd_value = sd(.data[[variable]], na.rm = TRUE)
        )
    
    # Create a grid of x values for the theoretical curves
    x_min <- min(data[[variable]], na.rm = TRUE)
    x_max <- max(data[[variable]], na.rm = TRUE)
    x_values <- seq(x_min, x_max, length.out = 200)
    
    # Create data frames for the Gaussian curves
    gaussian_data <- group_stats %>%
        rowwise() %>%
        do({
            data.frame(
                Imputation_Method = .$Imputation_Method,
                Missing_Percentage = .$Missing_Percentage,
                x = x_values,
                y = dnorm(x_values, mean = .$mean_value, sd = .$sd_value)
            )
        })
    
    # Create the plot
    plot <- ggplot(data, aes_string(x = variable)) +
        # Histogram
        geom_histogram(aes(y = ..density.., fill = Imputation_Method),
                       binwidth = binwidth_value, color = "black", alpha = 0.7, show.legend = TRUE) +
        # Density curve
        geom_density(aes(y = ..density.., color = "Density Estimate", linetype = "Density Estimate"),
                     size = 1, alpha = 0.5, show.legend = TRUE) +
        # Gaussian curve
        geom_line(data = gaussian_data,
                  aes(x = x, y = y, color = "Gaussian Distribution", linetype = "Gaussian Distribution"),
                  size = 1, show.legend = TRUE) +
        # Facet by Imputation_Method and Missing_Percentage
        facet_grid(Imputation_Method ~ Missing_Percentage, scales = "free_y") +
        # Add the color palette from ggsci
        scale_fill_jco() +
        # Customize colors and linetypes for the legend
        scale_color_manual(name = "Legend", values = c("Density Estimate" = "red", "Gaussian Distribution" = "blue")) +
        scale_linetype_manual(name = "Legend", values = c("Density Estimate" = "solid", "Gaussian Distribution" = "solid")) +
        # Labels and titles
        labs(title = paste("Distribution of", variable_name),
             x = variable_name,
             y = "Density") +
        # Custom theme settings
        theme_bw() +
        theme(
            plot.title = element_text(hjust = 0.5, face = "bold", size = 18),
            axis.title = element_text(size = 16),
            axis.text = element_text(size = 14),
            strip.text = element_text(face = "bold", size = 14),
            panel.spacing = unit(1, "lines"),
            legend.position = "top",
            legend.key = element_rect(fill = "white", color = "black"),
            legend.title = element_text(face = "bold", size = 14),
            legend.text = element_text(size = 12),
            panel.border = element_rect(color = "grey", fill = NA, size = 1),
            strip.background = element_rect(fill = "grey", color = "grey", size = 1)
        ) +
        # Adjust legend guides
        guides(
            fill = guide_legend(title = "Imputation Method", override.aes = list(alpha = 1)),
            linetype = guide_legend(title = "Curve Type", override.aes = list(color = c("red", "blue"))),
            color = "none"
        )
    
    # Return the plot
    return(plot)
}




plot_WT <- plot_histogram_with_gaussian_and_density_fixed(long_df, "WT", "Weight")
plot_CrCl <- plot_histogram_with_gaussian_and_density_fixed(long_df, "CrCl", "Creatinine Clearance")


png("WT_CrCl_Distribution.png", width = 16, height = 24, units = 'in', res = 900)
ggarrange(plot_WT,plot_CrCl,nrow=2,common.legend=TRUE)
dev.off()







# Ensure the column names are consistent (optional, depending on your specific case)
# If necessary, rename columns in one or more data frames to ensure consistency.

# Merging the data frames by rows
combined_data <- bind_rows(
  MIDAS_combined_data %>% mutate(Imputation_Method = "MIDAS"),
  MF_combined_data %>% mutate(Imputation_Method = "MF"),
  mice_combined_data %>% mutate(Imputation_Method = "Mice"),
  amelia_combined_data %>% mutate(Imputation_Method = "amelia"),
  XGB_combined_data %>% mutate(Imputation_Method = "XGB"),
)



# Compute summary statistics
summary_stats <- combined_data %>%
  group_by(Missing_Values, Imputation_Method) %>%
  summarise(
    # Absolute Error with Standard Deviation
    AE_WT = sprintf("%.2f ± %.2f", mean(AE_WT, na.rm = TRUE), sd(AE_WT, na.rm = TRUE)),
    AE_CrCl = sprintf("%.2f ± %.2f", mean(AE_CrCl, na.rm = TRUE), sd(AE_CrCl, na.rm = TRUE)),

    # Percentage Error with Standard Deviation
    PE_WT = sprintf("%.2f ± %.2f", mean(PE_WT, na.rm = TRUE), sd(PE_WT, na.rm = TRUE)),
    PE_CrCl = sprintf("%.2f ± %.2f", mean(PE_CrCl, na.rm = TRUE), sd(PE_CrCl, na.rm = TRUE)),
    
    # Percentage of Concordance
    Concordance_PPI = sprintf("%.2f%%", mean(ConcordancePPI, na.rm = TRUE) * 100),
    Concordance_UM = sprintf("%.2f%%", mean(ConcordanceUM, na.rm = TRUE) * 100)
  ) %>%
  ungroup()

# Ensure Missing_Values is ordered correctly and Imputation_Method sorted alphabetically
summary_stats <- summary_stats %>%
  mutate(
    Missing_Values = factor(
      Missing_Values,
      levels = c("5%", "20%", "50%", "75%")  # Ensure Missing_Values order
    ),
    Imputation_Method = factor(Imputation_Method, levels = sort(unique(Imputation_Method)))  # Alphabetical order
  ) %>%
  arrange(Missing_Values, Imputation_Method)  # Sort by Missing_Values and then Imputation_Method



# Create the flextable
ft <- flextable(summary_stats) %>%
  theme_box() %>%
  set_header_labels(
    Missing_Values = "Missing Percentage",
    Imputation_Method = "Imputation Method",
    AE_WT = "AE (WT) [Mean ± SD]",
    PE_WT = "PE (WT) [Mean ± SD]",
    AE_CrCl = "AE (CrCl) [Mean ± SD]",
    PE_CrCl = "PE (CrCl) [Mean ± SD]",
    Concordance_PPI = "% Concordance (PPI)",
    Concordance_UM = "% Concordance (UM)"
  ) %>%
  merge_v(j = c("Missing_Values", "Imputation_Method")) %>%  # Merge rows first by Missing_Values, then Imputation_Method
  fontsize(size = 10, part = "all") %>%
  font(part = "all", fontname = "Times New Roman") %>%
  bold(part = "header") %>%
  align(align = "center", part = "all") %>%
  width(j = c("Missing_Values", "Imputation_Method"), width = 1.5) %>%
  width(j = c("AE_WT", "PE_WT", "AE_CrCl", "PE_CrCl", "Concordance_PPI", "Concordance_UM"), width = 1.8)


# Create a Word document and add the flextable
doc <- read_docx() %>%
  body_add_par("Summary of AE, PE (WT and CrCl), and Concordance (PPI and UM)", style = "heading 1") %>%
  body_add_flextable(value = ft)

# Save the document
print(doc, target = "Summary_Statistics_with_Concordance.docx")




combined_data <- combined_data %>%
  mutate(
    Imputation_Method = recode(Imputation_Method,
                               "amelia" = "Amelia",
                               "mice" = "Mice")  # Ensure consistent capitalization
  )


# View the combined data frame
print(combined_data)
long_combined_data <- combined_data %>%
  pivot_longer(
    cols = c(AE_WT, PE_WT, AE_CrCl, PE_CrCl), 
    names_to = c("ErrorType", "Variable"), 
    names_sep = "_"
  ) %>%
  mutate(
    ErrorType = factor(
      ErrorType, 
      levels = c("AE", "PE"), 
      labels = c("Absolute Error", "Percentage Error")  # New labels
    )
  )




AEPE_plot<-ggplot(long_combined_data, aes(x = Missing_Values, y = value, fill = Imputation_Method)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA, color = "black", linewidth = 1.2) +  # Thicker boxplot borders
  geom_point(color = "black", position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.75), alpha = 1, size = 0.6, show.legend = FALSE) +  # Black points without legend
  facet_grid(ErrorType ~ Variable, scales = "free_y") +  # Facet by ErrorType in rows and Variable in columns
  labs(title = "Comparison of Absolute and Percentage Error for WT and CrCl by Percentage of Missing Data",
       x = "Missing Percentage",
       y = "Error Value",
       fill = "Imputation Method") +
  scale_color_jco() +  # Color scheme for lines
  scale_fill_jco() +   # Fill color scheme for ribbons
  my_theme +  # Apply your custom theme
  theme(strip.background = element_rect(fill = "grey90", color = "grey50"),  # Grey background and border
        strip.text = element_text(face = "bold", color = "black"))  # Bold text for facet labels


png("AE_PE_comparison.png", width = 12, height = 9, units = 'in', res = 900)  # Specify width, height, and resolution

AEPE_plot

# Close the device
dev.off()







# Merge the five imputed dataset lists into a single list


# Assuming 'combined_data' already contains both PPI and UM variables

combined_data <- combined_data %>%
  rename(PPI = ConcordancePPI, UM = ConcordanceUM)

# Count the number of TRUE/FALSE for PPI, grouped by Imputation_Method and Missing_Values
concordance_summary_PPI <- combined_data %>%
  group_by(Imputation_Method, Missing_Values, PPI) %>%
  summarise(Count = n()) %>%
  arrange(Imputation_Method, Missing_Values)

# Count the number of TRUE/FALSE for UM, grouped by Imputation_Method and Missing_Values
concordance_summary_UM <- combined_data %>%
  group_by(Imputation_Method, Missing_Values, UM) %>%
  summarise(Count = n()) %>%
  arrange(Imputation_Method, Missing_Values)

# Filter out rows with NA in PPI and UM
concordance_summary_PPI_filtered <- concordance_summary_PPI %>% filter(!is.na(PPI))
concordance_summary_UM_filtered <- concordance_summary_UM %>% filter(!is.na(UM))

# Calculate percentages for TRUE/FALSE concordance within each combination of Imputation_Method and Missing_Values
concordance_summary_PPI_pct <- concordance_summary_PPI_filtered %>%
  group_by(Imputation_Method, Missing_Values) %>%
  mutate(Percent = 100 * Count / sum(Count))  # Calculate percentage within each group

concordance_summary_UM_pct <- concordance_summary_UM_filtered %>%
  group_by(Imputation_Method, Missing_Values) %>%
  mutate(Percent = 100 * Count / sum(Count))  # Calculate percentage within each group

# Ensure both TRUE and FALSE categories are present for consistent bar widths
concordance_summary_PPI_pct <- concordance_summary_PPI_pct %>%
  complete(PPI = c(TRUE, FALSE), fill = list(Count = 0, Percent = 0))

concordance_summary_UM_pct <- concordance_summary_UM_pct %>%
  complete(UM = c(TRUE, FALSE), fill = list(Count = 0, Percent = 0))

# Create the bar plot for PPI
PPI_Plot <- ggplot(concordance_summary_PPI_pct, aes(x = Missing_Values, y = Percent, fill = PPI)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), color = "black", linewidth = 1.2) +
  geom_text(aes(label = Count, y = 28), position = position_dodge(width = 0.8), 
            vjust = 0, size = 3.5, fontface = "bold") +
  facet_wrap(~ Imputation_Method, scales = "free", ncol = 2) +
  labs(title = "Concordance for PPI (TRUE/FALSE) by Imputation Method and Missing Data",
       x = "Percentage of Missing Data",
       y = "Frequency (%) of TRUE/FALSE Concordance",
       fill = "Concordance (PPI)") +
  scale_fill_jco() +
  my_theme +
  scale_x_discrete(drop = FALSE) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold"),
    panel.border = element_rect(color = "black", fill = NA, size = 1),
    strip.placement = "outside",
    plot.margin = margin(2, 2, 2, 2, "cm")
  ) +
  my_theme  # Apply your custom theme

# Create the bar plot for UM
UM_Plot <- ggplot(concordance_summary_UM_pct, aes(x = Missing_Values, y = Percent, fill = UM)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), color = "black", linewidth = 1.2) +
  geom_text(aes(label = Count, y = 28), position = position_dodge(width = 0.8), 
            vjust = 0, size = 3.5, fontface = "bold") +
  facet_wrap(~ Imputation_Method, scales = "free", ncol = 2) +
  labs(title = "Concordance for UM (TRUE/FALSE) by Imputation Method and Missing Data",
       x = "Percentage of Missing Data",
       y = "Frequency (%) of TRUE/FALSE Concordance",
       fill = "Concordance (UM)") +
  scale_fill_jco() +
  my_theme +
  scale_x_discrete(drop = FALSE) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold"),
    panel.border = element_rect(color = "black", fill = NA, size = 1),
    strip.placement = "outside",
    plot.margin = margin(2, 2, 2, 2, "cm")
  ) +
  my_theme  # Apply your custom theme

# Save the PPI Plot
png("PPI_comparison.png", width = 12, height = 9, units = 'in', res = 900)
print(PPI_Plot)
dev.off()

# Save the UM Plot
png("UM_comparison.png", width = 12, height = 9, units = 'in', res = 900)
print(UM_Plot)
dev.off()


png("PPI_UM_comparison.png", width = 12, height = 16, units = 'in', res = 900)
ggarrange(print(PPI_Plot),print(UM_Plot),common.legend=TRUE,nrow=2,labels="AUTO")
dev.off()





merged_imputed_data_list <- c(
  mice_imputed_data_list,
  MF_imputed_data_list,
  MIDAS_imputed_data_list,
  amelia_imputed_data_list,
  XGB_imputed_data_list
)

# Append final_data to the merged list
merged_imputed_data_list$final_data <- final_data_mice

# Check the structure of the merged list (including final_data)
str(merged_imputed_data_list)

# Optionally, print the names of the datasets for verification
names(merged_imputed_data_list) <- c(
  paste0("Mice_", seq_along(mice_imputed_data_list)),
  paste0("MF_", seq_along(MF_imputed_data_list)),
  paste0("MIDAS_", seq_along(MIDAS_imputed_data_list)),
  paste0("Amelia_", seq_along(amelia_imputed_data_list)),
  paste0("XGB_", seq_along(XGB_imputed_data_list)),
  "final_data"
)

# Check the first few dataset names in the merged list
head(names(merged_imputed_data_list))





# Define the time points where new rows should be added (dosing times)
dosing_times <- c(0)

# Define the allowed times for rows with amt = 0, evid = 0
allowed_times <- c(0.25,0.5,4,11,12)

# Function to add columns and rows for each dataset and remove unwanted rows
modify_dataset_add_rows <- function(data) {
  # Get unique IDs
  unique_ids <- unique(data$ID)
  
  # Initialize an empty data frame to store the modified data
  expanded_data <- data.frame()
  
  # Loop through each ID and add new rows at dosing times
  for (id in unique_ids) {
    # Subset the data for the current ID
    id_data <- data[data$ID == id, ]
    
    # Create new rows for dosing times by copying the first row of id_data
    new_rows <- id_data[1:length(dosing_times), ]
    
    # Set the time for the new rows
    new_rows$time <- dosing_times
    
    # Assign amt, evid, and cmt for the new rows
    new_rows$amt <- 100
    new_rows$evid <- 1
    new_rows$cmt <- 1
    
    # Set DV to NA for the new rows (as DV doesn't exist at these times)
    new_rows$DV <- NA
    
    # Set amt, evid, and cmt for existing rows in id_data to 0
    id_data$amt <- 0
    id_data$evid <- 0
    id_data$cmt <- 0
    
    # Combine the original rows with the new dosing rows
    combined_data <- rbind(id_data, new_rows)
    
    # Sort the combined data by time
    combined_data <- combined_data[order(combined_data$time), ]
    
    # Remove rows where EVID = 0 and AMT = 0 outside the allowed times
    combined_data <- combined_data[!(combined_data$evid == 0 & combined_data$amt == 0 & !combined_data$time %in% allowed_times), ]
    
      
    # Append to the expanded_data
    expanded_data <- rbind(expanded_data, combined_data)
  }
  
  return(expanded_data)
}

# Apply the function to each dataset in the merged list
for (i in seq_along(merged_imputed_data_list)) {
  merged_imputed_data_list[[i]] <- modify_dataset_add_rows(merged_imputed_data_list[[i]])
}

# Check the result for the first dataset in the merged list
head(merged_imputed_data_list[[1]], 40)


# Identify the last dataframe (which has 18 columns)
last_df <- merged_imputed_data_list[[length(merged_imputed_data_list)]]

# Extract the last 5 columns from the last dataframe
last_5_columns <- last_df[, (ncol(last_df) - 4):ncol(last_df)]

# Loop through the first 20 dataframes and add the last 5 columns
for (i in 1:20) {
  # Add the last 5 columns to each dataframe in the list
  merged_imputed_data_list[[i]] <- cbind(merged_imputed_data_list[[i]], last_5_columns)
}

# Now check the result for one of the first 20 dataframes to ensure the columns have been added
head(merged_imputed_data_list[[1]], 10)


# Define the column order from final_data
final_data_columns <- c("ID", "time", "Height", "WT", "Alb", "CrCl", "Age", "PPI",
                        "UM", "amt", "evid", "cmt", "GUT", "CENTRAL", "DV", "CL", 
                        "VC", "KA")

# Reorder columns in each dataframe in merged_imputed_data_list
for (i in seq_along(merged_imputed_data_list)) {
  # Only reorder columns if all required columns are present in the dataframe
  common_columns <- intersect(final_data_columns, colnames(merged_imputed_data_list[[i]]))
  merged_imputed_data_list[[i]] <- merged_imputed_data_list[[i]][, common_columns, drop = FALSE]
  
  # If there are any missing columns in the current dataframe, add them as NA
  missing_columns <- setdiff(final_data_columns, colnames(merged_imputed_data_list[[i]]))
  for (col in missing_columns) {
    merged_imputed_data_list[[i]][, col] <- NA
  }
  
  # Ensure the final column order matches final_data
  merged_imputed_data_list[[i]] <- merged_imputed_data_list[[i]][, final_data_columns]
}

# Now all dataframes in merged_imputed_data_list should have the same column order as final_data





# Define the base directory by removing the "Figures" part
base_directory <- getwd()

# Define the new folder to store the imputed datasets
imputed_datasets_folder <- file.path(base_directory, "imputed_datasets_merged")

# Create the new folder if it doesn't exist
if (!dir.exists(imputed_datasets_folder)) {
  dir.create(imputed_datasets_folder)
}

# Save each dataframe in merged_imputed_data_list to a CSV file using its name
for (i in seq_along(merged_imputed_data_list)) {
  # Get the name of the current dataframe
  df_name <- names(merged_imputed_data_list)[i]
  
  # Define the file name and path based on the dataframe name
  file_name <- paste0(df_name, ".csv")
  file_path <- file.path(imputed_datasets_folder, file_name)
  
  # Write the dataframe to CSV
  write.csv(merged_imputed_data_list[[i]], file = file_path, row.names = FALSE)
}

# Print a message indicating completion
print("Merged imputed dataframes have been successfully exported into 'imputed_datasets_merged'.")



# Rename percentages for missing data
percentages <- c("0% NA", "5% NA", "20% NA", "50% NA", "75% NA")

# Map of imputation methods to the names used in the dataframe list
method_map <- c("Mice", "MF", "MIDAS", "Amelia", "XGB")

# Initialize an empty list to store the subsetted data
subsetted_data_list <- list()

# Add final_data with 0% missing data, keeping the ID column
final_data <- merged_imputed_data_list[["final_data"]] %>%
  select(ID, WT, CrCl) %>%
  mutate(Imputation_Method = "final", Missing_Percentage = "0% NA") %>%
  distinct(ID, .keep_all = TRUE)  # Keep only the first row of each ID
subsetted_data_list[["final_data"]] <- final_data

# Loop through the imputation methods and percentages of missing data
for (method in method_map) {
    for (i in 1:4) {
        df_name <- paste0(method, "_", i)
        
        # Check if the dataframe exists before processing
        if (!is.null(merged_imputed_data_list[[df_name]])) {
          data <- merged_imputed_data_list[[df_name]] %>%
            select(ID, WT, CrCl) %>%
            mutate(
              Imputation_Method = method,                     # String before "_"
              Missing_Percentage = percentages[i + 1]  # String after "_", skipping "0% NA"
            ) %>%
            distinct(ID, .keep_all = TRUE)  # Keep only the first row of each ID
        
          # Store the result in the list
          subsetted_data_list[[df_name]] <- data
        } else {
          message(paste("Dataframe", df_name, "not found, skipping."))
        }
    }
}

# Combine all subsetted dataframes into a single dataframe
combined_subset_data <- bind_rows(subsetted_data_list)

# View the combined dataframe structure
str(combined_subset_data)

# If you want to save the combined data:
write.csv(combined_subset_data, "combined_subset_data.csv", row.names = FALSE)




# Define file paths
dataFile      <- "C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1/imputed_datasets_merged/final_data.csv"
modelFile     <- "lib:oral1_1cpt_kaVCl.txt"
projects_folder <- "C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1/monolix_projects_OFV"

# Create all possible combinations for the covariate model
# ka: PPI, V: logtWEIGHT, Cl: logtCrCl and UM
combinations <- expand.grid(PPI = c(TRUE, FALSE),
                            logtWEIGHT = c(TRUE, FALSE),
                            logtCrCl = c(TRUE, FALSE),
                            UM = c(TRUE, FALSE),
                            stringsAsFactors = FALSE)

# Loop over each combination
for(i in 1:nrow(combinations)) {
  comb <- combinations[i, ]
  
  # Create a new project from scratch using the data and model file
  newProject(
    data = list(
      dataFile    = dataFile,
      headerTypes = c("id", "time", "ignore", "contcov", "ignore", "contcov", "ignore",
                      "catcov", "catcov", "amount", "evid", "ignore", "ignore", "ignore",
                      "observation", "ignore", "ignore", "ignore"),
      mapping     = NULL
    ),
    modelFile = modelFile
  )
  
  # Set population parameter initial values
  setPopulationParameterInformation(
    Cl_pop   = list(initialValue = 1), 
    V_pop    = list(initialValue = 10),
    ka_pop   = list(initialValue = 1),
    omega_Cl = list(initialValue = 0.15), 
    omega_V  = list(initialValue = 0.3),
    omega_ka = list(initialValue = 0.5),
    a        = list(initialValue = 0.1),
    b        = list(initialValue = 0.15)
  )
  
  # Add continuous transformed covariates
  addContinuousTransformedCovariate(logtWEIGHT = "log(WT/82.58)")
  addContinuousTransformedCovariate(logtCrCl  = "log(CrCl/106.33)")
  
  # Set the covariate model using the current combination:
  #   For ka, set PPI; for V, set logtWEIGHT; for Cl, set logtCrCl and UM.
  setCovariateModel(
    ka = c(PPI = comb$PPI),
    V  = c(logtWEIGHT = comb$logtWEIGHT),
    Cl = c(logtCrCl = comb$logtCrCl, UM = comb$UM)
  )
  
  # Set up the analysis scenario
  scenario <- getScenario()
  scenario$tasks <- c(
    populationParameterEstimation   = TRUE, 
    conditionalModeEstimation       = TRUE, 
    conditionalDistributionSampling = TRUE, 
    standardErrorEstimation         = TRUE, 
    logLikelihoodEstimation         = TRUE
  )
  scenario$linearization <- FALSE
  setScenario(scenario)
  
  # Run the analysis
  runScenario()
  
  # Create a unique project name based on the current combination
  proj_name <- paste0("final_data_",
                      ifelse(comb$PPI, "PPI_T", "PPI_F"), "_",
                      ifelse(comb$logtWEIGHT, "WT_T", "WT_F"), "_",
                      ifelse(comb$logtCrCl, "CrCl_T", "CrCl_F"), "_",
                      ifelse(comb$UM, "UM_T", "UM_F"), ".mlxtran")
  project_file <- file.path(projects_folder, proj_name)
  
  # Save the project
  saveProject(projectFile = project_file)
  
  cat("Completed project:", proj_name, "\n")
}






initializeLixoftConnectors(software = "monolix")

# Folder containing the .mlxtran project files
projects_folder <- "C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1/monolix_projects_ofv"

# List all .mlxtran files
mlxtran_files <- list.files(projects_folder, pattern = "\\.mlxtran$", full.names = TRUE)

# Helper function to parse the filename and return a readable model name
parseModelName <- function(filepath) {
  # Example filename: "final_data_PPI_F_WT_F_CrCl_T_UM_F.mlxtran"
  
  # 1) Remove path & extension, e.g. => "final_data_PPI_F_WT_F_CrCl_T_UM_F"
  base <- file_path_sans_ext(basename(filepath))
  
  # 2) Remove the prefix "final_data_" if present
  #    This should leave "PPI_F_WT_F_CrCl_T_UM_F"
  base <- str_remove(base, "^final_data_")
  
  # 3) Split by underscores
  #    => c("PPI","F","WT","F","CrCl","T","UM","F")
  parts <- str_split(base, "_", simplify = TRUE)
  
  # We expect them in pairs: (PPI,F), (WT,F), (CrCl,T), (UM,F)
  # But let's store them systematically:
  # e.g. covariateNames = c("PPI", "WT", "CrCl", "UM")
  # The boolean flags are the even indices in 'parts'
  # or we can pair them up directly
  # We'll assume the pattern is strictly: PPI, F, WT, F, CrCl, F, UM, F
  
  if (nrow(parts) == 0 || length(parts) < 8) {
    # Fallback if the pattern is unexpected
    return(base)
  }
  
  # Covariate names come from the odd indices: 1, 3, 5, 7
  covNames <- parts[c(1, 3, 5, 7)]
  # T/F from the even indices: 2, 4, 6, 8
  covFlags <- parts[c(2, 4, 6, 8)]
  
  # Pair them
  included <- covNames[covFlags == "T"]
  
  if (length(included) == 0) {
    return("null model")
  } else {
    # Join with " and ", or you could use commas if you prefer
    return(paste(included, collapse = " and "))
  }
}

# Create an empty data frame to store results
results_df <- data.frame(
  Model     = character(),
  OFV       = numeric(),
  BICc      = numeric(),
  stringsAsFactors = FALSE
)

# Loop over each project file
for (file in mlxtran_files) {
  loadProject(file)
  
  # Extract OFV and BICc from importanceSampling
  ll_info <- getEstimatedLogLikelihood()$importanceSampling
  ofv_value  <- as.numeric(ll_info["OFV"])
  bicc_value <- as.numeric(ll_info["BICc"])
  
  # Parse the model name from the filename
  model_name <- parseModelName(file)
  
  # Add a row to our results data frame
  results_df <- rbind(
    results_df,
    data.frame(
      Model = model_name,
      OFV   = ofv_value,
      BICc  = bicc_value,
      stringsAsFactors = FALSE
    )
  )
}

# Identify the "null model" row (all covariates F)
null_idx <- which(results_df$Model == "null model")

if (length(null_idx) == 1) {
  # Extract the null model's OFV and BICc
  ofv_null  <- results_df$OFV[null_idx]
  bicc_null <- results_df$BICc[null_idx]
  
  # Compute delta values relative to the null model
  results_df$Delta_OFV  <- results_df$OFV  - ofv_null
  results_df$Delta_BICc <- results_df$BICc - bicc_null
} else {
  # If you do not have a null model row, you can skip deltas or handle differently
  results_df$Delta_OFV  <- NA
  results_df$Delta_BICc <- NA
}

# Sort by descending OFV (so largest OFV at the top)
results_df <- results_df[order(-results_df$OFV), ]

# Print the final results
print(results_df)



OFVval <- results_df




# Define the custom palette using the first 10 colors of `jco` and colors 3 to 8 of `nejm`
custom_palette <- c(
  pal_jco("default")(10),  # First 10 colors from jco palette
  pal_nejm("default")(8)[3:8]   # Colors 3 to 8 from nejm palette
)

# Add a new column for the combined label with BICc and OFV values
OFVval <- OFVval %>%
  mutate(Label = paste0(Model, "\nOFV: ", OFV, "   BICc: ", BICc))  # Add values to labels

# Reshape data to have OFV and BICc in long format for faceting
plot_data <- OFVval %>%
  pivot_longer(cols = c(OFV, BICc), names_to = "Metric", values_to = "Value") %>%
  group_by(Metric) %>%
  arrange(desc(Value)) %>%
  mutate(
    Label = factor(Label, levels = unique(Label)),  # Set Label factor levels for sorted legend
    Model = factor(Model)  # Keep Model as factor without reordering x-axis
  ) %>%
  ungroup()  # Ungroup after setting factor levels

# Create the bar plot using Label as the fill
OFV_plot <- ggplot(plot_data, aes(x = reorder(Model, -Value), y = Value, fill = Label)) +
  geom_col(position = position_dodge(width = 0.3), color = "black") +  # Reduce width for more space between bars
  coord_cartesian(ylim = c(920, 1300)) +  # Zoom in on Y-axis from 920 to 1200
  facet_wrap(~ Metric, scales = "free_y") +  # Facet by Metric (OFV and BICc)
  labs(title = "Impact of covariates on BICc and OFV",
       x = "Model",
       y = "Value") +
  scale_fill_manual(values = custom_palette) +  # Apply the custom palette
  my_theme +  # Apply your custom theme
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for readability
    legend.spacing.y = unit(1, "cm"),  # Add space between legend items
    legend.key.height = unit(1, "cm"),  # Increase legend key height for additional spacing
    panel.spacing = unit(1, "cm")  # Increase space between facets
  )

# Save the plot with a wider width to accommodate the layout
png("OFV.png", width = 20, height = 9, units = 'in', res = 900)
print(OFV_plot)
dev.off()




# Define folder paths
data_folder     <- "C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1/imputed_datasets_merged"
projects_folder <- "C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1/monolix_projects"

# List all CSV files in the data folder
csv_files <- list.files(data_folder, pattern = "\\.csv$", full.names = TRUE)

# Loop over each CSV file to create and run a Monolix project
for (csv_file in csv_files) {
  
  # Generate a unique project name based on the CSV file name
  proj_name <- paste0("MLX_", tools::file_path_sans_ext(basename(csv_file)))
  cat("Creating project:", proj_name, "\n")
  
  # Create a new project from scratch using your CSV file and the model from the library
  newProject(
    data = list(
      dataFile    = csv_file,
      headerTypes = c("id", "time", "ignore", "contcov", "ignore", "contcov", "ignore", 
                      "catcov", "catcov", "amount", "evid", "ignore", "ignore", "ignore", 
                      "observation", "ignore", "ignore", "ignore"),
      mapping     = NULL
    ),
    modelFile = "lib:oral1_1cpt_kaVCl.txt"
  )
  
  # Set population parameter initial values
  setPopulationParameterInformation(
    Cl_pop   = list(initialValue = 1),
    V_pop    = list(initialValue = 10),
    ka_pop   = list(initialValue = 1),
    omega_Cl = list(initialValue = 0.15),
    omega_V  = list(initialValue = 0.3),
    omega_ka = list(initialValue = 0.5),
    a        = list(initialValue = 0.3),
    b        = list(initialValue = 0.15)
  )
  
  # Add continuous transformed covariates
  addContinuousTransformedCovariate(logtWEIGHT = "log(WT/82.58)")
  addContinuousTransformedCovariate(logtCrCl  = "log(CrCl/106.33)")
  
  # Set the covariate model
  setCovariateModel(
    ka = c(PPI = TRUE),
    V  = c(logtWEIGHT = TRUE),
    Cl = c(logtCrCl = TRUE, UM = TRUE)
  )
  

  # Define the analysis scenario tasks
  scenario <- getScenario()
  scenario$tasks <- c(
    populationParameterEstimation   = TRUE,
    conditionalModeEstimation       = TRUE,
    conditionalDistributionSampling = TRUE,
    standardErrorEstimation         = TRUE,
    logLikelihoodEstimation         = TRUE
  )
  scenario$linearization <- FALSE
  setScenario(scenario)
  
  # Run the full analysis
  runScenario()
  
  # Save the project with a unique file name
  project_file <- file.path(projects_folder, paste0(proj_name, ".mlxtran"))
  saveProject(projectFile = project_file)
  
  cat("Completed project:", proj_name, "\n\n")
}



projects_folder <- "C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1/monolix_projects"




# Define the base directory where the Monolix folder is located
base_dir <- "C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1/monolix_projects/"

# List all directories inside the Monolix folder (amelia_1, MF_1, etc.)
subfolders <- list.dirs(base_dir, full.names = TRUE, recursive = FALSE)





# Modify the function to import data as dataframes and keep them in a list
import_monolix_data <- function(subfolder) {
  # Paths to additional subfolders
  log_likelihood_folder <- file.path(subfolder, "LogLikelihood")
  tests_folder <- file.path(subfolder, "Tests")
  
  # Paths to main folder files
  population_parameters_file <- file.path(subfolder, "populationParameters.txt")
  predictions_file <- file.path(subfolder, "predictions.txt")
  
  # Paths to files in subfolders
  log_likelihood_file <- file.path(log_likelihood_folder, "logLikelihood.txt")
  correlation_file <- file.path(tests_folder, "correlationIndividualParametersCovariates.txt")
  
  # Path to the IndividualParameters folder
  ind_param_folder <- file.path(subfolder, "IndividualParameters")
  
  # Initialize an empty list to store dataframes for each file
  combined_data <- list()
  
  # Define the separator used in your .txt files (e.g., "\t" for tab-separated, "," for CSV)
  separator <- ","  # Adjust this based on the actual file structure
  
  # Check if the folders and files exist and import them
  if (dir.exists(ind_param_folder)) {
    param_file <- file.path(ind_param_folder, "estimatedIndividualParameters.txt")
    random_effects_file <- file.path(ind_param_folder, "estimatedRandomEffects.txt")
    shrinkage_file <- file.path(ind_param_folder, "shrinkage.txt")
    
    if (file.exists(param_file)) {
      param_data <- read.table(param_file, header = TRUE, sep = separator)
      combined_data[["parameters"]] <- param_data
    }
    if (file.exists(random_effects_file)) {
      random_effects_data <- read.table(random_effects_file, header = TRUE, sep = separator)
      combined_data[["random_effects"]] <- random_effects_data
    }
    if (file.exists(shrinkage_file)) {
      shrinkage_data <- read.table(shrinkage_file, header = TRUE, sep = separator)
      combined_data[["shrinkage"]] <- shrinkage_data
    }
    if (file.exists(log_likelihood_file)) {
      log_likelihood_data <- read.table(log_likelihood_file, header = TRUE, sep = separator)
      combined_data[["log_likelihood"]] <- log_likelihood_data
    }
    if (file.exists(correlation_file)) {
      correlation_data <- read.table(correlation_file, header = TRUE, sep = separator)
      combined_data[["correlation"]] <- correlation_data
    }
    if (file.exists(population_parameters_file)) {
      population_parameters_data <- read.table(population_parameters_file, header = TRUE, sep = separator)
      combined_data[["population_parameters"]] <- population_parameters_data
    }
    if (file.exists(predictions_file)) {
      predictions_data <- read.table(predictions_file, header = TRUE, sep = separator)
      combined_data[["predictions"]] <- predictions_data
    }
    
    return(combined_data)  # Return the list of dataframes for the subfolder
  } else {
    message("Folder not found: ", ind_param_folder)
    return(NULL)
  }
}

# Apply the function to each subfolder and store results in a list
all_data <- lapply(subfolders, import_monolix_data)

# Name the list elements with the subfolder names
names(all_data) <- basename(subfolders)






# Create an empty list to store the merged shrinkage data
shrinkage_combined <- list()

# Define the percentages corresponding to the suffixes
missing_data_percentages <- c("_1" = "5%", "_2" = "20%", "_3" = "50%", "_4" = "75%")

# Loop through each element in the all_data list
for (folder_name in names(all_data)) {
  # Extract the shrinkage dataframe
  shrinkage_df <- all_data[[folder_name]]$shrinkage
  
  # Add a column for the folder name (e.g., MLX_Amelia_1)
  shrinkage_df$File <- folder_name
  
  # 1) Remove the _[0-9]+ at the end
  base_name <- sub("_[0-9]+$", "", folder_name)
  # 2) Remove the MLX_ prefix if it exists
  base_name <- sub("^MLX_", "", base_name)
  
  # Store the cleaned base_name
  shrinkage_df$Base_Name <- base_name
  
  # Extract the missing data percentage from the folder name
  missing_suffix <- sub(".*(_[0-9]+)$", "\\1", folder_name)
  shrinkage_df$Missing_Percentage <- missing_data_percentages[missing_suffix]
  
  # Append the dataframe to the list
  shrinkage_combined[[folder_name]] <- shrinkage_df
}

# Combine all shrinkage dataframes into one dataframe
final_shrinkage_df <- do.call(rbind, shrinkage_combined)

# Convert to wider format
final_shrinkage_wide <- final_shrinkage_df %>%
  tidyr::pivot_wider(
    names_from = parameters,
    values_from = c(shrinkage_mode, shrinkage_mean, shrinkage_condDist)
  )

# Replace NA in Missing_Percentage with '0%'
final_shrinkage_wide$Missing_Percentage[is.na(final_shrinkage_wide$Missing_Percentage)] <- "0%"

# Reorder the Missing_Percentage as a factor with the specified order
final_shrinkage_wide$Missing_Percentage <- factor(
  final_shrinkage_wide$Missing_Percentage,
  levels = c("0%", "5%", "20%", "50%", "75%")
)

# Reshape to long format
final_shrinkage_long <- final_shrinkage_wide %>%
  tidyr::pivot_longer(
    cols = starts_with("shrinkage"),
    names_to = c("Shrinkage_Type", "Parameter"),
    names_pattern = "shrinkage_(.*)_(.*)",
    values_to = "Value"
  )

# Recode the Base_Name for consistent labeling
final_shrinkage_long$Base_Name <- final_shrinkage_long$Base_Name %>%
  dplyr::recode(
    # e.g. if "MF" remains "MF", no change needed
    # Otherwise add: "MF" = "MyFancyName",
    "amelia" = "Amelia",
    "mice" = "Mice",
    "midas" = "MIDAS",
    "xgb" = "XGB",
    "final_data" = "Original Data"
  )

# Convert Base_Name to a factor with a specific order (adjust as needed)
final_shrinkage_long$Base_Name <- factor(
  final_shrinkage_long$Base_Name,
  levels = c("Amelia", "MF", "Mice", "MIDAS", "XGB", "Original Data")
)

# Create separate plots for each Shrinkage_Type
plot_list <- list()
for (shrinkage_type in unique(final_shrinkage_long$Shrinkage_Type)) {
  
  plot_data <- final_shrinkage_long %>%
    dplyr::filter(Shrinkage_Type == shrinkage_type)
  
  p <- ggplot(plot_data, aes(x = Missing_Percentage, y = Value, fill = Base_Name)) +
    geom_col(
      position = position_dodge(width = 0.75), 
      color = "black", 
      aes(width = ifelse(Missing_Percentage == "0%", 0.14, 0.7))
    ) +
    facet_wrap(~ Parameter, scales = "free") +
    labs(
      title = paste("Shrinkage", shrinkage_type, "by Missing Percentage"),
      fill = "Imputation Method",
      x = "Missing Percentage",
      y = "Shrinkage Value (%)"
    ) +
    scale_fill_jco() +
    my_theme
  
  plot_list[[shrinkage_type]] <- p
}

# Arrange the 3 plots (mode, mean, condDist) in a single figure
combined_plot <- ggarrange(
  plot_list[["mode"]], 
  plot_list[["mean"]], 
  plot_list[["condDist"]],
  ncol = 1,
  nrow = 3,
  labels = c("A", "B", "C"),
  common.legend = TRUE,
  legend = "right"
)

# Save the plot
png("Shrinkage_custom_adjusted.png", width = 16, height = 9, units = 'in', res = 900)
print(combined_plot)
dev.off()






# Define which criteria we're interested in (OFV and BICc)
log_likelihood_criteria <- c("OFV", "BICc")

# Prepare data for plotting
log_likelihood_data <- list()

# Extract the log likelihood data (OFV and BICc) from each subfolder
for (subfolder in names(all_data)) {
  if ("log_likelihood" %in% names(all_data[[subfolder]])) {
    log_likelihood_df <- all_data[[subfolder]]$log_likelihood
    
    if ("OFV" %in% log_likelihood_df$criteria && "BICc" %in% log_likelihood_df$criteria) {
      
      # 1) Determine the missing-percentage label
      missing_percentage <- ifelse(grepl("final_data", subfolder), "0%",
                            ifelse(grepl("_1$", subfolder), "5%",
                            ifelse(grepl("_2$", subfolder), "20%",
                            ifelse(grepl("_3$", subfolder), "50%", "75%"))))
      
      # 2) Remove trailing _[0-9] from subfolder
      base_name <- sub("_[0-9]+$", "", subfolder)
      # 3) Remove MLX_ prefix if it exists
      base_name <- sub("^MLX_", "", base_name)
      
      # Create your data.frame
      log_likelihood_values <- data.frame(
        File = subfolder,
        Base_Name = base_name,
        Missing_Percentage = missing_percentage,
        OFV = log_likelihood_df$importanceSampling[log_likelihood_df$criteria == "OFV"],
        BICc = log_likelihood_df$importanceSampling[log_likelihood_df$criteria == "BICc"]
      )
      
      # Store the results
      log_likelihood_data[[subfolder]] <- log_likelihood_values
    }
  }
}

# Combine the data into one dataframe
log_likelihood_combined <- do.call(rbind, log_likelihood_data)

# Reorder the Missing_Percentage to start with 0%
log_likelihood_combined$Missing_Percentage <- factor(
  log_likelihood_combined$Missing_Percentage, 
  levels = c("0%", "5%", "20%", "50%", "75%")
)

# Reshape the data to long format for plotting
log_likelihood_long <- log_likelihood_combined %>%
  pivot_longer(cols = c(OFV, BICc), names_to = "Criteria", values_to = "Value") %>%
  filter(!is.na(Value))  # Remove rows with missing values

# Compute the delta (difference) for each criterion and handle missing baselines
log_likelihood_long <- log_likelihood_long %>%
  group_by(Criteria) %>%
  mutate(
    # Extract the baseline value if it exists
    Baseline = if (any(Base_Name == "final_data" & Missing_Percentage == "0%")) {
      Value[Base_Name == "final_data" & Missing_Percentage == "0%"]
    } else {
      NA
    },
    # Compute Delta if Baseline is not NA
    Delta = ifelse(!is.na(Baseline), Value - Baseline, NA),
    # Replace 0 with NA
    Delta = ifelse(Delta == 0, NA, Delta)
  ) %>%
  ungroup()

log_likelihood_long <- log_likelihood_long %>%
  mutate(
    Base_Name = recode(
      Base_Name,
      "amelia" = "Amelia",
      "mice" = "Mice",
      "midas" = "MIDAS",  # Ensure MIDAS remains in capital letters
      "xgb" = "XGB",      # Ensure XGB remains in capital letters
      "final_data" = "Original Data"
    )
  )




# Define the custom color mapping for each Base_Name
jco_colors <- pal_jco("default")(6)  # Extract the first 6 jco colors
custom_colors <- c(
  "Amelia" = jco_colors[1],
  "MF" = jco_colors[2],
  "Mice" = jco_colors[3],
  "MIDAS" = jco_colors[4],
  "XGB" = jco_colors[5],
  "Original Data" = jco_colors[6]
)

# Create separate plots for OFV and BICc
plot_list <- list()

for (criteria in log_likelihood_criteria) {
  
  # Filter data for the current criteria
  plot_data <- log_likelihood_long %>% filter(Criteria == criteria)
  
  # Separate data for Missing_Percentage == "0%" and the rest
  plot_data_0 <- plot_data %>% filter(Missing_Percentage == "0%")
  plot_data_rest <- plot_data %>% filter(Missing_Percentage != "0%")
  
  # Get the reference value for this criteria
  final_value <- plot_data_0$Value[1]  # Reference value for 0%
  
  # Adjust Y-axis limit based on criteria
  ylim_max <- ifelse(criteria == "BICc", 1300, 1300)  # Increase BICc to 1775, OFV remains 1750
  
  # Create the plot with delta values displayed at the top of each bar
  p <- ggplot() +
    geom_col(data = plot_data_0, aes(x = Missing_Percentage, y = Value, fill = Base_Name), 
             position = position_dodge(width = 0.75), color = "black", width = 0.14) +  # Narrow bars for 0%
    geom_col(data = plot_data_rest, aes(x = Missing_Percentage, y = Value, fill = Base_Name), 
             position = position_dodge(width = 0.75), color = "black", width = 0.7) +  # Regular bars for the rest
    geom_text(
      data = plot_data, 
      aes(x = Missing_Percentage, y = Value, label = ifelse(is.na(Delta), "", sprintf("%.1f", Delta)), group = Base_Name), 
      position = position_dodge(width = 0.75), vjust = -0.5, size = 3  # Adjust position and size of text
    ) +
    geom_hline(yintercept = final_value, linetype = "dashed", color = "black", size = 1) +  # Add horizontal line
    coord_cartesian(ylim = c(900, ylim_max)) +  # Adjust Y-axis limits
    labs(
      title = paste(criteria, "by Missing Percentage"),
      x = "Missing Percentage",
      y = criteria
    ) +
    scale_fill_manual(values = custom_colors, breaks = c("Amelia", "MF", "Mice", "MIDAS", "XGB", "Original Data")) +  # Set legend order manually
    my_theme +  # Apply your custom theme
    guides(fill = guide_legend(title = "Imputation Method"))  # Set legend title
  
  # Store the plot in the list
  plot_list[[criteria]] <- p
}

# Arrange the two plots (OFV and BICc) in a single figure using ggarrange
combined_log_likelihood_plot <- ggarrange(
  plot_list[["OFV"]], plot_list[["BICc"]],
  ncol = 1, nrow = 2,  # Arrange in one column and two rows
  labels = c("A", "B"),  # Add labels to the plots
  common.legend = TRUE,       # Share a common legend across all plots
  legend = "right"            # Position the legend on the right
)

# Save the plot as a PNG file with specified dimensions and resolution
png("LogLikelihood_OFV_BICc_Updated.png", 
    width = 16, height = 9, units = 'in', res = 900)
print(combined_log_likelihood_plot)
dev.off()


# Filter out rows with 0% NA
filtered_data <- log_likelihood_long %>%
  filter(Missing_Percentage != "0%")

# Compute mean and standard deviation of Delta for each Missing_Percentage
summary_stats <- filtered_data %>%
  group_by(Missing_Percentage) %>%
  summarise(
    mean = mean(Delta, na.rm = TRUE),
    sd = sd(Delta, na.rm = TRUE)
  )

# View the summary stats
print(summary_stats)


# Prepare data for plotting
population_data_list <- list()

# Extract the population parameters data from each subfolder
for (subfolder in names(all_data)) {
  
  # Extract the population parameters data from the subfolder
  pop_param_df <- all_data[[subfolder]]$population_parameters
  
  # Parse folder name: remove trailing _1, _2, etc. and the MLX_ prefix
  base_name <- sub("_[0-9]+$", "", subfolder)  # Remove trailing _<number>
  base_name <- sub("^MLX_", "", base_name)       # Remove MLX_ prefix if present
  
  # Recode "final_data" to "Original Data" if needed
  if (base_name == "final_data") {
    base_name <- "Original Data"
  }
  
  # Add the cleaned base name to the data frame
  pop_param_df$Base_Name <- base_name
  
  # Determine the missing percentage based on subfolder naming
  pop_param_df$Missing_Percentage <- ifelse(grepl("final_data", subfolder), "0%",
                                     ifelse(grepl("_1$", subfolder), "5%",
                                     ifelse(grepl("_2$", subfolder), "20%",
                                     ifelse(grepl("_3$", subfolder), "50%", "75%"))))
  
  # Create a data frame with the population values
  # (Assuming pop_param_df has the columns: parameter, value, P2.5_sa, and P97.5_sa)
  population_values <- data.frame(
      File = subfolder,
      Base_Name = base_name,
      Missing_Percentage = pop_param_df$Missing_Percentage,
      parameter = pop_param_df$parameter,
      Value = pop_param_df$value,
      P2.5 = pop_param_df$P2.5_sa,
      P97.5 = pop_param_df$P97.5_sa
  )
  
  # Apply exp() transformation to specific parameters
  parameters_to_transform <- c("beta_ka_PPI_1", "beta_Cl_UM_1")
  population_values$Value <- ifelse(population_values$parameter %in% parameters_to_transform,
                                    exp(population_values$Value),
                                    population_values$Value)
  population_values$P2.5 <- ifelse(population_values$parameter %in% parameters_to_transform,
                                   exp(population_values$P2.5),
                                   population_values$P2.5)
  population_values$P97.5 <- ifelse(population_values$parameter %in% parameters_to_transform,
                                    exp(population_values$P97.5),
                                    population_values$P97.5)
  
  population_data_list[[subfolder]] <- population_values
}

# Combine all data into one dataframe
population_param_combined <- do.call(rbind, population_data_list)

# Reorder Missing_Percentage factor to start with 0%
population_param_combined$Missing_Percentage <- factor(
  population_param_combined$Missing_Percentage, 
  levels = c("0%", "5%", "20%", "50%", "75%")
)

# Reorder and recode Base_Name for consistent capitalization and ordering.
# (After stripping the prefix, names should be like "Amelia", "MF", "Mice", "MIDAS", "XGB", or "Original Data")
population_param_combined$Base_Name <- dplyr::recode(population_param_combined$Base_Name,
                                                     "amelia" = "Amelia",
                                                     "mf" = "MF",
                                                     "mice" = "Mice",
                                                     "midas" = "MIDAS",
                                                     "xgb" = "XGB",
                                                     "final_data" = "Original Data")
population_param_combined$Base_Name <- factor(
  population_param_combined$Base_Name,
  levels = c("Amelia", "MF", "Mice", "MIDAS", "XGB", "Original Data")
)

# Get the jco color palette and define the custom color mapping
jco_colors <- pal_jco("default")(6)  # Extract the first 6 jco colors
custom_colors <- c(
  "Amelia" = jco_colors[1],
  "MF" = jco_colors[2],
  "Mice" = jco_colors[3],
  "MIDAS" = jco_colors[4],
  "XGB" = jco_colors[5],
  "Original Data" = jco_colors[6]
)

# Create a mapping for formatted parameter names with subscripts
parameter_mapping <- list(
  "Cl_pop" = expression(CL[pop]),
  "ka_pop" = expression(ka[pop]),
  "V_pop" = expression(V[pop]),
  "beta_ka_PPI_1" = expression(beta[PPI_ka]),
  "beta_Cl_UM_1" = expression(beta[UM_CL]),
  "beta_V_logtWEIGHT" = expression(beta[WT_V]),
  "beta_Cl_logtCrCl" = expression(beta[CrCl_CL]),
  "omega_ka" = expression(omega[ka]),
  "omega_V" = expression(omega[V]),
  "omega_Cl" = expression(omega[CL]),
  "a" = expression(italic(a)),
  "b" = expression(italic(b))
)

# Create plots for each population parameter
plot_list <- list()

for (param in unique(population_param_combined$parameter)) {
  
  # Filter data for the current parameter
  plot_data <- dplyr::filter(population_param_combined, parameter == param)
  
  # Separate the data for Missing_Percentage = 0% and the rest
  plot_data_0 <- dplyr::filter(plot_data, Missing_Percentage == "0%")
  plot_data_rest <- dplyr::filter(plot_data, Missing_Percentage != "0%")
  
  # Get the "Original Data" value for this parameter (averaging if needed)
  final_value <- mean(dplyr::filter(plot_data, Base_Name == "Original Data")$Value, na.rm = TRUE)
  
  # Get the formatted title for the current parameter
  formatted_title <- parameter_mapping[[param]] %>% as.expression()
  
  # Create the plot using barplots and error bars
  p <- ggplot() +
    geom_col(data = plot_data_0, aes(x = Missing_Percentage, y = Value, fill = Base_Name), 
             position = position_dodge(width = 0.75), color = "black", width = 0.14) +  # Narrow bars for 0%
    geom_col(data = plot_data_rest, aes(x = Missing_Percentage, y = Value, fill = Base_Name), 
             position = position_dodge(width = 0.75), color = "black", width = 0.7) +   # Regular bars
    geom_errorbar(data = plot_data, aes(x = Missing_Percentage, ymin = P2.5, ymax = P97.5, group = Base_Name), 
                  position = position_dodge(width = 0.75), width = 0.25, color = "black") +
    geom_hline(yintercept = final_value, linetype = "dashed", color = "black", size = 1) +
    labs(title = formatted_title, x = "Missing Percentage", y = "Parameter estimates") +
    scale_fill_manual(values = custom_colors,
                      breaks = c("Amelia", "MF", "Mice", "MIDAS", "XGB", "Original Data")) +
    my_theme +
    theme(plot.title = element_text(hjust = 0.5)) +
    guides(fill = guide_legend(title = "Imputation Method"))
  
  plot_list[[param]] <- p
}

# Arrange the plots in a grid with 3 columns
combined_population_plot <- ggarrange(plotlist = plot_list, ncol = 3, 
                                      nrow = ceiling(length(plot_list) / 3), 
                                      common.legend = TRUE, legend = "right")

# Save the combined plot as a high-resolution PNG file
png("Population_Parameters_Plot_with_Subscripts_Corrected.png", 
    width = 16, height = 9, units = 'in', res = 900)
print(combined_population_plot)
dev.off()
























# Prepare list to store p-values from correlation files
correlation_data_list <- list()

# Loop through each folder in your data
for (subfolder in names(all_data)) {
  if ("correlation" %in% names(all_data[[subfolder]])) {
    correlation_df <- all_data[[subfolder]]$correlation
    
    # Extract Missing_Percentage from folder names
    missing_percentage <- ifelse(grepl("final_data", subfolder), "0%", 
                                 ifelse(grepl("_1$", subfolder), "5%",
                                        ifelse(grepl("_2$", subfolder), "20%",
                                               ifelse(grepl("_3$", subfolder), "50%", "75%"))))
    
    # Create a new dataframe with relevant columns
    correlation_values <- data.frame(
      Base_Name = sub("_[0-9]+$", "", subfolder),  # Extract base name before the number
      Missing_Percentage = missing_percentage,     # Use extracted Missing_Percentage
      covariate = correlation_df$covariate,        # Covariate column (instead of parameter)
      p_value = correlation_df$p.value             # P-value column
    )
    
    # Append to list
    correlation_data_list[[subfolder]] <- correlation_values
  }
}

# Combine all data into one dataframe
correlation_combined <- do.call(rbind, correlation_data_list)

# Reorder the Missing_Percentage to start with 0%
correlation_combined$Missing_Percentage <- factor(
  correlation_combined$Missing_Percentage, 
  levels = c("0%", "5%", "20%", "50%", "75%")
)


# Convert p_value to numeric and then filter
correlation_combined <- correlation_combined %>%
  mutate(p_value = as.numeric(p_value)) %>%  # Convert p_value column to numeric
  filter(!is.na(p_value) & p_value > 0.05)  # Remove rows with NA or p-values < 0.05

# View the resulting dataframe
head(correlation_combined)



# Reorder columns as: Covariate, Missing Percentage, Base Name, and P-value
correlation_combined <- correlation_combined %>%
  dplyr::select(covariate, Missing_Percentage, Base_Name, p_value) %>%
  dplyr::arrange(covariate, Missing_Percentage, Base_Name)

# View the resulting dataframe
correlation_combined





# Create the flextable from the reordered dataframe
flextable_table <- flextable(correlation_combined)

# Merge vertically where values are the same for covariate, Missing_Percentage, and Base_Name
flextable_table <- flextable_table %>%
  merge_v(j = "covariate") %>%  # Merge covariate column when values are the same
  merge_v(j = "Missing_Percentage") %>%  # Merge Missing_Percentage column when values are the same
  merge_v(j = "Base_Name") %>%  # Merge Base_Name column when values are the same
  theme_box() %>%  # Apply a simple box theme to the table
  set_header_labels(
    covariate = "Covariate",
    Missing_Percentage = "Missing Percentage",
    Base_Name = "Imputation Method",
    p_value = "P-Value"
  ) %>% 
  fontsize(size = 10, part = "all") %>%  # Set font size to 10 for all parts of the table
  bold(part = "header") %>%  # Make the headers bold
  align(align = "center", part = "all") %>%  # Center-align all text
  width(j = c("covariate"), width = 2.5) %>%  # Set custom column widths for better readability
  width(j = c("Missing_Percentage", "Base_Name", "p_value"), width = 1.5) %>%  # Adjust width of other columns
  border_inner_h(border = fp_border(color = "gray", width = 1)) %>%  # Set inner horizontal borders
  border_outer(border = fp_border(color = "black", width = 1.5)) %>%  # Set outer borders
  padding(padding = 5, part = "all") %>%  # Add some padding for better readability
  add_footer_lines(values = "This table summarizes the non-significant p-values for each covariate.") %>%  # Add a custom footer
  font(part = "all", fontname = "Times New Roman")  # Set font to Times New Roman for publication

# Display the flextable
flextable_table

# Export the flextable as a Word document with improved formatting
doc <- officer::read_docx()
doc <- flextable::body_add_flextable(doc, flextable_table)
print(doc, target = "Improved_Merged_Correlation_Table.docx")





directory_path <- "C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1/imputed_datasets_merged"
# Get full paths of all CSV files in the directory
csv_files <- list.files(path = directory_path, pattern = "\\.csv$", full.names = TRUE)
# Read all CSV files into a list
merged_imputed_data_list <- lapply(csv_files, read.csv)
# Extract file names without directory and extension
file_names <- basename(csv_files)
file_names <- tools::file_path_sans_ext(file_names)

# Assign names to the list elements
names(merged_imputed_data_list) <- file_names


mrgsolv_val <- merged_imputed_data_list$final_data %>%
  group_by(ID) %>%
  dplyr::slice(1) %>%
  select(ID, CL, VC, KA) %>%
  ungroup()

mlx_val <-   all_data$MLX_final_data$parameters %>%
group_by(id) %>%
  dplyr::slice(1) %>%
  select(id, Cl_mode, V_mode, ka_mode) %>%
  ungroup()

all_data_subset <- all_data[!names(all_data) %in% "mrgsolv_val"]

# Now map over only those elements that actually have $parameters
mlx_val_list <- map(
  all_data_subset,
  ~ .x$parameters %>%
    group_by(id) %>%
    dplyr::slice(1) %>%
    select(ID = id, CL = Cl_mode, VC = V_mode, KA = ka_mode) %>%
    ungroup()
)

# 2) Combine all extracted data frames into one, and remove "MLX_" prefix from source
combined_mlx_val <- bind_rows(mlx_val_list, .id = "source") %>%
  mutate(source = sub("^MLX_", "", source))

# Step 3: Update source to readable missing percentage levels and imputation methods
combined_mlx_val <- combined_mlx_val %>%
  mutate(
    Missing_Percentage = case_when(
      source == "final_data" ~ "0% NA",
      grepl("_1$", source) ~ "5% NA",
      grepl("_2$", source) ~ "20% NA",
      grepl("_3$", source) ~ "50% NA",
      grepl("_4$", source) ~ "75% NA"
    ),
    Imputation_Method = case_when(
      grepl("amelia", source, ignore.case = TRUE) ~ "Amelia",
      grepl("MF", source, ignore.case = TRUE)     ~ "MF",
      grepl("mice", source, ignore.case = TRUE)   ~ "mice",
      grepl("midas", source, ignore.case = TRUE)  ~ "MIDAS",
      grepl("xgb", source, ignore.case = TRUE)    ~ "XGB",
      source == "final_data"                      ~ "Final Data"
    )
  )

# Step 4: Duplicate "0% NA" data for each imputation method and remove original "Final Data" rows
final_data_rows <- combined_mlx_val %>%
  filter(Missing_Percentage == "0% NA") %>%
  select(-Imputation_Method) %>%
  expand_grid(Imputation_Method = c("Amelia", "MF", "mice", "MIDAS", "XGB"))

combined_mlx_val <- combined_mlx_val %>%
  filter(Imputation_Method != "Final Data") %>%
  bind_rows(final_data_rows)

combined_mlx_val <- combined_mlx_val %>%
  mutate(
    Imputation_Method = factor(Imputation_Method, levels = c("Amelia", "MF", "mice", "MIDAS", "XGB")),
    Missing_Percentage = factor(Missing_Percentage, levels = c("0% NA", "5% NA", "20% NA", "50% NA", "75% NA"))
  )

# Rename VC column to V
colnames(combined_mlx_val)[colnames(combined_mlx_val) == "VC"] <- "V"

# Optionally replace method labels if they're still in lowercase in your data
combined_mlx_val <- combined_mlx_val %>%
  mutate(
    Imputation_Method = recode(
      Imputation_Method,
      "amelia" = "Amelia",
      "mf"     = "MF",
      "mice"   = "Mice",
      "midas"  = "MIDAS",
      "xgb"    = "XGB"
    )
  )

# Proceed with plotting
plot_histogram_with_lognormal_and_density_fixed <- function(data, variable, variable_name) {
    
  # Set binwidth based on the variable
  binwidth_value <- if (variable == "CL") 0.2 else if (variable == "V") 1 else 0.1
  
  # Compute per-group parameters for log-normal distribution
  subset_params <- data %>%
    filter(!!sym(variable) > 0) %>%
    group_by(Missing_Percentage, Imputation_Method) %>%
    summarize(
      meanlog = mean(log(!!sym(variable)), na.rm = TRUE),
      sdlog   = sd(log(!!sym(variable)), na.rm = TRUE),
      count   = n(),
      .groups = "keep"  # Retain grouping
    )
  
  # Create a grid of x-values
  x_grid <- seq(min(data[[variable]], na.rm = TRUE), max(data[[variable]], na.rm = TRUE), length.out = 100)
  
  # Compute log-normal density values per group
  lognormal_data <- subset_params %>%
    group_modify(~ {
      meanlog <- .x$meanlog[1]
      sdlog   <- .x$sdlog[1]
      count   <- .x$count[1]
      
      # Handle cases where sdlog is NA or zero
      if (is.na(sdlog) || sdlog == 0 || count < 2) {
        return(data.frame(x = numeric(0), y = numeric(0)))
      }
      data.frame(
        x = x_grid,
        y = dlnorm(x_grid, meanlog, sdlog) * count * binwidth_value
      )
    })
  
  # Compute density estimates per group
  density_data <- data %>%
    group_by(Missing_Percentage, Imputation_Method) %>%
    group_modify(~ {
      if (nrow(.x) < 2) {
        return(data.frame(x = numeric(0), y = numeric(0)))
      }
      dens <- density(.x[[variable]], na.rm = TRUE)
      data.frame(
        x = dens$x,
        y = dens$y * length(.x[[variable]]) * binwidth_value
      )
    })
  
  # Create the base plot
  plot <- ggplot(data, aes_string(x = variable)) +
    # Histogram
    geom_histogram(
      aes(y = ..count.., fill = Imputation_Method),
      binwidth = binwidth_value, color = "black", alpha = 0.7, show.legend = TRUE
    ) +
    # Log-normal curve per group
    geom_line(
      data = lognormal_data,
      aes(x = x, y = y, color = "Log-normal Distribution", linetype = "Log-normal Distribution"),
      size = 1, show.legend = TRUE
    ) +
    # Density curve per group
    geom_line(
      data = density_data,
      aes(x = x, y = y, color = "Density Estimate", linetype = "Density Estimate"),
      size = 1, alpha = 0.5, show.legend = TRUE
    ) +
    # Facet by Missing_Percentage and Imputation_Method
    facet_grid(Imputation_Method ~ Missing_Percentage, scales = "free_y") +
    # Use the color palette from ggsci
    scale_fill_jco() +
    # Add manual legends for Log-normal and Density Estimate
    scale_linetype_manual(
      name = "Legend",
      values = c("Density Estimate" = "solid", "Log-normal Distribution" = "solid")
    ) +
    scale_color_manual(
      name = "Legend",
      values = c("Log-normal Distribution" = "blue", "Density Estimate" = "red")
    ) +
    labs(
      title = paste("Distribution of", variable_name),
      x = variable_name,
      y = "Count"
    ) +
    # Example custom theme
    my_theme +
    theme(
      plot.title      = element_text(hjust = 0.5, face = "bold"),
      axis.title      = element_text(size = 16),
      axis.text       = element_text(size = 14),
      strip.text      = element_text(face = "bold"),
      panel.spacing   = unit(1, "lines"),
      legend.position = "top",
      legend.key      = element_rect(fill = "white", color = "black"),
      legend.title    = element_text(face = "bold"),
      legend.text     = element_text(size = 12),
      panel.border    = element_rect(color = "grey", fill = NA, size = 1),
      strip.background = element_rect(fill = "grey", color = "grey", size = 1)
    ) +
    # Custom guides
    guides(
      fill     = guide_legend(title = "Imputation Method"),
      linetype = guide_legend(override.aes = list(color = c("red", "blue"), fill = "white")),
      color    = "none"
    )
  
  list(
    plot          = plot,
    lognormal_data = lognormal_data,
    density_data   = density_data
  )
}

# Example usage for CL
CL_Plot_fixed <- plot_histogram_with_lognormal_and_density_fixed(combined_mlx_val, "CL", "CL")
png("CL_Distribution.png", width = 16, height = 9, units = 'in', res = 900)
print(CL_Plot_fixed$plot)
dev.off()

# Example usage for V
V_Plot_fixed <- plot_histogram_with_lognormal_and_density_fixed(combined_mlx_val, "V", "V")
png("V_Distribution.png", width = 16, height = 9, units = 'in', res = 900)
print(V_Plot_fixed$plot)
dev.off()

# Example usage for KA
KA_Plot_fixed <- plot_histogram_with_lognormal_and_density_fixed(combined_mlx_val, "KA", "KA")
png("KA_Distribution.png", width = 16, height = 9, units = 'in', res = 900)
print(KA_Plot_fixed$plot)
dev.off()

# Combined figure with all PK parameters
png("PK_param_Distribution.png", width = 12, height = 18, units = 'in', res = 900)
ggarrange(
  KA_Plot_fixed$plot,
  V_Plot_fixed$plot,
  CL_Plot_fixed$plot,
  nrow = 3,
  common.legend = TRUE
)
dev.off()



# Define the desired order of imputation methods
desired_method_order <- c("Amelia", "MF", "Mice", "MIDAS", "XGB", "Original Data")

# Step 1: Extract PK parameters from all datasets and label them

# Initialize a list to store the parameter data frames
param_dfs <- list()

# 1A. Process the 'MLX_final_data' (the "Original Data")
final_data_params <- all_data$MLX_final_data$parameters %>%
  dplyr::group_by(id) %>%
  dplyr::summarise(
    CL = dplyr::first(Cl_mode),
    VC = dplyr::first(V_mode),
    KA = dplyr::first(ka_mode)
  ) %>%
  dplyr::ungroup() %>%
  dplyr::rename(ID = id) %>%
  dplyr::mutate(
    Missing_Percentage = factor("0% NA", levels = c("0% NA", "5% NA", "20% NA", "50% NA", "75% NA")),
    Imputation_Method  = factor("Original Data", levels = desired_method_order)
  )

# Store it
param_dfs[["Final Data"]] <- final_data_params

# 1B. Create a character vector of dataset names, excluding both "MLX_final_data" and "mrgsolv_val"
dataset_names <- setdiff(names(all_data), c("MLX_final_data", "mrgsolv_val"))

# Process each imputed dataset
for (dataset_name in dataset_names) {
  
  # Extract the dataset from all_data
  dataset <- all_data[[dataset_name]]
  
  # If this dataset does not have $parameters, skip
  if (is.null(dataset$parameters)) {
    message(paste("Skipping", dataset_name, "because it has no 'parameters' component."))
    next
  }
  
  # Extract PK parameters
  params_df <- dataset$parameters %>%
    dplyr::group_by(id) %>%
    dplyr::summarise(
      CL = dplyr::first(Cl_mode),
      VC = dplyr::first(V_mode),
      KA = dplyr::first(ka_mode)
    ) %>%
    dplyr::ungroup() %>%
    dplyr::rename(ID = id)
  
  # Remove "MLX_" prefix if you want simpler matching
  short_name <- sub("^MLX_", "", dataset_name)
  
  # Determine the Imputation_Method by matching "Amelia", "MF", etc.
  # (Use grepl(...) rather than grepl("^...") if your dataset name is "Amelia_1" or "XGB_1")
  # or simply check whether short_name starts with the method name.
  if (grepl("Amelia", short_name, ignore.case = TRUE)) {
    method <- "Amelia"
  } else if (grepl("MF", short_name, ignore.case = TRUE)) {
    method <- "MF"
  } else if (grepl("Mice", short_name, ignore.case = TRUE)) {
    method <- "Mice"
  } else if (grepl("MIDAS", short_name, ignore.case = TRUE)) {
    method <- "MIDAS"
  } else if (grepl("XGB", short_name, ignore.case = TRUE)) {
    method <- "XGB"
  } else {
    stop(paste("Unknown method in dataset name:", short_name))
  }
  
  # Extract the missing percentage from the dataset name by looking at the trailing number
  # If short_name ends with _1, _2, etc.
  if (grepl("_1$", short_name)) {
    percentage <- "5%"
  } else if (grepl("_2$", short_name)) {
    percentage <- "20%"
  } else if (grepl("_3$", short_name)) {
    percentage <- "50%"
  } else if (grepl("_4$", short_name)) {
    percentage <- "75%"
  } else {
    stop(paste("Could not determine Missing_Percentage for dataset:", short_name))
  }
  
  # Add the columns to params_df with the correct factor levels
  params_df <- params_df %>%
    dplyr::mutate(
      Missing_Percentage = factor(percentage, 
                                  levels = c("0%", "5%", "20%", "50%", "75%")),
      Imputation_Method = factor(method, levels = desired_method_order)
    )
  
  # Add to the list with the dataset_name as key
  param_dfs[[dataset_name]] <- params_df
}

# param_dfs now has a "Final Data" entry + one entry per imputed dataset (excluding mrgsolv_val).
# The rest of your steps for comparing, reshaping, and plotting can proceed.


# Step 2: Extract IDs with missing data for each missing percentage

# Initialize a list to store IDs with missing data
missing_ids_list <- list()

# Loop over each dataset in 'missing_data_list'
for (dataset_name in names(missing_data_list)) {
  # Extract the percentage from the dataset name
  # Assuming dataset names are like "missing_5perc", "missing_20perc", etc.
  percentage_match <- str_extract(dataset_name, "\\d+")
  
  if (!is.na(percentage_match)) {
    percentage_value <- as.integer(percentage_match)
    percentage <- paste0(percentage_value, "% NA")
  } else {
    stop(paste("Could not extract percentage from dataset name:", dataset_name))
  }
  
  # Get the dataset with missing data
  data_with_missing <- missing_data_list[[dataset_name]]
  
  # Identify IDs with any missing data
  ids_with_missing <- data_with_missing %>%
    filter(is.na(WT) | is.na(CrCl) | is.na(PPI) | is.na(UM)) %>%
    select(ID) %>%
    distinct()
  
  # Store the IDs in the list
  missing_ids_list[[percentage]] <- ids_with_missing$ID
}

# Step 3: Perform comparisons for IDs with missing data

# Define the comparison function
compare_params <- function(imputed_df, final_df, ids_with_missing) {
  # Filter both data frames to include only IDs with missing data
  imputed_df_filtered <- imputed_df %>% filter(ID %in% ids_with_missing)
  final_df_filtered <- final_df %>% filter(ID %in% ids_with_missing)
  
  # Ensure IDs are of the same type
  imputed_df_filtered$ID <- as.character(imputed_df_filtered$ID)
  final_df_filtered$ID <- as.character(final_df_filtered$ID)
  
  # Proceed with the comparison
  comparison_df <- imputed_df_filtered %>%
    inner_join(final_df_filtered, by = "ID", suffix = c("_imputed", "_original")) %>%
    mutate(
      CL_diff = CL_imputed - CL_original,
      VC_diff = VC_imputed - VC_original,
      KA_diff = KA_imputed - KA_original,
      CL_PE = (CL_imputed - CL_original) / CL_original * 100,
      VC_PE = (VC_imputed - VC_original) / VC_original * 100,
      KA_PE = (KA_imputed - KA_original) / KA_original * 100
    )
  return(comparison_df)
}

# Initialize a list to store comparison results
comparison_results <- list()

# Loop over the imputed datasets (excluding 'Final Data')
for (name in names(param_dfs)) {
  if (name != "Final Data") {
    imputed_df <- param_dfs[[name]]
    final_df <- param_dfs[["Final Data"]]
    
    # Get the Imputation_Method and Missing_Percentage (from earlier in the loop)
    method <- as.character(unique(imputed_df$Imputation_Method))
    percentage <- as.character(unique(imputed_df$Missing_Percentage))
    
    # Get the IDs with missing data for this percentage
    ids_with_missing <- missing_ids_list[[percentage]]
    
    # Check if ids_with_missing is NULL or empty
    if (is.null(ids_with_missing)) {
      message(paste("No IDs with missing data found for", percentage))
      next  # Skip to the next iteration
    } else if (length(ids_with_missing) == 0) {
      message(paste("IDs with missing data list is empty for", percentage))
      next  # Skip to the next iteration
    }
    
    # Ensure IDs are of the same type
    ids_with_missing <- as.character(ids_with_missing)
    imputed_df$ID <- as.character(imputed_df$ID)
    final_df$ID <- as.character(final_df$ID)
    
    # Perform the comparison using the filtered IDs
    comparison_df <- compare_params(imputed_df, final_df, ids_with_missing)
    
    # Add metadata by creating new columns using the scalar values
    comparison_df <- comparison_df %>%
      mutate(
        Imputation_Method = factor(rep(method, nrow(.)), levels = desired_method_order),
        Missing_Percentage = factor(rep(percentage, nrow(.)), levels = c("5%", "20%", "50%", "75%"))
      )
    
    # Store in the list
    comparison_results[[name]] <- comparison_df
  }
}




# Combine all comparison results into one data frame
pk_param_comparison_combined <- bind_rows(comparison_results)

# Ensure Imputation_Method and Missing_Percentage are factors with desired levels
pk_param_comparison_combined <- pk_param_comparison_combined %>%
  mutate(
    Imputation_Method = factor(Imputation_Method, levels = desired_method_order),
    Missing_Percentage = factor(Missing_Percentage, levels = c("5%", "20%", "50%", "75%"))
  )

# Step 4: Reshape the data for plotting

# Reshape the data to have both Absolute Error and Percentage Error
pk_params_long <- pk_param_comparison_combined %>%
  pivot_longer(
    cols = c(CL_diff, VC_diff, KA_diff, CL_PE, VC_PE, KA_PE),
    names_to = c("Parameter", "ErrorType"),
    names_pattern = "([A-Z]{2})_(.*)",
    values_to = "ErrorValue"
  ) %>%
  mutate(
    ErrorType = case_when(
      ErrorType == "diff" ~ "Absolute Error",
      ErrorType == "PE" ~ "Percentage Error"
    ),
    Parameter = factor(Parameter, levels = c("CL", "VC", "KA")),
    ErrorType = factor(ErrorType, levels = c("Absolute Error", "Percentage Error")),
    Imputation_Method = factor(Imputation_Method, levels = desired_method_order)  # Ensure levels are set
  )

pk_params_long <- pk_params_long %>%
    mutate(Parameter = as.character(Parameter))

pk_params_long <- pk_params_long %>%
    mutate(Parameter = ifelse(Parameter == "VC", "V", Parameter))

pk_params_long <- pk_params_long %>%
    mutate(Parameter = factor(Parameter, levels = c("CL", "V", "KA")))


# Step 5: Create and save plots for each parameter

# Function to create a plot for a given parameter
create_parameter_plot <- function(param_name) {
  # Filter the data for the specific parameter
  data_param <- pk_params_long %>% filter(Parameter == param_name)
  
  # Create the plot
  plot_param <- ggplot(data_param, aes(x = Missing_Percentage, y = ErrorValue, fill = Imputation_Method)) +
    geom_boxplot(alpha = 0.7, outlier.shape = NA, color = "black", linewidth = 1.2) +
    geom_point(
      color = "black",
      position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.75),
      alpha = 1,
      size = 0.6,
      show.legend = FALSE
    ) +
    facet_grid2(ErrorType ~ ., scales = "free", independent = "y") +  # Facet by ErrorType
    labs(
      title = paste("Comparison of Absolute and Percentage Errors for", param_name),
      x = "Percentage of Missing Data",
      y = "Error Value",
      fill = "Imputation Method"
    ) +
    scale_color_jco() +
    scale_fill_jco() +
    my_theme +
    theme(
      strip.background = element_rect(fill = "grey90", color = "grey50"),
      strip.text = element_text(face = "bold", color = "black"),
      plot.title = element_text(hjust = 0.5),
      axis.text.x = element_text(angle = 45, hjust = 1),
      panel.spacing.y = unit(1, "cm"),
      panel.border = element_rect(
        colour = "black",
        fill = NA,
        size = 0.5
      )
    )
  
  return(plot_param)
}

# Get the unique parameters
parameters <- unique(pk_params_long$Parameter)

# Generate plots for each parameter and store them in a list
plots <- list()
for (param in parameters) {
  plots[[param]] <- create_parameter_plot(param)
}

# Use ggarrange to arrange the plots in 3 rows
final_plot <- ggarrange(
  plots[["CL"]], plots[["V"]], plots[["KA"]],
  ncol = 3, nrow = 1,  # Arrange in 3 rows
  labels = c("A", "B", "C"),  # Add labels (optional)
  common.legend = TRUE, legend = "bottom"  # Shared legend at the bottom
)


# Save the final plot with a wider output (28 inches wide)
png("PK_Error_Comparison_All.png", width = 28, height = 13.5, units = "in", res = 900)
print(final_plot)
dev.off()


# Compute MAE, MPE, RMSE, and RMSE% for each parameter
combined_stats <- pk_param_comparison_combined %>%
  group_by(Imputation_Method, Missing_Percentage) %>%
  summarise(
    # Metrics for CL
    MAE_CL = mean(abs(CL_imputed - CL_original), na.rm = TRUE),
    MPE_CL = mean(((CL_imputed - CL_original) / CL_original) * 100, na.rm = TRUE),
    RMSE_CL = sqrt(mean((CL_imputed - CL_original)^2, na.rm = TRUE)),
    RMSE_percent_CL = (RMSE_CL / mean(CL_original, na.rm = TRUE)) * 100,
    
    # Metrics for V
    MAE_V = mean(abs(VC_imputed - VC_original), na.rm = TRUE),
    MPE_V = mean(((VC_imputed - VC_original) / VC_original) * 100, na.rm = TRUE),
    RMSE_V = sqrt(mean((VC_imputed - VC_original)^2, na.rm = TRUE)),
    RMSE_percent_V = (RMSE_V / mean(VC_original, na.rm = TRUE)) * 100,
    
    # Metrics for KA
    MAE_KA = mean(abs(KA_imputed - KA_original), na.rm = TRUE),
    MPE_KA = mean(((KA_imputed - KA_original) / KA_original) * 100, na.rm = TRUE),
    RMSE_KA = sqrt(mean((KA_imputed - KA_original)^2, na.rm = TRUE)),
    RMSE_percent_KA = (RMSE_KA / mean(KA_original, na.rm = TRUE)) * 100,
    .groups = "drop"  # Drop grouping after summarization
  )

# Round values to 3 decimal places
combined_stats <- combined_stats %>%
  mutate(across(
    starts_with("MAE_") | starts_with("MPE_") | starts_with("RMSE_"),
    ~ round(.x, 3)
  ))



# Reorder columns for RMSE and RMSE%
combined_stats_wide <- combined_stats %>%
  select(
    Missing_Percentage, Imputation_Method, 
    MAE_CL, MAE_V, MAE_KA, 
    MPE_CL, MPE_V, MPE_KA, 
    RMSE_CL, RMSE_V, RMSE_KA, 
    RMSE_percent_CL, RMSE_percent_V, RMSE_percent_KA
  )

# Reorder Missing_Percentage and Imputation_Method
combined_stats_wide <- combined_stats_wide %>%
  arrange(
    Missing_Percentage,
    factor(Imputation_Method, levels = c("Amelia", "MF", "Mice", "MIDAS", "XGB"))
  )

# Update header data for the new structure
header_data <- data.frame(
  col_keys = c("Missing_Percentage", "Imputation_Method",
               "CL_MAE", "CL_MPE", "CL_RMSE", "CL_RMSE_percent", 
               "V_MAE", "V_MPE", "V_RMSE", "V_RMSE_percent",
               "KA_MAE", "KA_MPE", "KA_RMSE", "KA_RMSE_percent"),
  level1 = c("Missing Percentage", "Imputation Method", 
             "CL", "CL", "CL", "CL", 
             "V", "V", "V", "V", 
             "KA", "KA", "KA", "KA"),
  level2 = c("", "", 
             "MAE", "MPE", "RMSE", "RMSE%", 
             "MAE", "MPE", "RMSE", "RMSE%", 
             "MAE", "MPE", "RMSE", "RMSE%")
)

# Reorder and rename the columns in combined_stats_wide
combined_stats_wide <- combined_stats %>%
  transmute(
    Missing_Percentage,
    Imputation_Method,
    CL_MAE = MAE_CL, CL_MPE = MPE_CL, CL_RMSE = RMSE_CL, CL_RMSE_percent = RMSE_percent_CL,
    V_MAE = MAE_V, V_MPE = MPE_V, V_RMSE = RMSE_V, V_RMSE_percent = RMSE_percent_V,
    KA_MAE = MAE_KA, KA_MPE = MPE_KA, KA_RMSE = RMSE_KA, KA_RMSE_percent = RMSE_percent_KA
  ) %>%
  arrange(
    Missing_Percentage,
    factor(Imputation_Method, levels = c("Amelia", "MF", "Mice", "MIDAS", "XGB"))
  )

# Ensure header_data aligns perfectly with combined_stats_wide
header_data <- header_data %>%
  filter(col_keys %in% colnames(combined_stats_wide))

colnames(combined_stats_wide) <- header_data$col_keys

# Create the flextable with the updated structure
flextable_table <- flextable(combined_stats_wide) %>%
  set_header_df(mapping = header_data, key = "col_keys") %>%
  merge_h(part = "header") %>%
  merge_v(j = c("Missing_Percentage")) %>%  # Merge vertically by Missing_Percentage
  theme_box() %>%
  set_header_labels(
    Missing_Percentage = "Missing Percentage",
    Imputation_Method = "Imputation Method"
  ) %>%
  fontsize(size = 10, part = "all") %>%
  font(part = "all", fontname = "Times New Roman") %>%
  bold(part = "header") %>%
  align(align = "center", part = "all") %>%
  width(j = c("Missing_Percentage", "Imputation_Method"), width = 1.5) %>%
  width(j = c("CL_MAE", "CL_MPE", "CL_RMSE", "CL_RMSE_percent", 
              "V_MAE", "V_MPE", "V_RMSE", "V_RMSE_percent", 
              "KA_MAE", "KA_MPE", "KA_RMSE", "KA_RMSE_percent"), width = 1.5) %>%
  border_inner_h(border = fp_border(color = "gray", width = 1)) %>%
  border_outer(border = fp_border(color = "black", width = 1.5)) %>%
  padding(padding = 5, part = "all") %>%
  add_footer_lines(values = "This table summarizes MAE, MPE, RMSE, and RMSE% for each parameter (CL, V, KA), imputation method, and missing percentage.")

# Save the flextable in a Word document
doc <- read_docx()
doc <- body_add_flextable(doc, value = flextable_table)
print(doc, target = "Indiv_Param_Metrics_By_Parameter.docx")







# Iterate through each dataset in the list
for (dataset_name in names(all_data)) {
  # Skip "mrgsolv_val" if it's not structured like the others
  if (dataset_name == "mrgsolv_val") next
  
  pop_params <- all_data[[dataset_name]][["population_parameters"]]
  ind_params <- all_data[[dataset_name]][["parameters"]]
  
  Cl_pop <- as.numeric(pop_params$value[pop_params$parameter == "Cl_pop"])
  V_pop <- as.numeric(pop_params$value[pop_params$parameter == "V_pop"])
  ka_pop <- as.numeric(pop_params$value[pop_params$parameter == "ka_pop"])
  beta_Cl_logtCrCl <- as.numeric(pop_params$value[pop_params$parameter == "beta_Cl_logtCrCl"])
  beta_Cl_UM_1 <- exp(as.numeric(pop_params$value[pop_params$parameter == "beta_Cl_UM_1"]))
  beta_V_logtWT <- as.numeric(pop_params$value[pop_params$parameter == "beta_V_logtWEIGHT"])
  beta_ka_PPI_1 <- exp(as.numeric(pop_params$value[pop_params$parameter == "beta_ka_PPI_1"]))
  
  ind_params <- ind_params %>%
    mutate(
      CL_fixed = Cl_pop * (CrCl / 90)^beta_Cl_logtCrCl * beta_Cl_UM_1^UM,
      VCpri = V_pop * (WT / 70)^beta_V_logtWT,
      KApri = ka_pop * beta_ka_PPI_1^PPI
    )
  
  all_data[[dataset_name]][["parameters"]] <- ind_params
}


# Check one dataset to ensure the computation worked
head(all_data[["MLX_Amelia_1"]][["parameters"]])
library(dplyr)
library(stringr)
library(tidyr)

# Desired order of imputation methods for final plots, tables, etc.
desired_method_order <- c("Amelia", "MF", "Mice", "MIDAS", "XGB", "Original Data")

################################################################################
# Step 1: Extract PK parameters from all datasets and label them
################################################################################

# Initialize a list to store the parameter data frames
param_dfs <- list()

### 1A. Process the 'MLX_final_data' (the "Original Data")
final_data_params <- all_data$MLX_final_data$parameters %>%
  dplyr::group_by(id) %>%
  dplyr::summarise(
    CL = dplyr::first(Cl_mode),
    VC = dplyr::first(V_mode),
    KA = dplyr::first(ka_mode)
  ) %>%
  dplyr::ungroup() %>%
  dplyr::rename(ID = id) %>%
  # Label as '0% NA' + 'Original Data'
  dplyr::mutate(
    Missing_Percentage = factor("0% NA",
                                levels = c("0% NA", "5% NA", "20% NA", "50% NA", "75% NA")),
    Imputation_Method  = factor("Original Data", levels = desired_method_order)
  )

# Store the final data frame
param_dfs[["Final Data"]] <- final_data_params

### 1B. Create a character vector of dataset names, excluding both "MLX_final_data" and "mrgsolv_val"
dataset_names <- setdiff(names(all_data), c("MLX_final_data", "mrgsolv_val"))

# Process each imputed dataset
for (dataset_name in dataset_names) {
  
  # Extract the dataset from all_data
  dataset <- all_data[[dataset_name]]
  
  # If this dataset does not have $parameters, skip
  if (is.null(dataset$parameters)) {
    message(paste("Skipping", dataset_name, "because it has no 'parameters' component."))
    next
  }
  
  # Extract PK parameters
  params_df <- dataset$parameters %>%
    dplyr::group_by(id) %>%
    dplyr::summarise(
      CL = dplyr::first(Cl_mode),
      VC = dplyr::first(V_mode),
      KA = dplyr::first(ka_mode)
    ) %>%
    dplyr::ungroup() %>%
    dplyr::rename(ID = id)
  
  # Remove "MLX_" prefix if present
  short_name <- sub("^MLX_", "", dataset_name)
  
  # Determine the Imputation_Method
  if (grepl("Amelia", short_name, ignore.case = TRUE)) {
    method <- "Amelia"
  } else if (grepl("MF", short_name, ignore.case = TRUE)) {
    method <- "MF"
  } else if (grepl("Mice", short_name, ignore.case = TRUE)) {
    method <- "Mice"
  } else if (grepl("MIDAS", short_name, ignore.case = TRUE)) {
    method <- "MIDAS"
  } else if (grepl("XGB", short_name, ignore.case = TRUE)) {
    method <- "XGB"
  } else {
    stop(paste("Unknown method in dataset name:", short_name))
  }
  
  # Extract the missing percentage from the trailing number (_1, _2, _3, _4)
  # Assign "5% NA", "20% NA", etc.
  if (grepl("_1$", short_name)) {
    percentage <- "5% NA"
  } else if (grepl("_2$", short_name)) {
    percentage <- "20% NA"
  } else if (grepl("_3$", short_name)) {
    percentage <- "50% NA"
  } else if (grepl("_4$", short_name)) {
    percentage <- "75% NA"
  } else {
    stop(paste("Could not determine Missing_Percentage for dataset:", short_name))
  }
  
  # Add columns to params_df
  params_df <- params_df %>%
    dplyr::mutate(
      Missing_Percentage = factor(percentage,
                                  levels = c("0% NA", "5% NA", "20% NA", "50% NA", "75% NA")),
      Imputation_Method  = factor(method, levels = desired_method_order)
    )
  
  # Add to the list with the original dataset_name as key
  param_dfs[[dataset_name]] <- params_df
}

# param_dfs now contains:
# - "Final Data" for the Original Data
# - One entry per imputed dataset (excluding those with no parameters)

################################################################################
# Step 2: Extract IDs with missing data for each missing percentage
################################################################################

# Initialize a list to store IDs with missing data
missing_ids_list <- list()

# Loop over each dataset in 'missing_data_list'
# Expected naming in missing_data_list: e.g. "missing_5perc", "missing_20perc", ...
for (dataset_name in names(missing_data_list)) {
  
  # Attempt to extract numeric percentage from the dataset name
  # e.g. "missing_5perc" -> 5
  percentage_match <- str_extract(dataset_name, "\\d+")
  
  if (!is.na(percentage_match)) {
    # Append " NA" to match param_dfs labeling
    percentage_value <- as.integer(percentage_match)
    # e.g. "5" -> "5% NA"
    percentage <- paste0(percentage_value, "% NA")
  } else {
    stop(paste("Could not extract percentage from dataset name:", dataset_name))
  }
  
  # Retrieve the dataset with missing data
  data_with_missing <- missing_data_list[[dataset_name]]
  
  # Identify IDs that have any missing data in WT, CrCl, PPI, or UM
  ids_with_missing <- data_with_missing %>%
    filter(is.na(WT) | is.na(CrCl) | is.na(PPI) | is.na(UM)) %>%
    select(ID) %>%
    distinct()
  
  # Store the IDs in the list, keyed by "5% NA", "20% NA", etc.
  missing_ids_list[[percentage]] <- ids_with_missing$ID
}

################################################################################
# Step 3: Perform comparisons for IDs with missing data
################################################################################

compare_params <- function(imputed_df, final_df, ids_with_missing) {
  
  # Filter both data frames to only the IDs with missing data
  imputed_df_filtered <- imputed_df %>% filter(ID %in% ids_with_missing)
  final_df_filtered   <- final_df   %>% filter(ID %in% ids_with_missing)
  
  # Convert ID to character if needed
  imputed_df_filtered$ID <- as.character(imputed_df_filtered$ID)
  final_df_filtered$ID   <- as.character(final_df_filtered$ID)
  
  # Inner join on ID
  comparison_df <- imputed_df_filtered %>%
    inner_join(final_df_filtered, by = "ID", suffix = c("_imputed", "_original")) %>%
    mutate(
      CL_diff = CL_imputed - CL_original,
      VC_diff = VC_imputed - VC_original,
      KA_diff = KA_imputed - KA_original,
      CL_PE   = (CL_imputed - CL_original) / CL_original * 100,
      VC_PE   = (VC_imputed - VC_original) / VC_original * 100,
      KA_PE   = (KA_imputed - KA_original) / KA_original * 100
    )
  
  comparison_df
}

# Initialize a list to store comparison results
comparison_results <- list()

# Loop over the imputed datasets (excluding 'Final Data')
for (name in names(param_dfs)) {
  if (name != "Final Data") {
    
    imputed_df <- param_dfs[[name]]
    final_df   <- param_dfs[["Final Data"]]
    
    # Each imputed_df has exactly one method & missing percentage
    method     <- as.character(unique(imputed_df$Imputation_Method))
    percentage <- as.character(unique(imputed_df$Missing_Percentage))
    
    # Retrieve the IDs with missing data for this percentage
    ids_with_missing <- missing_ids_list[[percentage]]
    
    # If no such key or no IDs, skip
    if (is.null(ids_with_missing) || length(ids_with_missing) == 0) {
      message(paste("No IDs with missing data found for", percentage))
      next
    }
    
    # Ensure ID is character
    ids_with_missing <- as.character(ids_with_missing)
    imputed_df$ID    <- as.character(imputed_df$ID)
    final_df$ID      <- as.character(final_df$ID)
    
    # Perform the comparison
    comparison_df <- compare_params(imputed_df, final_df, ids_with_missing)
    
    # Tag with metadata
    comparison_df <- comparison_df %>%
      mutate(
        Imputation_Method  = factor(method, levels = desired_method_order),
        Missing_Percentage = factor(percentage,
                                    levels = c("5% NA", "20% NA", "50% NA", "75% NA", "0% NA"))
      )
    
    comparison_results[[name]] <- comparison_df
  }
}

################################################################################
# Step 4: Combine all comparison results
################################################################################

pk_param_comparison_combined <- bind_rows(comparison_results)

# If pk_param_comparison_combined is empty, you'll have no columns to mutate.
if (nrow(pk_param_comparison_combined) > 0) {
  
  # Final factor cleanup (if needed)
  pk_param_comparison_combined <- pk_param_comparison_combined %>%
    mutate(
      Imputation_Method  = factor(Imputation_Method,  levels = desired_method_order),
      Missing_Percentage = factor(Missing_Percentage, levels = c("0% NA", "5% NA", "20% NA", "50% NA", "75% NA"))
    )
  
} else {
  message("No comparison data was generated (pk_param_comparison_combined is empty).")
  # If you like, create empty factor columns for consistent structure:
  pk_param_comparison_combined <- tibble(
    ID                 = character(0),
    CL_imputed         = numeric(0),
    CL_original        = numeric(0),
    # ... any other columns you'd expect ...
    Imputation_Method  = factor(character(0), levels = desired_method_order),
    Missing_Percentage = factor(character(0), levels = c("0% NA","5% NA","20% NA","50% NA","75% NA"))
  )
}



# =============================================================================
# Step 5: Create and save plots for each parameter
# =============================================================================

# Function to create a plot for a given parameter
create_parameter_plot <- function(param_name) {
  # Filter the data for the specific parameter
  data_param <- pk_params_long %>% filter(Parameter == param_name)
  
  # Create the plot
  plot_param <- ggplot(data_param, aes(x = Missing_Percentage, y = ErrorValue, fill = Imputation_Method)) +
    geom_boxplot(alpha = 0.7, outlier.shape = NA, color = "black", linewidth = 1.2) +
    geom_point(
      color = "black",
      position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.75),
      alpha = 1,
      size = 0.6,
      show.legend = FALSE
    ) +
    facet_grid2(ErrorType ~ ., scales = "free", independent = "y") +  # Facet by ErrorType
    labs(
      title = paste("Comparison of Absolute and Percentage Errors for", param_name),
      x = "Percentage of Missing Data",
      y = "Error Value",
      fill = "Imputation Method"
    ) +
    scale_color_jco() +
    scale_fill_jco() +
    my_theme +
    theme(
      strip.background = element_rect(fill = "grey90", color = "grey50"),
      strip.text = element_text(face = "bold", color = "black"),
      plot.title = element_text(hjust = 0.5),
      axis.text.x = element_text(angle = 45, hjust = 1),
      panel.spacing.y = unit(1, "cm"),
      panel.border = element_rect(
        colour = "black",
        fill = NA,
        size = 0.5
      )
    )
  
  return(plot_param)
}

# Get the unique parameters
parameters <- unique(pk_params_long$Parameter)

# Generate plots for each parameter and store them in a list
plots <- list()
for (param in parameters) {
  plots[[param]] <- create_parameter_plot(param)
}

# Arrange the plots in one row with three columns
final_plot <- ggarrange(
  plots[["CL"]], plots[["V"]], plots[["KA"]],
  ncol = 3, nrow = 1,
  labels = c("A", "B", "C"),
  common.legend = TRUE, legend = "bottom"
)

# Save the combined plot
png("PKpri_Error_Comparison_All.png", width = 24, height = 13.5, units = 'in', res = 900)
print(final_plot)
dev.off()



# Compute MAE, MPE, RMSE, and RMSE% for each parameter
combined_stats <- pk_param_comparison_combined %>%
  group_by(Imputation_Method, Missing_Percentage) %>%
  summarise(
    # Metrics for CL
    MAE_CL = mean(abs(CL_imputed - CL_original), na.rm = TRUE),
    MPE_CL = mean(((CL_imputed - CL_original) / CL_original) * 100, na.rm = TRUE),
    RMSE_CL = sqrt(mean((CL_imputed - CL_original)^2, na.rm = TRUE)),
    RMSE_percent_CL = (RMSE_CL / mean(CL_original, na.rm = TRUE)) * 100,
    
    # Metrics for V
    MAE_V = mean(abs(VC_imputed - VC_original), na.rm = TRUE),
    MPE_V = mean(((VC_imputed - VC_original) / VC_original) * 100, na.rm = TRUE),
    RMSE_V = sqrt(mean((VC_imputed - VC_original)^2, na.rm = TRUE)),
    RMSE_percent_V = (RMSE_V / mean(VC_original, na.rm = TRUE)) * 100,
    
    # Metrics for KA
    MAE_KA = mean(abs(KA_imputed - KA_original), na.rm = TRUE),
    MPE_KA = mean(((KA_imputed - KA_original) / KA_original) * 100, na.rm = TRUE),
    RMSE_KA = sqrt(mean((KA_imputed - KA_original)^2, na.rm = TRUE)),
    RMSE_percent_KA = (RMSE_KA / mean(KA_original, na.rm = TRUE)) * 100,
    .groups = "drop"  # Drop grouping after summarization
  )

# Round values to 3 decimal places
combined_stats <- combined_stats %>%
  mutate(across(
    starts_with("MAE_") | starts_with("MPE_") | starts_with("RMSE_"),
    ~ round(.x, 3)
  ))



# Reorder columns for RMSE and RMSE%
combined_stats_wide <- combined_stats %>%
  select(
    Missing_Percentage, Imputation_Method, 
    MAE_CL, MAE_V, MAE_KA, 
    MPE_CL, MPE_V, MPE_KA, 
    RMSE_CL, RMSE_V, RMSE_KA, 
    RMSE_percent_CL, RMSE_percent_V, RMSE_percent_KA
  )

# Reorder Missing_Percentage and Imputation_Method
combined_stats_wide <- combined_stats_wide %>%
  arrange(
    Missing_Percentage,
    factor(Imputation_Method, levels = c("Amelia", "MF", "Mice", "MIDAS", "XGB"))
  )



# Update header data
header_data <- data.frame(
  col_keys = c("Missing_Percentage", "Imputation_Method",
               "MAE_CL", "MAE_V", "MAE_KA", 
               "MPE_CL", "MPE_V", "MPE_KA", 
               "RMSE_CL", "RMSE_V", "RMSE_KA", 
               "RMSE_percent_CL", "RMSE_percent_V", "RMSE_percent_KA"),
  level1 = c("Missing Percentage", "Imputation Method", 
             "MAE", "MAE", "MAE", 
             "MPE", "MPE", "MPE", 
             "RMSE", "RMSE", "RMSE", 
             "RMSE%", "RMSE%", "RMSE%"),
  level2 = c("", "", 
             "CL", "V", "KA", 
             "CL", "V", "KA", 
             "CL", "V", "KA", 
             "CL", "V", "KA")
)

# Ensure header_data aligns perfectly
header_data <- header_data %>%
  filter(col_keys %in% colnames(combined_stats_wide))

colnames(combined_stats_wide) <- header_data$col_keys

flextable_table <- flextable(combined_stats_wide) %>%
  set_header_df(mapping = header_data, key = "col_keys") %>%
  merge_h(part = "header") %>%
  merge_v(j = c("Missing_Percentage")) %>%  # Merge vertically by Missing_Percentage
  theme_box() %>%
  set_header_labels(
    Missing_Percentage = "Missing Percentage",
    Imputation_Method = "Imputation Method"
  ) %>%
  fontsize(size = 10, part = "all") %>%
  font(part = "all", fontname = "Times New Roman") %>%
  bold(part = "header") %>%
  align(align = "center", part = "all") %>%
  width(j = c("Missing_Percentage", "Imputation_Method"), width = 1.5) %>%
  width(j = c("MAE_CL", "MAE_V", "MAE_KA", 
              "MPE_CL", "MPE_V", "MPE_KA", 
              "RMSE_CL", "RMSE_V", "RMSE_KA", 
              "RMSE_percent_CL", "RMSE_percent_V", "RMSE_percent_KA"), width = 1.5) %>%
  border_inner_h(border = fp_border(color = "gray", width = 1)) %>%
  border_outer(border = fp_border(color = "black", width = 1.5)) %>%
  padding(padding = 5, part = "all") %>%
  add_footer_lines(values = "This table summarizes MAE, MPE, RMSE, and RMSE% for each parameter (KA, V, CL), imputation method, and missing percentage.")


doc <- read_docx()
doc <- body_add_flextable(doc, value = flextable_table)
print(doc, target = "Pop Param Pri Metrics.docx")








# -------------------------------
# Step A: Compute Individual Parameters
# -------------------------------
for (dataset_name in names(all_data_subset)) {
  # Access the population and individual parameters for the dataset
  pop_params <- all_data[[dataset_name]][["population_parameters"]]
  ind_params <- all_data[[dataset_name]][["parameters"]]
  
  # Extract required values from population parameters
  Cl_pop <- pop_params$value[pop_params$parameter == "Cl_pop"]
  V_pop <- pop_params$value[pop_params$parameter == "V_pop"]
  ka_pop <- pop_params$value[pop_params$parameter == "ka_pop"]
  beta_Cl_logtCrCl <- pop_params$value[pop_params$parameter == "beta_Cl_logtCrCl"]
  beta_Cl_UM_1 <- exp(pop_params$value[pop_params$parameter == "beta_Cl_UM_1"])
  beta_V_logtWT <- pop_params$value[pop_params$parameter == "beta_V_logtWEIGHT"]
  beta_ka_PPI_1 <- exp(pop_params$value[pop_params$parameter == "beta_ka_PPI_1"])
  
  # Compute individual PK parameters for each subject
  ind_params <- ind_params %>%
    mutate(
      CLpri = Cl_pop * (CrCl / 90)^beta_Cl_logtCrCl * beta_Cl_UM_1^UM,
      VCpri = V_pop * (WT / 70)^beta_V_logtWT,
      KApri = ka_pop * beta_ka_PPI_1^PPI
    )
  
  # Update the parameters dataframe in the all_data list
  all_data[[dataset_name]][["parameters"]] <- ind_params
}

# Check one dataset to ensure the computation worked (using the MLX_ prefix)
head(all_data[["MLX_amelia_1"]][["parameters"]])

# -------------------------------
# Step B: Combine Data for Plotting/Analysis
# -------------------------------
combined_data_list <- list()

for (dataset_name in names(all_data_subset)) {
    # Skip final data; note: final data now has the MLX_ prefix
    if (dataset_name == "MLX_final_data") next
    
    dataset <- all_data[[dataset_name]]
    print(paste("Processing dataset:", dataset_name))
    
    if ("parameters" %in% names(dataset)) {
        print("parameters exists")
        parameters_df <- dataset$parameters
        
        # Check for required columns (CLpri, VCpri, KApri)
        if (all(c("CLpri", "VCpri", "KApri") %in% names(parameters_df))) {
            print("Required columns exist")
            
            # Remove the MLX_ prefix for extracting method and missing percentage
            name_no_prefix <- sub("^MLX_", "", dataset_name)
            # Extract imputation method (text before the first underscore)
            method <- sub("_.*", "", name_no_prefix)
            # Extract missing percentage code (text after the last underscore)
            missing_perc <- sub(".*_", "", name_no_prefix)
            
            # Build a temporary data frame
            temp_df <- parameters_df %>%
                select(ID = id, CLpri, VCpri, KApri) %>%
                mutate(
                    Imputation_Method = method,
                    Missing_Percentage = missing_perc
                )
            
            # Store in our list
            combined_data_list[[dataset_name]] <- temp_df
        } else {
            print("Required columns missing")
        }
    } else {
        print("parameters missing")
    }
}

# Process the final data separately (dataset name: MLX_final_data)
if ("MLX_final_data" %in% names(all_data_subset)) {
  final_parameters_df <- all_data$MLX_final_data$parameters
  if (all(c("CLpri", "VCpri", "KApri") %in% names(final_parameters_df))) {
    final_temp_df <- final_parameters_df %>%
      select(ID = id, CLpri, VCpri, KApri)
    
    # Here final data represents the complete dataset so assign a missing percentage of "0"
    # and assign it to all imputation methods.
    imputation_methods <- c("amelia", "MF", "mice", "midas", "xgb")
    final_data_list <- list()
    for (method in imputation_methods) {
      method_df <- final_temp_df %>%
        mutate(
          Imputation_Method = method,
          Missing_Percentage = "0"  # will be recoded later
        )
      final_data_list[[method]] <- method_df
    }
    final_combined_df <- bind_rows(final_data_list)
    combined_data_list[["MLX_final_data"]] <- final_combined_df
  }
}

# Combine all data frames into one
combined_mlx_val <- bind_rows(combined_data_list)

# Recode the imputation method and missing percentage values
combined_mlx_val <- combined_mlx_val %>%
  mutate(
    Imputation_Method = recode(Imputation_Method,
      "amelia" = "Amelia",
      "mice" = "Mice",
      "midas" = "MIDAS",
      "xgb" = "XGB"
      # "MF" remains "MF"
    ),
    Missing_Percentage = recode(Missing_Percentage,
      "0" = "0%",
      "1" = "5%",
      "2" = "20%",
      "3" = "50%",
      "4" = "75%"
    )
  ) %>%
  mutate(
    Imputation_Method = factor(Imputation_Method, levels = c("Amelia", "MF", "Mice", "MIDAS", "XGB")),
    Missing_Percentage = factor(Missing_Percentage, levels = c("0%", "5%", "20%", "50%", "75%"))
  )

# -------------------------------
# Step C: Plotting
# -------------------------------
plot_histogram_without_curves <- function(data, variable, variable_name) {
  # Set binwidth based on the variable
  binwidth_value <- if (variable == "CLpri") 0.2 else if (variable == "VCpri") 1 else 0.1
  
  # Filter out non-positive values
  data_filtered <- data %>% filter(!!sym(variable) > 0)
  
  # Define a custom theme (here using theme_bw())
  my_theme <- theme_bw()
  
  # Create the histogram plot
plot_histogram_without_curves <- function(data, variable, variable_name) {
  # Set binwidth based on the variable
  binwidth_value <- if (variable == "CLpri") 0.2 else if (variable == "VCpri") 1 else 0.1
  
  # Filter out non-positive values
  data_filtered <- data %>% filter(!!sym(variable) > 0)
  
  # Helper function to compute the mode via histogram bins
  compute_mode <- function(x, binwidth) {
    x <- x[!is.na(x)]
    if (length(x) == 0) return(NA)
    # If all values are the same, return that value
    if (min(x) == max(x)) return(min(x))
    # Extend one binwidth beyond the max to ensure coverage
    breaks <- seq(min(x), max(x) + binwidth, by = binwidth)
    h <- hist(x, breaks = breaks, plot = FALSE, include.lowest = TRUE)
    h$mids[which.max(h$counts)]
  }
  
  # Compute summary statistics (mean and mode) for each facet grouping
  summary_df <- data_filtered %>%
    group_by(Imputation_Method, Missing_Percentage) %>%
    summarise(
      mean_value = mean(!!sym(variable), na.rm = TRUE),
      mode_value = compute_mode(!!sym(variable), binwidth_value),
      .groups = "drop"
    )
  
  # Base histogram
  plot <- ggplot(data_filtered, aes_string(x = variable)) +
    geom_histogram(
      aes(y = ..count.., fill = Imputation_Method),
      binwidth = binwidth_value, color = "black", alpha = 0.7, show.legend = TRUE
    ) +
    facet_grid(Imputation_Method ~ Missing_Percentage, scales = "free_y") +
    scale_fill_jco() +
    labs(
      title = paste("Distribution of", variable_name),
      subtitle = "Red dashed line = Mean; Blue dotted line = Mode",
      x = variable_name,
      y = "Count"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      axis.title = element_text(size = 16),
      axis.text = element_text(size = 14),
      strip.text = element_text(face = "bold"),
      panel.spacing = unit(1, "lines"),
      legend.position = "top",
      legend.key = element_rect(fill = "white", color = "black"),
      legend.title = element_text(face = "bold"),
      legend.text = element_text(size = 12),
      panel.border = element_rect(color = "grey", fill = NA, size = 1),
      strip.background = element_rect(fill = "grey", color = "grey", size = 1)
    ) +
    guides(fill = guide_legend(title = "Imputation Method"))
  
  # Add vertical lines for mean (red, dashed) and mode (blue, dotted)
  plot <- plot +
    geom_vline(
      data = summary_df, aes(xintercept = mean_value),
      color = "red", linetype = "dashed", size = 1
    ) +
    geom_vline(
      data = summary_df, aes(xintercept = mode_value),
      color = "blue", linetype = "dotted", size = 1
    )
  
  # Decide where to place text based on the parameter
  if (variable_name == "KA") {
    # For KA, place text in the top-left corner
    plot <- plot +
      geom_text(
        data = summary_df,
        aes(x = -Inf, y = Inf, 
            label = paste0("Mean=", round(mean_value, 2))),
        color = "red", hjust = -0.1, vjust = 2, size = 3
      ) +
      geom_text(
        data = summary_df,
        aes(x = -Inf, y = Inf,
            label = paste0("Mode=", round(mode_value, 2))),
        color = "blue", hjust = -0.1, vjust = 4, size = 3
      )
  } else {
    # For CL or V, place text in the top-right corner
    plot <- plot +
      geom_text(
        data = summary_df,
        aes(x = Inf, y = Inf,
            label = paste0("Mean=", round(mean_value, 2))),
        color = "red", hjust = 1.1, vjust = 2, size = 3
      ) +
      geom_text(
        data = summary_df,
        aes(x = Inf, y = Inf,
            label = paste0("Mode=", round(mode_value, 2))),
        color = "blue", hjust = 1.1, vjust = 4, size = 3
      )
  }
  
  return(plot)
}


# Generate histogram plots for each parameter
CL_Plot_simple <- plot_histogram_without_curves(combined_mlx_val, "CLpri", "CL")
V_Plot_simple <- plot_histogram_without_curves(combined_mlx_val, "VCpri", "V")
KA_Plot_simple <- plot_histogram_without_curves(combined_mlx_val, "KApri", "KA")

# Arrange the plots (3 rows) and save to a PNG file
png("PK_param_Prior_Simple_Distribution.png", width = 12, height = 18, units = 'in', res = 900)
ggarrange(KA_Plot_simple, V_Plot_simple, CL_Plot_simple, nrow = 3, common.legend = TRUE)
dev.off()




















# Define your custom theme if not already defined
my_theme <- theme_bw() +
  theme(
    plot.title = element_text(size = 12, hjust = 0.5),
    axis.title.x = element_text(size = 10),
    axis.title.y = element_text(size = 10)
  )


# Define the directory where your .mlxtran files are stored
mlxtran_directory <- "C:/Users/duflo/OneDrive/Bureau/AAPSJ-D-25-00020/R1/monolix_projects"

# Helper function to extract the method name and percentage of NA
# after removing the "MLX_" prefix
extract_project_info <- function(filename_no_prefix) {
  # Split on underscores
  name_parts <- unlist(strsplit(filename_no_prefix, "_"))
  
  # The first part is the "project_name"
  project_name <- name_parts[1]
  
  # The second part (if present) indicates the missing-percentage code
  if (length(name_parts) > 1) {
    na_percentage <- switch(
      name_parts[2],
      "1" = "5%",
      "2" = "20%",
      "3" = "50%",
      "4" = "75%",
      "data" = "0%",  # e.g., "final_data"
      "0%"            # default if no match
    )
  } else {
    na_percentage <- "0%"
  }
  
  return(list(project_name = project_name, na_percentage = na_percentage))
}

# Get a list of all .mlxtran files in the directory
mlxtran_files <- list.files(path = mlxtran_directory, pattern = "*.mlxtran", full.names = TRUE)

# Define fixed x and y axis limits
x_limits <- c(0, 12.5)
y_limits <- c(0, 10)

# Initialize an empty list to store the plots
vpc_plots <- list()

# Loop through each .mlxtran file to create the plots
for (i in seq_along(mlxtran_files)) {
  
  # Full path to the project
  project_file <- mlxtran_files[i]
  
  # Remove the directory path and the extension
  filename <- tools::file_path_sans_ext(basename(project_file))
  
  # Remove the "MLX_" prefix if present
  filename_no_prefix <- sub("^MLX_", "", filename)
  
  # Extract project info (method name, missing percentage)
  project_info <- extract_project_info(filename_no_prefix)
  
  # Load the Monolix project
  loadProject(project_file)
  
  # Draw the VPC without stratification
  vpc_plot <- plotVpc(
    obsName = "DV",
    settings = list(
      outlierDots = FALSE,
      useCorrpred = TRUE,
      grid = FALSE,
      ylab = "Concentration (mg/L)",
      empPercentiles = TRUE,
      predPercentiles = TRUE,
      empirical = TRUE,
      theoretical = TRUE,
      xlab = "Time (hour)"
    )
  )
  
  # Build the new title
  new_title <- paste0(
    project_info$project_name,
    "  Missing Percentage: ", project_info$na_percentage
  )
  
  # Add consistent x and y axis limits, title, and custom theme
  vpc_plot <- vpc_plot +
    ggtitle(new_title) +
    xlim(x_limits) +
    ylim(y_limits) +
    my_theme
  
  # Store the plot in the list
  vpc_plots[[i]] <- vpc_plot
}

# OPTIONAL: If you specifically want to rename the 5th plot to "Original Data"
# with 0% missing, you can do so here:
if (length(vpc_plots) >= 5) {
  vpc_plots[[5]] <- vpc_plots[[5]] +
    ggtitle("Original Data")
}

# If you want to standardize certain lowercase words (e.g., "amelia" -> "Amelia"),
# you can do it in a loop:
for (i in seq_along(vpc_plots)) {
  current_title <- vpc_plots[[i]]$labels$title
  
  # Replace words as needed
  updated_title <- current_title %>%
    str_replace("amelia", "Amelia") %>%
    str_replace("mice", "Mice") %>%
    str_replace("midas", "MIDAS") %>%
    str_replace("xgb", "XGB") %>%
    str_replace("mf", "MF")
  
  vpc_plots[[i]] <- vpc_plots[[i]] + ggtitle(updated_title)
}

# Example of a custom arrangement
plot_indices <- c(
  5, 1, 2, 3, 4, 5, 6, 7, 8, 9,
  5, 10, 11, 12, 13, 5, 14, 15,
  16, 17, 5, 18, 19, 20, 21
)

combined_plot <- ggarrange(
  plotlist = vpc_plots[plot_indices],
  ncol = 5,
  nrow = 5,
  common.legend = TRUE,
  legend = "bottom"
)


png("pcVPC.png", width = 18, height = 12, units = 'in', res = 900)
print(combined_plot)
dev.off()






# Define the desired order of imputation methods
imputation_methods <- c("Amelia", "MF", "Mice", "MIDAS", "XGB")

# Define the missing data percentages
missing_percentages <- c("0% NA", "5% NA", "20% NA", "50% NA", "75% NA")

# Initialize a list to store the data frames
predictions_list <- list()

# ------------------------------------------------------------------------------
# Step A: Process the "final_data" (no MLX_ prefix here)
# ------------------------------------------------------------------------------
final_predictions <- all_data_subset[["MLX_final_data"]][["predictions"]]

# Check if 'final_predictions' exists and has the required columns
if (!is.null(final_predictions) && all(c("DV", "popPred", "indivPred_mode") %in% names(final_predictions))) {
  
  # Assign Missing_Percentage as "0% NA"
  final_predictions <- final_predictions %>%
    mutate(
      Missing_Percentage = factor("0% NA", levels = missing_percentages)
    )
  
  # For each imputation method, create a copy of final_predictions
  for (method in imputation_methods) {
    method_predictions <- final_predictions %>%
      mutate(
        Imputation_Method = factor(method, levels = imputation_methods)
      )
    
    # Store in the list with a key combining method and percentage
    key <- paste(method, "0% NA", sep = "_")
    predictions_list[[key]] <- method_predictions
  }
}

# ------------------------------------------------------------------------------
# Step B: Process each imputed dataset (remove "MLX_" prefix)
# ------------------------------------------------------------------------------
dataset_names <- setdiff(names(all_data_subset), "MLX_final_data")

for (dataset_name in dataset_names) {
  
  # Extract the dataset
  dataset <- all_data[[dataset_name]]
  
  # Extract the 'predictions' data frame
  predictions_df <- dataset[["predictions"]]
  
  # Check if 'predictions' data frame exists and has the required columns
  if (!is.null(predictions_df) && all(c("DV", "popPred", "indivPred_mode") %in% names(predictions_df))) {
    
    # Remove "MLX_" prefix if present
    name_without_prefix <- sub("^MLX_", "", dataset_name)
    
    # Determine the Imputation_Method based on dataset name (now prefix-free)
    if (grepl("^amelia", name_without_prefix, ignore.case = TRUE)) {
      method <- "Amelia"
    } else if (grepl("^MF", name_without_prefix, ignore.case = TRUE)) {
      method <- "MF"
    } else if (grepl("^mice", name_without_prefix, ignore.case = TRUE)) {
      method <- "Mice"
    } else if (grepl("^MIDAS", name_without_prefix, ignore.case = TRUE)) {
      method <- "MIDAS"
    } else if (grepl("^XGB", name_without_prefix, ignore.case = TRUE)) {
      method <- "XGB"
    } else {
      stop(paste("Unknown method in dataset name:", dataset_name))
    }
    
    # Extract the missing percentage from the dataset name (prefix-free)
    if (grepl("_1$", name_without_prefix)) {
      percentage <- "5% NA"
    } else if (grepl("_2$", name_without_prefix)) {
      percentage <- "20% NA"
    } else if (grepl("_3$", name_without_prefix)) {
      percentage <- "50% NA"
    } else if (grepl("_4$", name_without_prefix)) {
      percentage <- "75% NA"
    } else {
      stop(paste("Could not determine Missing_Percentage for dataset:", dataset_name))
    }
    
    # Assign Missing_Percentage and Imputation_Method
    predictions_df <- predictions_df %>%
      mutate(
        Missing_Percentage = factor(percentage, levels = missing_percentages),
        Imputation_Method = factor(method, levels = imputation_methods)
      )
    
    # Store in the list with a key combining method and missing percentage
    key <- paste(method, percentage, sep = "_")
    predictions_list[[key]] <- predictions_df
  } else {
    warning(paste("Dataset", dataset_name, "does not have the required columns for plotting."))
  }
}

# ------------------------------------------------------------------------------
# Step C: Combine and Reshape
# ------------------------------------------------------------------------------
all_predictions <- bind_rows(predictions_list, .id = "Dataset")

all_predictions_long <- all_predictions %>%
  select(id, time, DV, popPred, indivPred_mode, Imputation_Method, Missing_Percentage) %>%
  pivot_longer(
    cols = c(popPred, indivPred_mode),
    names_to = "Prediction_Type",
    values_to = "Predicted_Value"
  ) %>%
  mutate(
    Prediction_Type = factor(
      Prediction_Type,
      levels = c("popPred", "indivPred_mode"),
      labels = c("Population Predictions", "Individual Predictions")
    ),
    Imputation_Method = factor(Imputation_Method, levels = imputation_methods),
    Missing_Percentage = factor(Missing_Percentage, levels = missing_percentages),
    Missing_Percentage = dplyr::recode_factor(
      Missing_Percentage,
      "0% NA"  = "Missing Percentage: 0%",
      "5% NA"  = "Missing Percentage: 5%",
      "20% NA" = "Missing Percentage: 20%",
      "50% NA" = "Missing Percentage: 50%",
      "75% NA" = "Missing Percentage: 75%"
    )
  )

# Split data into Population and Individual Predictions
population_data <- all_predictions_long %>%
  filter(Prediction_Type == "Population Predictions")

individual_data <- all_predictions_long %>%
  filter(Prediction_Type == "Individual Predictions")

# ------------------------------------------------------------------------------
# Step D: Create Plots
# ------------------------------------------------------------------------------

# Population Predictions plot
population_plot <- ggplot(population_data, aes(x = Predicted_Value, y = DV, color = Imputation_Method)) +
  geom_point(alpha = 0.6, size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  facet_grid(Imputation_Method ~ Missing_Percentage, scales = "free") +
  # Change the color legend title to "Imputation Method"
scale_color_jco(
  name = "Imputation Method",
  guide = guide_legend(override.aes = list(size = 3))
) + labs(
    title = "Observations vs. Population Predictions",
    x = "Predicted Concentration (mg/L)",
    y = "Observed Concentration (mg/L)"
  ) +
  my_theme +
  theme(
    strip.background = element_rect(fill = "grey90", color = "grey50"),
    strip.text = element_text(face = "bold", color = "black"),
    axis.text.x = element_text(angle = 45, hjust = 1),
  legend.key.size = unit(1.5, "lines") , # Adjust as needed
    panel.spacing.y = unit(1, "cm"),
    plot.title = element_text(size = 14, hjust = 0.5),  # Increase title size
    legend.title = element_text(size = 14),             # Increase legend title size
    legend.text = element_text(size = 12) ,
    panel.border = element_rect(colour = "black", fill = NA, size = 0.5)
  )

# Individual Predictions plot
individual_plot <- ggplot(individual_data, aes(x = Predicted_Value, y = DV, color = Imputation_Method)) +
  geom_point(alpha = 0.6, size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  facet_grid(Imputation_Method ~ Missing_Percentage, scales = "free") +
  # Change the color legend title to "Imputation Method"
scale_color_jco(
  name = "Imputation Method",
  guide = guide_legend(override.aes = list(size = 3))
) +  labs(
    title = "Observations vs. Individual Predictions",
    x = "Predicted Concentration (mg/L)",
    y = "Observed Concentration (mg/L)"
  ) +
  my_theme +
  theme(
    strip.background = element_rect(fill = "grey90", color = "grey50"),
    strip.text = element_text(face = "bold", color = "black"),
    axis.text.x = element_text(angle = 45, hjust = 1),
  legend.key.size = unit(1.5, "lines"),  # Adjust as needed
    panel.spacing.y = unit(1, "cm"),
    panel.border = element_rect(colour = "black", fill = NA, size = 0.5),
    plot.title = element_text(size = 14, hjust = 0.5),  # Increase title size
    legend.title = element_text(size = 14),             # Increase legend title size
    legend.text = element_text(size = 12)  
  )

# ------------------------------------------------------------------------------
# Step E: Arrange Plots
# ------------------------------------------------------------------------------
combined_plot <- ggarrange(
  population_plot,
  individual_plot,
  ncol = 2,
  labels = c("A", "B"),
  align = "hv",
  common.legend = TRUE
)

# Optional: Add an overall title
combined_plot <- annotate_figure(
  combined_plot,
  top = text_grob(
    "Observations vs. Predictions Across Imputation Methods and Missing Data Percentages", 
    face = "bold", size = 18
  )
)


# Save the combined plot as a PNG
png("ObsPred.png", width = 20, height = 12, units = 'in', res = 900)
print(combined_plot)
dev.off()



# Compute statistics for population_data
population_stats <- population_data %>%
  group_by(Imputation_Method, Missing_Percentage) %>%
  summarise(
    MAE = mean(abs(DV - Predicted_Value), na.rm = TRUE),
    MPE = mean(((DV - Predicted_Value) / DV) * 100, na.rm = TRUE),
    RMSE = sqrt(mean((DV - Predicted_Value)^2, na.rm = TRUE)),
    RMSE_percent = (RMSE / mean(DV, na.rm = TRUE)) * 100,
    R_squared = cor(DV, Predicted_Value, use = "complete.obs")^2
  ) %>%
  ungroup()


# Compute statistics for individual_data
individual_stats <- individual_data %>%
  group_by(Imputation_Method, Missing_Percentage) %>%
  summarise(
    MAE = mean(abs(DV - Predicted_Value), na.rm = TRUE),
    MPE = mean(((DV - Predicted_Value) / DV) * 100, na.rm = TRUE),
    RMSE = sqrt(mean((DV - Predicted_Value)^2, na.rm = TRUE)),
    RMSE_percent = (RMSE / mean(DV, na.rm = TRUE)) * 100,
    R_squared = cor(DV, Predicted_Value, use = "complete.obs")^2
  ) %>%
  ungroup()



# Round the statistics for population_data
population_stats <- population_stats %>%
  mutate(
    MAE = round(MAE, 4),
    MPE = round(MPE, 2),
    RMSE = round(RMSE, 4),
    RMSE_percent = round(RMSE_percent, 2),
    R_squared = round(R_squared, 4)
  )

# Round the statistics for individual_data
individual_stats <- individual_stats %>%
  mutate(
    MAE = round(MAE, 4),
    MPE = round(MPE, 2),
    RMSE = round(RMSE, 4),
    RMSE_percent = round(RMSE_percent, 2),
    R_squared = round(R_squared, 4)
  )

# Add Prediction_Type to each data frame
population_stats <- population_stats %>%
  mutate(Prediction_Type = "Population")

individual_stats <- individual_stats %>%
  mutate(Prediction_Type = "Individual")

# Combine the statistics
combined_stats <- bind_rows(population_stats, individual_stats)

# Arrange columns in desired order
combined_stats <- combined_stats %>%
  select(Imputation_Method, Missing_Percentage, Prediction_Type,
         MAE, MPE, RMSE, RMSE_percent, R_squared)


combined_stats <- combined_stats %>%
  mutate(across(c(R_squared, RMSE, RMSE_percent, MAE, MPE), ~ round(.x, 3)))

# View combined statistics
print(combined_stats)


# Reshape the data: explicitly maintain alignment for each metric and prediction type
wide_combined_stats <- combined_stats %>%
  pivot_wider(
    names_from = Prediction_Type,
    values_from = c(MAE, MPE, RMSE, RMSE_percent, R_squared),
    names_glue = "{Prediction_Type}_{.value}"
  )

# Ensure column names align correctly by reviewing wide_combined_stats
col_order <- c(
  "Imputation_Method", "Missing_Percentage",
  "Population_MAE", "Population_MPE", "Population_RMSE", "Population_RMSE_percent", "Population_R_squared",
  "Individual_MAE", "Individual_MPE", "Individual_RMSE", "Individual_RMSE_percent", "Individual_R_squared"
)

wide_combined_stats <- wide_combined_stats[, col_order]

# Create the flextable
flextable_table <- flextable(wide_combined_stats)

# Add two-level header: Population and Individual first, metrics below
flextable_table <- flextable_table %>%
  set_header_df(mapping = data.frame(
    col_keys = colnames(wide_combined_stats),
    # First row of headers: Group by Population and Individual
    Population_Group = c(
      "Imputation Method", "Missing Percentage", 
      rep("Population", 5), rep("Individual", 5)
    ),
    # Second row of headers: Split by Metrics
    Metric_Group = c(
      "", "", "MAE", "MPE", "RMSE", "RMSE_percent", "R_squared",
      "MAE", "MPE", "RMSE", "RMSE_percent", "R_squared"
    )
  )) %>%
  merge_v(j = c("Imputation_Method", "Missing_Percentage")) %>%
  merge_h(part = "header") %>%
  autofit() %>%
  theme_vanilla()

# Print the flextable
flextable_table

# Create a new Word document
doc <- read_docx()

# Add the flextable to the Word document
doc <- body_add_flextable(doc, value = flextable_table)

# Save the Word document to a file
print(doc, target = "Statistical_Metrics_Table.docx")
