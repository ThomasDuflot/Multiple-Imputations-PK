library(lixoftConnectors)
initializeLixoftConnectors(software = "monolix")
library(mrgsolve)
library(PopED)
library(MASS) 
library(dplyr) 
library(ggplot2)
library(missForest)
library(rMIDAS)
library(dplyr)
library(mice)
library(FactoMineR)
library(ggcorrplot)
library(tidyr)
library(ggsci)
library(ggpubr)
library(Amelia)
library(xgboost)
library(caret)
library(PFIM)
library(magick)
library(VIM)
library(stringr)
library(ggh4x)
library(purrr)
library(flextable)
library(officer)

set.seed(123)
# Define the means and standard deviations for each parameter
Height <- 170  # cm
Height_sd <- 10

WT <- 70  # kg
WT_sd <- 15

Alb <- 42  # g/L
Alb_sd <- 4

CrCl <- 90  # mL/min/1.73m2
CrCl_sd <- 15

Age <- 50  # years
Age_sd <- 15

# Correlation matrix between parameters
# Adding Age with hypothetical correlations to Weight, Albumin, and Renal Function
cor_matrix <- matrix(c(1, 0.7, 0, 0, 0.2,   # Height
                       0.7, 1, 0, 0, 0.3,   # WT
                       0, 0, 1, 0.5, -0.2,  # Alb
                       0, 0, 0.5, 1, -0.4,  # CrCl
                       0.2, 0.3, -0.2, -0.4, 1), # Age
                     nrow=5, byrow=TRUE)

# Parameter means in the same order as the correlation matrix
means <- c(Height, WT, Alb, CrCl, Age)

# Standard deviations of the parameters
sds <- c(Height_sd, WT_sd, Alb_sd, CrCl_sd, Age_sd)

# Calculate the covariance matrix from the correlation matrix and standard deviations
cov_matrix <- cor_matrix * (sds %*% t(sds))

# Generate 100 patients with correlated distributions
set.seed(123)  # For reproducibility
sim_data <- mvrnorm(n = 100, mu = means, Sigma = cov_matrix)

# Convert to a dataframe and rename columns
sim_data <- as.data.frame(sim_data)
colnames(sim_data) <- c("Height", "WT", "Alb", "CrCl", "Age")

# Display a preview of the simulated data
head(sim_data)

# Add binary variables for PPI and UM
sim_data$PPI <- rbinom(n = 100, size = 1, prob = 0.5)
sim_data$UM <- rbinom(n = 100, size = 1, prob = 0.5)
sim_data$ID <- 1:100

# Optional: Check correlations in the simulated data
cor(sim_data)




modelEquations = list(

  outcomes = list( "RespPK"),

  equations = list(  "RespPK" = "dose_RespPK/V * ka/(ka - Cl/V) * (exp(-Cl/V * t) - exp(-ka * t))"))

# model parameters
modelParameters = list(
  ModelParameter( name = "V",    distribution = LogNormal( mu = 10, omega = sqrt(0.09)) ),
  ModelParameter( name = "Cl",   distribution = LogNormal( mu = 1, omega = sqrt(0.09 )) ),
  ModelParameter( name = "ka",   distribution = LogNormal( mu = 1, omega = sqrt(0.09)) ))


# error Model
errorModelRespPK = Combined1( outcome = "RespPK", sigmaInter = 0.1, sigmaSlope = 0.15 )
modelError = list( errorModelRespPK )

## sampling times
samplingTimesRespPK = SamplingTimes( outcome = "RespPK", samplings = c(0.25,0.5,0.75,1,2,4,8,32,32.25,32.5,32.75,33,34,36,40))

# arm
administrationRespPK = Administration( outcome = "RespPK", tau=8, dose = c( 100 ))

arm1 = Arm( name = "BrasTest1",
            size = 100,
            administrations = list( administrationRespPK ),
            samplingTimes   = list( samplingTimesRespPK  ))



design1 = Design( name = "design1", arms = list( arm1 ))



evaluationPop = Evaluation( name = "",
                            modelEquations = modelEquations,
                            modelParameters = modelParameters,
                            modelError = modelError,
                            outcomes = list( "RespPK"),
                            designs = list( design1 ),
                            fim = "population",
                            odeSolverParameters = list( atol = 1e-8, rtol = 1e-8 ) )

evaluationPop = run( evaluationPop )


outputPath = "D:/MOBILITE"
outputFile = "EvaluationPopFIM.html"
plotOptions = list( unitTime=c("hour"), unitResponses= c("mcg/mL") )

Report( evaluationPop, outputPath, outputFile, plotOptions )




# constraints
administrationConstraintsRespPK = AdministrationConstraints( outcome = "RespPK", doses = c( 100) )
		
samplingConstraintsRespPK  = SamplingTimeConstraints( outcome = "RespPK",
                                                      initialSamplings = c(0.25,0.5,0.75,1,2,4,8,32,32.25,32.5,32.75,33,34,36,40),
                                                      numberOfsamplingsOptimisable = 5 )


arm1 = Arm( name = "BrasTest1",
            size = 100,
            administrations = list( administrationRespPK ),
            samplingTimes   = list( samplingTimesRespPK  ),
            administrationsConstraints = list( administrationConstraintsRespPK ),
            samplingTimesConstraints = list( samplingConstraintsRespPK ) )



design1 = Design( name = "design1", arms = list( arm1 ))


# --------------------------------------
# Optimization

# optimize the Fisher Information Matrix for the PopulationFIM
optimizationPopFIM = Optimization( name = "PK_analytic_populationFIM",
                             modelEquations = modelEquations,
                             modelParameters = modelParameters,
                             modelError = modelError,
                             optimizer = "FedorovWynnAlgorithm",
                             optimizerParameters = list( elementaryProtocols = list(c(0.25,0.5,0.75,1,2,4,8,32,32.25,32.5,32.75,33,34,36,40)),
                             numberOfSubjects = c(100),
                             proportionsOfSubjects = c(1),
                             showProcess = T),
                             designs = list( design1 ),
                             fim = "population",
                             outcomes = list( "RespPK" ) )

optimizationPopFIM = run( optimizationPopFIM )


# plots
plotOptions = list( unitTime=c("hour"), unitResponses= c("mcg/mL"))
outputPath = "D:/MOBILITE/1er papier - Simulation Covariables/"
outputFile = "EvaluationPopFIM.html"
outputFile = "Optimization_PopFIM_100Sub_5Samp.html"
Report( optimizationPopFIM, outputPath, outputFile, plotOptions )







my_theme <- theme_minimal(base_size = 16) +
  theme(
    axis.title = element_text(size = 18, color = "black"),      # Axis labels size and color
    axis.text = element_text(size = 16, color = "black"),       # Axis ticks size and color
    axis.line = element_line(linewidth = 1.5),                  # Axis lines size
    legend.title = element_text(size = 18, color = "black"),    # Legend title size and color
    legend.text = element_text(size = 16, color = "black")      # Legend text size and color
  )






# Define the PK model
code <- '
$PROB
$PARAM @annotated
TVCL : 1  : 1  Clearance (L.h-1)
TVVC : 10  : 2  Central volume (L)
TVKA : 1  : 3  Absorption constant (L)


$PARAM @annotated @covariates

WT : 70 : Weight (kg)
CrCl : 90 : Creatinine clearance (mL/min/1.73m²)
WT_VC : 1 : Weight on Vc ()
CrCl_CL : -0.8 : Creatinine Clearance on CL
PPI : 0 : PPI intake or not
PPI_KA : 0.5 : Effect of Proton Pump Inhibitor on KA
UM : 0 : Ultrarapid metabolizer
UM_CL : 2 : Effect of UM on CL

$CMT @annotated
GUT : Depot compartment [ADM]
CENTRAL : Central compartment (mg/L) [OBS]



$TABLE
double DV  = (CENTRAL / VC);


$MAIN
double CL = TVCL * pow((CrCl/90),CrCl_CL) * pow((UM_CL),UM)    ;
double VC = TVVC * pow((WT/70),WT_VC)  ;
double KA = TVKA * pow((PPI_KA),PPI)   ;
double K10 = CL/VC ;



$ODE

dxdt_GUT        = -KA*GUT ;
dxdt_CENTRAL    = KA*GUT - K10*CENTRAL ;

$CAPTURE DV CL VC KA'

mod <- mrgsolve::mcode("optim", code, atol=1e-8, rtol=1e-8,maxsteps=5000)








ff <- function(model_switch, xt, p, poped.db){
  times_xt <- drop(xt)  
  dose_times <- seq(from=0,to=max(times_xt),by=p[["TAU"]])
  time <- sort(unique(c(times_xt,dose_times)))
  is.dose <- time %in% dose_times
  
  data <- data.frame(
    ID = 1,
    time = time,
    amt = ifelse(is.dose,p[["DOSE"]], 0), 
    cmt = ifelse(is.dose, 1, 0)
  )
  
  data[["evid"]] <- data[["cmt"]]
  
  mod <- param(mod, as.list(p))
  
  out <- mrgsim_q(mod,data)
  
  y <- out$DV[match(times_xt,out$time)]
  
  return(list(y=matrix(y,ncol=1),poped.db=poped.db))
}
  


sfg.mrgsolve.cov <- function(x,a,bpop,b,bocc){
  parameters=c(TVCL=bpop[1]*exp(b[1]),
               TVVC=bpop[2]*exp(b[2]),
               TVKA=bpop[3]*exp(b[3]),
               WT_VC=bpop[4],
               CrCl_CL=bpop[5],
               PPI_KA=bpop[6],
               UM_CL=bpop[7],
               DOSE=a[1],
               TAU =a[2],
               WT  =a[3],
               CrCl =a[4],
               PPI  =a[5],
               UM = a[6])
  return(parameters) 
}


feps <- function(model_switch,xt,parameters,epsi,poped.db){
  returnArgs <- do.call(poped.db$model$ff_pointer,list(model_switch,xt,parameters,poped.db)) 
  y <- returnArgs[[1]]
  poped.db <- returnArgs[[2]]
  
  y = y*(1+epsi[,1])+epsi[,2]
  
  return(list( y= y,poped.db =poped.db )) 
}


poped.cov_mrg_poped <- create.poped.database(
  ff_fun=ff,
  fg_fun=sfg.mrgsolve.cov,
  fError_fun=feps,
  bpop=c(TVCL=1, TVVC=10,TVKA=1, WT_VC=1, CrCl_CL=-0.8, PPI_KA=0.5, UM_CL=2), 
  d=c(TVCL=0.09,TVVC=0.09,TVKA=0.09), 
  notfixed_bpop=c(1,1,1,1,1,1,1),
  sigma=c(prop=0.15^2,add=0.1^2),
  notfixed_sigma=c(1,1),
  m=100,
  groupsize=1,
  xt=c(0.25,0.5,4,36,40),
  discrete_xt=c(0.25,0.5,4,36,40),
  minxt=c(0),
  maxxt=c(40),
  a = cbind(DOSE=rep(100,100), TAU=rep(8,100), WT=sim_data$WT, CrCl=sim_data$CrCl,PPI=sim_data$PPI,UM=sim_data$UM)
)

plot_model_prediction(poped.cov_mrg_poped,model_num_points = 500, PI=FALSE) + theme_bw()
ev<-evaluate_design(fim.calc.type=4,poped.cov_mrg_poped)
ev
shrinkage(poped.cov_mrg_poped)




# Define the PK model
codeb <- '

$PROB

$PARAM @annotated
TVCL : 1  : 1  Clearance (L.h-1)
TVVC : 10  : 2  Central volume (L)
TVKA : 1  : 3  Absorption constant (L)


$PARAM @annotated @covariates

WT : 70 : Weight (kg)
CrCl : 90 : Creatinine clearance (mL/min/1.73m²)
PPI : 0 : PPI intake or not
UM : 0 : Ultrarapid metabolizer
WT_VC : 1 : Weight on Vc ()
CrCl_CL : -0.8 : Creatinine Clearance on CL
PPI_KA : 0.5 : Effect of Proton Pump Inhibitor on KA
UM_CL : 2 : Effect of UM on CL

$OMEGA @block
0.09 // CL
0 0.09 // VC
0 0 0.09 // KA


$SIGMA 
0.0225 // err prop
0.01 //  err additive


$CMT @annotated
GUT : Depot compartment [ADM]
CENTRAL : Central compartment (mg/L) [OBS]



$TABLE
double DV  = (CENTRAL / VC)+ (EPS(2)+(CENTRAL / VC)*EPS(1)) ;


$MAIN
double CL = TVCL * pow((CrCl/90),CrCl_CL) * pow((UM_CL),UM) *exp(ETA(1))    ;
double VC = TVVC * pow((WT/70),WT_VC)*exp(ETA(2))  ;
double KA = TVKA * pow((PPI_KA),PPI)*exp(ETA(3))   ;
double K10 = CL/VC ;


$ODE

dxdt_GUT        = -KA*GUT ;
dxdt_CENTRAL    = KA*GUT - K10*CENTRAL ;

$CAPTURE DV CL VC KA'

modbsv <- mrgsolve::mcode("optim", codeb, atol=1e-8, rtol=1e-8,maxsteps=5000)



ev1 <- ev(amt = 100, cmt = 1,ii=8,addl=4)
out <- 
  modbsv %>% 
  ev(ev1) %>%
  idata_set(sim_data) %>%
  mrgsim(delta=0.05, end=40, obsonly=TRUE)

out

png("mrgsolve output.png", width = 24, height = 9, units = 'in', res = 900)

plot(out)

# Close the graphics device
dev.off()




# Define the time sequence (from 0 to 40 by 0.05)
time_grid <- seq(0, 40, by = 0.05)

# Expand sim_data to create a new dataframe with time grid
expanded_sim_data <- sim_data[rep(1:nrow(sim_data), each = length(time_grid)), ]
expanded_sim_data$time <- rep(time_grid, nrow(sim_data))

# Initialize amt, evid, and cmt columns
expanded_sim_data$amt <- 0
expanded_sim_data$evid <- 0
expanded_sim_data$cmt <- 0

# Define dosing regimen: 100 mg every 8 hours until 40 hours
dosing_times <- seq(0, 40, by = 8)

# Apply the dosing regimen for each subject
for (i in 1:nrow(sim_data)) {
  # Get rows corresponding to the current subject
  subject_rows <- expanded_sim_data$ID == sim_data$ID[i]
  
  # Assign amt, evid, and cmt for the dosing times
  expanded_sim_data$amt[subject_rows & expanded_sim_data$time %in% dosing_times] <- 100
  expanded_sim_data$evid[subject_rows & expanded_sim_data$time %in% dosing_times] <- 1
  expanded_sim_data$cmt[subject_rows & expanded_sim_data$time %in% dosing_times] <- 1
}

# Check the expanded dataframe
head(expanded_sim_data)

# Merge expanded_sim_data and out based on 'ID' and 'time'
merged_data <- merge(expanded_sim_data, as.data.frame(out), by = c("ID", "time"))

# Check the merged data
head(merged_data)


# Filter merged_data to keep only rows where 'time' is (0, 0.25,0.5, 4, 8,16,24,32,36 or 40
filtered_data <- merged_data[merged_data$time %in% c(0, 0.25,0.5, 4, 8,16,24,32,36, 40), ]

# Check the filtered data
head(filtered_data)

# Step 1: Identify rows to duplicate (time is  40, amt == 100, evid == 1)
rows_to_duplicate <- filtered_data[filtered_data$time %in% c( 40) & filtered_data$amt == 100 & filtered_data$evid == 1, ]


# Step 2: Duplicate the rows
duplicated_rows <- rows_to_duplicate

# Step 3: Set amt and evid to 0 in the duplicated rows
duplicated_rows$amt <- 0
duplicated_rows$evid <- 0

# Step 4: Append the duplicated rows back to the original dataframe
final_data <- rbind(filtered_data, duplicated_rows)

# Step 5: Sort the final_data by ID and time to maintain order (optional)
final_data <- final_data[order(final_data$ID, final_data$time), ]

final_data_mice <-final_data
final_data_mice$PPI<-as.factor(as.character(final_data_mice$PPI))
final_data_mice$UM<-as.factor(as.character(final_data_mice$UM))




# Define the columns you want to use for imputation
selected_columns <- c(1:9, 15)  # Columns: ID, time, 3:9, and DV (column 14)




# Create a list to store dataframes with different percentages of missing data
missing_data_list <- list()

# Define the percentages of missing data to generate (5%, 20%, 50% and 75%)
percentages <- c(5, 20, 50,75)

# Get the unique IDs and the number of individuals
unique_ids <- unique(final_data$ID)
n_individuals <- length(unique_ids)

# Step 1: Create missing data for WT, CrCl, and PPI
introduce_missing_data <- function(df, variable, percentage) {
  n_missing <- round(n_individuals * percentage / 100)
  individuals_to_missing <- sample(unique_ids, n_missing, replace = FALSE)
  df[df$ID %in% individuals_to_missing, variable] <- NA
  return(df)
}

# Loop over each percentage and generate the corresponding dataframe with missing data
for (perc in percentages) {
  data_with_missing <- final_data_mice
  data_with_missing <- introduce_missing_data(data_with_missing, "CrCl", perc)
  data_with_missing <- introduce_missing_data(data_with_missing, "WT", perc)
  data_with_missing <- introduce_missing_data(data_with_missing, "PPI", perc)
  data_with_missing <- introduce_missing_data(data_with_missing, "UM", perc)
  missing_data_list[[paste0("missing_", perc, "perc")]] <- data_with_missing
}



# Define your list of missing data percentages
missing_percentages <- c("5%", "20%", "50%", "75%")

# Loop over each dataframe in missing_data_list and save each plot as PNG
for (i in 1:length(missing_data_list)) {
  # Subset the data to keep only the variables of interest
  selected_data <- missing_data_list[[i]][, c("UM", "CrCl", "PPI", "WT")]
  
  # Create the PNG filename, removing special characters like %
  png_filename <- paste0("Missing_Data_", gsub("%", "", missing_percentages[i]), ".png")
  
  # Save the plot as PNG with a larger width to provide more space
  png(png_filename, width = 1600, height = 700)
  
  # Increase the top margin to make space for the title
  par(mar = c(5, 5, 10, 5))  # Increase top margin to 10 for title space
  
  # Generate the aggr plot
  aggr(selected_data, numbers = TRUE, sortVars = TRUE, 
       prop = TRUE, cex.axis = 2, cex.lab = 2, cex.numbers = 2, gap = 5, 
       labels = c("UM", "CrCl", "PPI", "WT"), combined = FALSE, 
       col = c("skyblue", "red"), border = "black")
  
  # Add the title using mtext(), positioning it above the plot
  mtext(paste("Missing Data -", missing_percentages[i]), side = 3, line = 8.5, cex = 2, font = 2)
  
  # Close the PNG device
  dev.off()
}


# Load the 4 PNG images in the correct order
img1 <- image_read("Missing_Data_5.png")
img2 <- image_read("Missing_Data_20.png")
img3 <- image_read("Missing_Data_50.png")
img4 <- image_read("Missing_Data_75.png")

# Combine the 4 images into a 2x2 layout in the correct order
combined_img <- image_append(c(
  image_append(c(img1, img2), stack = TRUE), 
  image_append(c(img3, img4), stack = TRUE)
))

# Display the combined image
print(combined_img)

# Optionally, save the combined image as a PNG
image_write(combined_img, path = "Combined_Missing_Data_Final.png")



# Read the files for each method into separate lists
Amelia_imputed_data_list <- read_imputed_data("amelia")
MF_imputed_data_list     <- read_imputed_data("MF")
mice_imputed_data_list   <- read_imputed_data("mice")
MIDAS_imputed_data_list  <- read_imputed_data("MIDAS")
XGB_imputed_data_list    <- read_imputed_data("XGB")





# Initialize the list for storing imputed data
mice_imputed_data_list <- list()

# Define the imputation methods:
# pmm for WT and CrCl (continuous), logreg for PPI and UM (binomial),
mice_imputation_methods <- c("", "","","pmm","", "pmm","", "logreg","logreg","")

# Loop through each dataframe with missing data for imputation
for (i in 1:length(missing_data_list)) {
  
  # Subset the data to the relevant columns (including all potential predictors)
  mice_df_subset <- missing_data_list[[i]][, selected_columns]

  # Create a predictorMatrix to handle clustering by ID
  mice_predictorMatrix <- make.predictorMatrix(mice_df_subset)
  
  # Specify that ID should be used for clustering but not as a predictor
  mice_predictorMatrix[, "ID"] <- -2  # Cluster by ID
  mice_predictorMatrix["ID", ] <- 0   # ID shouldn't be a predictor
  
  # Perform the imputation using all variables as predictors
  # Use DV and time as predictors along with WT, CrCl, PPI, etc.
  mice_imputed_data <- mice(
    mice_df_subset, 
    m = 5,  # Perform multiple imputations
    method = mice_imputation_methods,  # Imputation methods for each variable
    predictorMatrix = mice_predictorMatrix, 
    seed = 500
  )
  
  # Store the completed (imputed) dataset in the list
  mice_imputed_data_list[[paste0("mice_imputed_data_", i)]] <- mice::complete(mice_imputed_data)
  
  # Adjust imputed data to enforce consistency within each ID
  for (id in unique(mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$ID)) {
    idx <- mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$ID == id  # Get all rows for the specific ID
    
    # Calculate the mean for WT and CrCl, and the most frequent value for PPI
    mean_WT <- mean(mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$WT[idx], na.rm = TRUE)
    mean_CrCl <- mean(mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$CrCl[idx], na.rm = TRUE)
    most_frequent_PPI <- as.numeric(names(sort(table(mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$PPI[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1
    most_frequent_UM <- as.numeric(names(sort(table(mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$UM[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1
    # Replace all values within that ID with the calculated values
    mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$WT[idx] <- mean_WT
    mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$CrCl[idx] <- mean_CrCl
    mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$PPI[idx] <- most_frequent_PPI
    mice_imputed_data_list[[paste0("mice_imputed_data_", i)]]$UM[idx] <- most_frequent_UM
  }
}

# Set your working directory to the folder containing the files
setwd("D:/MOBILITE/1er papier - Simulation Covariables/imputed_datasets_merged")

# Read the 'final_data' file into a dataframe
final_data <- read.csv("final_data.csv")  # Adjust the file extension if necessary

# Function to read files with a specific prefix into a list
read_imputed_data <- function(prefix) {
  # List all files that match the prefix and have an underscore followed by a number
  files <- list.files(pattern = paste0("^", prefix, "_\\d+\\.csv$"))
  
  # Sort the files to ensure they are in the correct order
  files <- sort(files)
  
  # Read each file and store it in a list
  data_list <- lapply(files, read.csv)
  
  # Optionally, name the list elements for easier reference
  names(data_list) <- paste0(prefix, "_", 1:length(data_list))
  
  return(data_list)
}





# Create a vector for the missing data percentages
missing_percentages <- c("5%", "20%", "50%", "75%")

# Initialize lists to store results
WT_errors <- list()
CrCl_errors <- list()
PPI_concordance <- list()
UM_concordance <- list()

# Loop through the missing and imputed datasets
for (i in 1:length(missing_data_list)) {
  
  # Extract missing rows from the current missing_data_list
  missing_WT <- missing_data_list[[i]] %>% filter(is.na(WT))
  missing_CrCl <- missing_data_list[[i]] %>% filter(is.na(CrCl))
  missing_PPI <- missing_data_list[[i]] %>% filter(is.na(PPI))
  missing_UM <- missing_data_list[[i]] %>% filter(is.na(UM))
 
  # Extract corresponding imputed data
  imputed_WT <- mice_imputed_data_list[[i]] %>% filter(ID %in% missing_WT$ID)
  imputed_CrCl <- mice_imputed_data_list[[i]] %>% filter(ID %in% missing_CrCl$ID)
  imputed_PPI <- mice_imputed_data_list[[i]] %>% filter(ID %in% missing_PPI$ID)
  imputed_UM <- mice_imputed_data_list[[i]] %>% filter(ID %in% missing_PPI$UM)  
  # Match imputed with original data and compute errors for WT
  WT_error <- imputed_WT %>%
    inner_join(final_data %>% select(ID, WT), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%  # Keep only first row of each ID
    mutate(WT_imputed = WT.x, WT_original = WT.y,
           AE_WT = abs(WT_imputed - WT_original),
           PE_WT = (WT_imputed - WT_original) / WT_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "Mice")
  
  # Match imputed with original data and compute errors for CrCl
  CrCl_error <- imputed_CrCl %>%
    inner_join(final_data %>% select(ID, CrCl), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(CrCl_imputed = CrCl.x, CrCl_original = CrCl.y,
           AE_CrCl = abs(CrCl_imputed - CrCl_original),
           PE_CrCl = (CrCl_imputed - CrCl_original) / CrCl_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "Mice")
  
  # Match imputed with original data and compute concordance for PPI
  PPI_conc <- imputed_PPI %>%
    inner_join(final_data %>% select(ID, PPI), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(PPI_imputed = PPI.x, PPI_original = PPI.y,
           ConcordancePPI = PPI_imputed == PPI_original,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "Mice")
  UM_conc <- imputed_PPI %>%
    inner_join(final_data %>% select(ID, UM), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(UM_imputed = UM.x, UM_original = UM.y,
           ConcordanceUM = UM_imputed == UM_original,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "Mice")  


  # Store the results in the lists
  WT_errors[[i]] <- WT_error
  CrCl_errors[[i]] <- CrCl_error
  PPI_concordance[[i]] <- PPI_conc
  UM_concordance[[i]] <- UM_conc
}

# Combine the results into single dataframes for WT, CrCl, and PPI
WT_errors_combined <- bind_rows(WT_errors)
CrCl_errors_combined <- bind_rows(CrCl_errors)
PPI_concordance_combined <- bind_rows(PPI_concordance)
UM_concordance_combined <- bind_rows(UM_concordance)

# Combine the dataframes using cbind without joining by ID
mice_combined_data <- cbind(
  WT_errors_combined %>% select(ID, AE_WT, PE_WT, Missing_Values, Imputation_Method),
  CrCl_errors_combined %>% select(AE_CrCl, PE_CrCl),
  PPI_concordance_combined %>% select(ConcordancePPI),
  UM_concordance_combined %>% select(ConcordanceUM)
)

# View the combined dataframe
mice_combined_data






MF_imputed_data_list <- list()  # Initialize an empty list to store the results

for (i in seq_along(missing_data_list)) {
  # Perform the imputation with missForest
  MF_imputed_result <- missForest(missing_data_list[[i]][, selected_columns], 
                                  maxiter = 50, 
                                  ntree = 300, 
                                  replace = TRUE, 
                                  variablewise = TRUE)
  
  # Extract the imputed data (ximp contains the completed dataset)
  MF_imputed_data <- MF_imputed_result[[1]]
  
  # Store the imputed data in the list
  MF_imputed_data_list[[paste0("MF_imputed_data_", i)]] <- MF_imputed_data
  
  # Adjust imputed data to enforce consistency within each ID
  for (id in unique(MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$ID)) {
    idx <- MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$ID == id  # Get all rows for the specific ID
    
    # Calculate the mean for WT and CrCl, and the most frequent value for PPI
    mean_WT <- mean(MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$WT[idx], na.rm = TRUE)
    mean_CrCl <- mean(MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$CrCl[idx], na.rm = TRUE)
    most_frequent_PPI <- as.numeric(names(sort(table(MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$PPI[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1
    most_frequent_UM <- as.numeric(names(sort(table(MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$UM[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1

    # Replace all values within that ID with the calculated values
    MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$WT[idx] <- mean_WT
    MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$CrCl[idx] <- mean_CrCl
    MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$PPI[idx] <- most_frequent_PPI
    MF_imputed_data_list[[paste0("MF_imputed_data_", i)]]$UM[idx] <- most_frequent_UM
  }
}




# Create a vector for the missing data percentages
missing_percentages <- c("5%", "20%", "50%", "75%")

# Initialize lists to store results
WT_errors <- list()
CrCl_errors <- list()
PPI_concordance <- list()
UM_concordance <- list()

# Loop through the missing and imputed datasets
for (i in 1:length(missing_data_list)) {
  
  # Extract missing rows from the current missing_data_list
  missing_WT <- missing_data_list[[i]] %>% filter(is.na(WT))
  missing_CrCl <- missing_data_list[[i]] %>% filter(is.na(CrCl))
  missing_PPI <- missing_data_list[[i]] %>% filter(is.na(PPI))
  missing_UM <- missing_data_list[[i]] %>% filter(is.na(UM))
 
  # Extract corresponding imputed data
  imputed_WT <- MF_imputed_data_list[[i]] %>% filter(ID %in% missing_WT$ID)
  imputed_CrCl <- MF_imputed_data_list[[i]] %>% filter(ID %in% missing_CrCl$ID)
  imputed_PPI <- MF_imputed_data_list[[i]] %>% filter(ID %in% missing_PPI$ID)
  imputed_UM <- MF_imputed_data_list[[i]] %>% filter(ID %in% missing_PPI$UM)  

  
  # Match imputed with original data and compute errors for WT
  WT_error <- imputed_WT %>%
    inner_join(final_data %>% select(ID, WT), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%  # Keep only first row of each ID
    mutate(WT_imputed = WT.x, WT_original = WT.y,
           AE_WT = abs(WT_imputed - WT_original),
           PE_WT = (WT_imputed - WT_original) / WT_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MF")
  
  # Match imputed with original data and compute errors for CrCl
  CrCl_error <- imputed_CrCl %>%
    inner_join(final_data %>% select(ID, CrCl), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(CrCl_imputed = CrCl.x, CrCl_original = CrCl.y,
           AE_CrCl = abs(CrCl_imputed - CrCl_original),
           PE_CrCl = (CrCl_imputed - CrCl_original) / CrCl_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MF")
  
  # Match imputed with original data and compute concordance for PPI
  PPI_conc <- imputed_PPI %>%
    inner_join(final_data %>% select(ID, PPI), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(PPI_imputed = PPI.x, PPI_original = PPI.y,
           ConcordancePPI = PPI_imputed == PPI_original,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MF")
  UM_conc <- imputed_PPI %>%
    inner_join(final_data %>% select(ID, UM), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(UM_imputed = UM.x, UM_original = UM.y,
           ConcordanceUM = UM_imputed == UM_original,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MF")
  
  # Store the results in the lists
  WT_errors[[i]] <- WT_error
  CrCl_errors[[i]] <- CrCl_error
  PPI_concordance[[i]] <- PPI_conc
  UM_concordance[[i]] <- UM_conc
}

# Combine the results into single dataframes for WT, CrCl, and PPI
WT_errors_combined <- bind_rows(WT_errors)
CrCl_errors_combined <- bind_rows(CrCl_errors)
PPI_concordance_combined <- bind_rows(PPI_concordance)
UM_concordance_combined <- bind_rows(UM_concordance)

# Combine the dataframes using cbind without joining by ID
MF_combined_data <- cbind(
  WT_errors_combined %>% select(ID, AE_WT, PE_WT, Missing_Values, Imputation_Method),
  CrCl_errors_combined %>% select(AE_CrCl, PE_CrCl),
  PPI_concordance_combined %>% select(ConcordancePPI),
  UM_concordance_combined %>% select(ConcordanceUM)
)

# View the combined dataframe
MF_combined_data




# Initialize the list to store imputed datasets
MIDAS_imputed_data_list <- list()

# Define binary column
MIDAS_bin <- c('PPI','UM')  # PPI and UM are binary

# Import numpy from Python (via reticulate package if necessary)
np <- import("numpy")

# Loop over each dataset with missing data
for(i in seq_along(missing_data_list)) {
  
  # Replace NAs with NA for the current data frame (rMIDAS handles NA internally)
  missing_data_list[[i]] <- as.data.frame(lapply(missing_data_list[[i]], function(x) {
    ifelse(is.na(x), NA, x)  # Convert NAs for rMIDAS
  }))
  
  # Apply rMIDAS preprocessing steps
  # Ensure DV, time, Height, Alb, etc., are included as predictors
  MIDAS_conv <- rMIDAS::convert(missing_data_list[[i]][, selected_columns], 
                        bin_cols = MIDAS_bin, 
                        minmax_scale = TRUE)  # Normalize for neural network
  
  # Train the rMIDAS model
  MIDAS_train <- rMIDAS::train(MIDAS_conv,
                       training_epochs = 50,           # Number of training epochs
                       layer_structure = c(128,64,32),  # Hidden layers
                       input_drop = 0.5,              # Dropout to prevent overfitting
                       seed = 89)                      # Set a seed for reproducibility
  
  # Perform imputation with the trained model (m=1 for single imputation)
  MIDAS_complete <- rMIDAS::complete(MIDAS_train, m = 1)
  
  # Extract the imputed data
  MIDAS_complete_df <- MIDAS_complete[[1]]

  # Adjust imputed data for consistency within each ID
  for (id in unique(MIDAS_complete_df$ID)) {
    idx <- MIDAS_complete_df$ID == id  # Get all rows for the specific ID
    
    # Calculate mean for WT and CrCl, and the most frequent value for PPI
    mean_WT <- mean(MIDAS_complete_df$WT[idx], na.rm = TRUE)
    mean_CrCl <- mean(MIDAS_complete_df$CrCl[idx], na.rm = TRUE)
    most_frequent_PPI <- as.numeric(names(sort(table(MIDAS_complete_df$PPI[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1
    most_frequent_UM <- as.numeric(names(sort(table(MIDAS_complete_df$UM[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1

    # Replace all values within that ID with the calculated values
    MIDAS_complete_df$WT[idx] <- mean_WT
    MIDAS_complete_df$CrCl[idx] <- mean_CrCl
    MIDAS_complete_df$PPI[idx] <- most_frequent_PPI
    MIDAS_complete_df$UM[idx] <- most_frequent_UM
  }
  
  # Store the adjusted completed data in the list
  MIDAS_imputed_data_list[[i]] <- MIDAS_complete_df
}


# Loop through each data frame in the list
for (i in seq_along(MIDAS_imputed_data_list)) {
  #MIDAS_imputed_data_list[[i]]$ID <- as.integer(MIDAS_imputed_data_list[[i]]$ID)   # Convert ID to integer
  MIDAS_imputed_data_list[[i]]$PPI <- as.numeric(as.character(MIDAS_imputed_data_list[[i]]$PPI)) - 1  # Convert PPI to numeric and reduce by 1
  MIDAS_imputed_data_list[[i]]$UM <- as.numeric(as.character(MIDAS_imputed_data_list[[i]]$UM)) - 1  # Convert PPI to numeric and reduce by 1
}



for (i in seq_along(missing_data_list)) {
  missing_data_list[[i]]$PPI <- as.numeric(as.character(missing_data_list[[i]]$PPI)) - 1  # Convert PPI to numeric and reduce by 1
  missing_data_list[[i]]$UM <- as.numeric(as.character(missing_data_list[[i]]$UM)) - 1  # Convert PPI to numeric and reduce by 1
}




# Create a vector for the missing data percentages
missing_percentages <- c("5%", "20%", "50%", "75%")

# Initialize lists to store results
WT_errors <- list()
CrCl_errors <- list()
PPI_concordance <- list()
UM_concordance <- list()

# Loop through the missing and imputed datasets
for (i in 1:length(missing_data_list)) {
  
  # Extract missing rows from the current missing_data_list
  missing_data <- missing_data_list[[i]]
  
  # Get indices of rows with NA values for WT, CrCl, and PPI
  na_indices_WT <- which(is.na(missing_data$WT))
  na_indices_CrCl <- which(is.na(missing_data$CrCl))
  na_indices_PPI <- which(is.na(missing_data$PPI))
  na_indices_UM <- which(is.na(missing_data$UM))
  
  # Retrieve the corresponding imputed values using the indices
  imputed_WT <- MIDAS_imputed_data_list[[i]][na_indices_WT, ]
  imputed_CrCl <- MIDAS_imputed_data_list[[i]][na_indices_CrCl, ]
  imputed_PPI <- MIDAS_imputed_data_list[[i]][na_indices_PPI, ]
  imputed_UM <- MIDAS_imputed_data_list[[i]][na_indices_UM, ]
  
  # Ensure IDs are unique in the imputed data
  imputed_WT <- imputed_WT %>% distinct(ID, .keep_all = TRUE)
  imputed_CrCl <- imputed_CrCl %>% distinct(ID, .keep_all = TRUE)
  imputed_PPI <- imputed_PPI %>% distinct(ID, .keep_all = TRUE)
  imputed_UM <- imputed_UM %>% distinct(ID, .keep_all = TRUE)

  # Match imputed with original data and compute errors for WT
  WT_error <- imputed_WT %>%
    left_join(final_data %>% select(ID, WT), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(WT_imputed = WT.x, WT_original = WT,
           AE_WT = abs(WT_imputed - WT_original),
           PE_WT = (WT_imputed - WT_original) / WT_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MIDAS")
  
  # Match imputed with original data and compute errors for CrCl
  CrCl_error <- imputed_CrCl %>%
    left_join(final_data %>% select(ID, CrCl), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(CrCl_imputed = CrCl.x, CrCl_original = CrCl,
           AE_CrCl = abs(CrCl_imputed - CrCl_original),
           PE_CrCl = (CrCl_imputed - CrCl_original) / CrCl_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MIDAS");

  # Match imputed with original data and compute concordance for PPI
PPI_conc <- imputed_PPI %>%
  left_join(final_data %>% select(ID, PPI), by = "ID") %>%
  distinct(ID, .keep_all = TRUE) %>%
  mutate(PPI_imputed = PPI.x, PPI_original = PPI.y,  # Adjust this line based on the column names
         ConcordancePPI = PPI_imputed == PPI_original,
         Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
         Imputation_Method = "MIDAS")
UM_conc <- imputed_UM %>%
    left_join(final_data %>% select(ID, UM), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(UM_imputed = UM.x, UM_original = UM.y,
           ConcordanceUM = UM_imputed == UM_original,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "MIDAS")

  # Store the results in the lists
  WT_errors[[i]] <- WT_error
  CrCl_errors[[i]] <- CrCl_error
  PPI_concordance[[i]] <- PPI_conc
  UM_concordance[[i]] <- UM_conc
}

# Combine the results into single dataframes for WT, CrCl, and PPI
WT_errors_combined <- bind_rows(WT_errors)
CrCl_errors_combined <- bind_rows(CrCl_errors)
PPI_concordance_combined <- bind_rows(PPI_concordance)
UM_concordance_combined <- bind_rows(UM_concordance)

# Combine the dataframes using cbind without joining by ID
MIDAS_combined_data <- cbind(
  WT_errors_combined %>% select(ID, AE_WT, PE_WT, Missing_Values, Imputation_Method),
  CrCl_errors_combined %>% select(AE_CrCl, PE_CrCl),
  PPI_concordance_combined %>% select(ConcordancePPI),
  UM_concordance_combined %>% select(ConcordanceUM)
)

# View the combined dataframe
MIDAS_combined_data









# Initialize an empty list to store the imputed datasets
amelia_imputed_data_list <- list()

for (i in seq_along(missing_data_list)) {
    # Perform the imputation with Amelia
    amelia_result <- amelia(missing_data_list[[i]][, selected_columns], 
                            m = 1,     # Generate 1 imputed datasets for more robust results
                            idvars = "ID",  # Treat 'ID' as an identifier
                            noms = c("PPI","UM"),   # Treat 'PPI' as a categorical variable
                            ts = "time",    # Specify time-series structure
                            tolerance = 1e-05,    # Stricter convergence criteria
                            empri = 0.01,    # Stricter convergence criteria
                            polytime = 3)   # Capture linear time effects
    
    # Extract the imputed data (taking the first imputed dataset here)
    amelia_imputed_data <- amelia_result$imputations[[1]]  # Extract one imputed dataset
    
    # Store the imputed data in the list
    amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]] <- amelia_imputed_data
    
    # (Optional: Post-processing to ensure consistency within each ID)
    for (id in unique(amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$ID)) {
        idx <- amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$ID == id
        
        # Calculate the mean for WT and CrCl, and the most frequent value for PPI
        mean_WT <- mean(amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$WT[idx], na.rm = TRUE)
        mean_CrCl <- mean(amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$CrCl[idx], na.rm = TRUE)
        most_frequent_PPI <- as.numeric(names(sort(table(amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$PPI[idx]), decreasing = TRUE)[1]))
        most_frequent_UM <- as.numeric(names(sort(table(amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$UM[idx]), decreasing = TRUE)[1]))
        
        # Replace all values within that ID with the calculated values
        amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$WT[idx] <- mean_WT
        amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$CrCl[idx] <- mean_CrCl
        amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$PPI[idx] <- most_frequent_PPI
        amelia_imputed_data_list[[paste0("amelia_imputed_data_", i)]]$UM[idx] <- most_frequent_UM
    }
}



# Loop through each data frame in the list
for (i in seq_along(amelia_imputed_data_list)) {
  #amelia_imputed_data_list[[i]]$ID <- as.integer(amelia_imputed_data_list[[i]]$ID)   # Convert ID to integer
  amelia_imputed_data_list[[i]]$PPI <- as.numeric(as.character(amelia_imputed_data_list[[i]]$PPI))   # Convert PPI to numeric and reduce by 1
  amelia_imputed_data_list[[i]]$UM <- as.numeric(as.character(amelia_imputed_data_list[[i]]$UM))   # Convert PPI to numeric and reduce by 1
}


# Create a vector for the missing data percentages
missing_percentages <- c("5%", "20%", "50%", "75%")

# Initialize lists to store results
WT_errors <- list()
CrCl_errors <- list()
PPI_concordance <- list()
UM_concordance <- list()

# Loop through the missing and imputed datasets
for (i in 1:length(missing_data_list)) {
  
  # Extract missing rows from the current missing_data_list
  missing_data <- missing_data_list[[i]]
  
  # Get indices of rows with NA values for WT, CrCl, and PPI
  na_indices_WT <- which(is.na(missing_data$WT))
  na_indices_CrCl <- which(is.na(missing_data$CrCl))
  na_indices_PPI <- which(is.na(missing_data$PPI))
  na_indices_UM <- which(is.na(missing_data$UM))  

  # Retrieve the corresponding imputed values using the indices
  imputed_WT <- amelia_imputed_data_list[[i]][na_indices_WT, ]
  imputed_CrCl <- amelia_imputed_data_list[[i]][na_indices_CrCl, ]
  imputed_PPI <- amelia_imputed_data_list[[i]][na_indices_PPI, ]
  imputed_UM <- amelia_imputed_data_list[[i]][na_indices_UM, ]
  
  # Ensure IDs are unique in the imputed data
  imputed_WT <- imputed_WT %>% distinct(ID, .keep_all = TRUE)
  imputed_CrCl <- imputed_CrCl %>% distinct(ID, .keep_all = TRUE)
  imputed_PPI <- imputed_PPI %>% distinct(ID, .keep_all = TRUE)
  imputed_UM <- imputed_UM %>% distinct(ID, .keep_all = TRUE)

  # Match imputed with original data and compute errors for WT
  WT_error <- imputed_WT %>%
    left_join(final_data %>% select(ID, WT), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(WT_imputed = WT.x, WT_original = WT,
           AE_WT = abs(WT_imputed - WT_original),
           PE_WT = (WT_imputed - WT_original) / WT_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "amelia")
  
  # Match imputed with original data and compute errors for CrCl
  CrCl_error <- imputed_CrCl %>%
    left_join(final_data %>% select(ID, CrCl), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(CrCl_imputed = CrCl.x, CrCl_original = CrCl,
           AE_CrCl = abs(CrCl_imputed - CrCl_original),
           PE_CrCl = (CrCl_imputed - CrCl_original) / CrCl_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "amelia");

  # Match imputed with original data and compute concordance for PPI
PPI_conc <- imputed_PPI %>%
  left_join(final_data %>% select(ID, PPI), by = "ID") %>%
  distinct(ID, .keep_all = TRUE) %>%
  mutate(PPI_imputed = PPI.x, PPI_original = PPI.y,  # Adjust this line based on the column names
         ConcordancePPI = PPI_imputed == PPI_original,
         Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
         Imputation_Method = "amelia")
UM_conc <- imputed_UM %>%
  left_join(final_data %>% select(ID, UM), by = "ID") %>%
  distinct(ID, .keep_all = TRUE) %>%
  mutate(UM_imputed = UM.x, UM_original = UM.y,  # Adjust this line based on the column names
         ConcordanceUM = UM_imputed == UM_original,
         Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
         Imputation_Method = "amelia")

  # Store the results in the lists
  WT_errors[[i]] <- WT_error
  CrCl_errors[[i]] <- CrCl_error
  PPI_concordance[[i]] <- PPI_conc
  UM_concordance[[i]] <- UM_conc
}

# Combine the results into single dataframes for WT, CrCl, and PPI
WT_errors_combined <- bind_rows(WT_errors)
CrCl_errors_combined <- bind_rows(CrCl_errors)
PPI_concordance_combined <- bind_rows(PPI_concordance)
UM_concordance_combined <- bind_rows(UM_concordance)

# Combine the dataframes using cbind without joining by ID
amelia_combined_data <- cbind(
  WT_errors_combined %>% select(ID, AE_WT, PE_WT, Missing_Values, Imputation_Method),
  CrCl_errors_combined %>% select(AE_CrCl, PE_CrCl),
  PPI_concordance_combined %>% select(ConcordancePPI),
  UM_concordance_combined %>% select(ConcordanceUM)
)

# View the combined dataframe
amelia_combined_data








for (i in seq_along(missing_data_list)) {
  missing_data_list[[i]]$PPI <- as.numeric(as.character(missing_data_list[[i]]$PPI))   # Convert PPI to numeric 
  missing_data_list[[i]]$UM <- as.numeric(as.character(missing_data_list[[i]]$UM))  # Convert PPI to numeric 
}


# Initialize the list to store imputed datasets
XGB_imputed_data_list <- list()

# Set parameters for XGBoost
params_continuous <- list(
  booster = "gbtree",
  objective = "reg:squarederror",  # For continuous variables (WT, CrCl)
  eval_metric = "rmse",
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

params_binary <- list(
  booster = "gbtree",
  objective = "binary:logistic",  # For binary variable (PPI)
  eval_metric = "logloss",
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Helper function to scale continuous variables between 0 and 1
scale_data <- function(data) {
  scaled_data <- as.data.frame(scale(data))
  attr(scaled_data, "scaled:center") <- attr(scale(data), "scaled:center")
  attr(scaled_data, "scaled:scale") <- attr(scale(data), "scaled:scale")
  return(scaled_data)
}

# Helper function to reverse scaling
reverse_scale <- function(scaled_data, original_data) {
  scaled_center <- attr(scaled_data, "scaled:center")
  scaled_scale <- attr(scaled_data, "scaled:scale")
  original_data <- (scaled_data * scaled_scale) + scaled_center
  return(original_data)
}

# Function to impute continuous variables using XGBoost
impute_xgboost_continuous <- function(data, target_column, params) {
  complete_cases <- !is.na(data[[target_column]])
  incomplete_cases <- is.na(data[[target_column]])
  
  # Split data into predictors (features) and target (response)
  x_train <- data[complete_cases, !(names(data) %in% c(target_column))]
  y_train <- data[complete_cases, target_column]
  
  x_test <- data[incomplete_cases, !(names(data) %in% c(target_column))]

  # Scale continuous data for training and testing
  x_train_scaled <- scale_data(x_train)
  x_test_scaled <- scale_data(x_test)
  
  # Convert to XGBoost matrix format
  dtrain <- xgb.DMatrix(data = as.matrix(x_train_scaled), label = y_train)
  dtest <- xgb.DMatrix(data = as.matrix(x_test_scaled))
  
  # Cross-validation to determine the best number of rounds
  cv <- xgb.cv(
    params = params, 
    data = dtrain, 
    nrounds = 1000, 
    nfold = 5, 
    early_stopping_rounds = 10, 
    print_every_n = 10
  )
  best_nrounds <- cv$best_iteration
  
  # Train the final model using best number of rounds
  model <- xgb.train(params = params, data = dtrain, nrounds = best_nrounds)
  
  # Predict missing values
  predictions <- predict(model, dtest)
  
  # Fill in the missing values with predictions
  data[incomplete_cases, target_column] <- predictions
  
  return(data)
}

# Function to impute binary variables using XGBoost
impute_xgboost_binary <- function(data, target_column, params) {
  complete_cases <- !is.na(data[[target_column]])
  incomplete_cases <- is.na(data[[target_column]])
  
  # Split data into predictors (features) and target (response)
  x_train <- data[complete_cases, !(names(data) %in% c(target_column))]
  y_train <- data[complete_cases, target_column]
  
  x_test <- data[incomplete_cases, !(names(data) %in% c(target_column))]
  
  # Convert to XGBoost matrix format
  dtrain <- xgb.DMatrix(data = as.matrix(x_train), label = y_train)
  dtest <- xgb.DMatrix(data = as.matrix(x_test))
  
  # Cross-validation to determine the best number of rounds
  cv <- xgb.cv(
    params = params, 
    data = dtrain, 
    nrounds = 1000, 
    nfold = 5, 
    early_stopping_rounds = 10, 
    print_every_n = 10
  )
  best_nrounds <- cv$best_iteration
  
  # Train the final model using best number of rounds
  model <- xgb.train(params = params, data = dtrain, nrounds = best_nrounds)
  
  # Predict missing values
  predictions <- predict(model, dtest)
  
  # Threshold predictions at 0.5 to determine class labels (binary classification)
  data[incomplete_cases, target_column] <- ifelse(predictions > 0.5, 1, 0)
  
  return(data)
}

# Loop through each dataset with missing data
for (i in seq_along(missing_data_list)) {
  
  # Make a copy of the dataset
  data <- missing_data_list[[i]][, selected_columns]
  
  # Impute continuous variables (WT and CrCl)
  data <- impute_xgboost_continuous(data, "WT", params_continuous)
  data <- impute_xgboost_continuous(data, "CrCl", params_continuous)
  
  # Impute binary variable (PPI)
  data <- impute_xgboost_binary(data, "PPI", params_binary)
  data <- impute_xgboost_binary(data, "UM", params_binary)  

  # Store the imputed dataset
  XGB_imputed_data_list[[paste0("XGB_imputed_data_", i)]] <- data
  
  # Optional: Post-process to ensure consistency within each ID (like in your original script)
  for (id in unique(data$ID)) {
    idx <- data$ID == id  # Get all rows for the specific ID
    
    # Calculate mean for WT and CrCl, and the most frequent value for PPI
    mean_WT <- mean(data$WT[idx], na.rm = TRUE)
    mean_CrCl <- mean(data$CrCl[idx], na.rm = TRUE)
    most_frequent_PPI <- as.numeric(names(sort(table(data$PPI[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1
    most_frequent_UM <- as.numeric(names(sort(table(data$UM[idx]), decreasing = TRUE)[1]))  # Most frequent 0 or 1
    
    # Replace all values within that ID with the calculated values
    data$WT[idx] <- mean_WT
    data$CrCl[idx] <- mean_CrCl
    data$PPI[idx] <- most_frequent_PPI
    data$UM[idx] <- most_frequent_UM
  }
  
  # Store the adjusted data back into the list
  XGB_imputed_data_list[[paste0("XGB_imputed_data_", i)]] <- data
}



# Create a vector for the missing data percentages
missing_percentages <- c("5%", "20%", "50%", "75%")

# Initialize lists to store results
WT_errors <- list()
CrCl_errors <- list()
PPI_concordance <- list()
UM_concordance <- list()

# Loop through the missing and imputed datasets
for (i in 1:length(missing_data_list)) {
  
  # Extract missing rows from the current missing_data_list
  missing_data <- missing_data_list[[i]]
  
  # Get indices of rows with NA values for WT, CrCl, and PPI
  na_indices_WT <- which(is.na(missing_data$WT))
  na_indices_CrCl <- which(is.na(missing_data$CrCl))
  na_indices_PPI <- which(is.na(missing_data$PPI))
  na_indices_UM <- which(is.na(missing_data$UM))
  
  # Retrieve the corresponding imputed values using the indices
  imputed_WT <- XGB_imputed_data_list[[i]][na_indices_WT, ]
  imputed_CrCl <- XGB_imputed_data_list[[i]][na_indices_CrCl, ]
  imputed_PPI <- XGB_imputed_data_list[[i]][na_indices_PPI, ]
  imputed_UM <- XGB_imputed_data_list[[i]][na_indices_UM, ]
  
  # Ensure IDs are unique in the imputed data
  imputed_WT <- imputed_WT %>% distinct(ID, .keep_all = TRUE)
  imputed_CrCl <- imputed_CrCl %>% distinct(ID, .keep_all = TRUE)
  imputed_PPI <- imputed_PPI %>% distinct(ID, .keep_all = TRUE)
  imputed_UM <- imputed_UM %>% distinct(ID, .keep_all = TRUE)

  # Match imputed with original data and compute errors for WT
  WT_error <- imputed_WT %>%
    left_join(final_data %>% select(ID, WT), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(WT_imputed = WT.x, WT_original = WT,
           AE_WT = abs(WT_imputed - WT_original),
           PE_WT = (WT_imputed - WT_original) / WT_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "XGB")
  
  # Match imputed with original data and compute errors for CrCl
  CrCl_error <- imputed_CrCl %>%
    left_join(final_data %>% select(ID, CrCl), by = "ID") %>%
    distinct(ID, .keep_all = TRUE) %>%
    mutate(CrCl_imputed = CrCl.x, CrCl_original = CrCl,
           AE_CrCl = abs(CrCl_imputed - CrCl_original),
           PE_CrCl = (CrCl_imputed - CrCl_original) / CrCl_original * 100,
           Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
           Imputation_Method = "XGB");

  # Match imputed with original data and compute concordance for PPI
PPI_conc <- imputed_PPI %>%
  left_join(final_data %>% select(ID, PPI), by = "ID") %>%
  distinct(ID, .keep_all = TRUE) %>%
  mutate(PPI_imputed = PPI.x, PPI_original = PPI.y,  # Adjust this line based on the column names
         ConcordancePPI = PPI_imputed == PPI_original,
         Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
         Imputation_Method = "XGB")
UM_conc <- imputed_UM %>%
  left_join(final_data %>% select(ID, UM), by = "ID") %>%
  distinct(ID, .keep_all = TRUE) %>%
  mutate(UM_imputed = UM.x, UM_original = UM.y,  # Adjust this line based on the column names
         ConcordanceUM = UM_imputed == UM_original,
         Missing_Values = factor(missing_percentages[i], levels = missing_percentages),
         Imputation_Method = "XGB")
;

  # Store the results in the lists
  WT_errors[[i]] <- WT_error
  CrCl_errors[[i]] <- CrCl_error
  PPI_concordance[[i]] <- PPI_conc
  UM_concordance[[i]] <- UM_conc
}

# Combine the results into single dataframes for WT, CrCl, and PPI
WT_errors_combined <- bind_rows(WT_errors)
CrCl_errors_combined <- bind_rows(CrCl_errors)
PPI_concordance_combined <- bind_rows(PPI_concordance)
UM_concordance_combined <- bind_rows(UM_concordance)

# Combine the dataframes using cbind without joining by ID
XGB_combined_data <- cbind(
  WT_errors_combined %>% select(ID, AE_WT, PE_WT, Missing_Values, Imputation_Method),
  CrCl_errors_combined %>% select(AE_CrCl, PE_CrCl),
  PPI_concordance_combined %>% select(ConcordancePPI),
  UM_concordance_combined %>% select(ConcordanceUM)
)

# View the combined dataframe
XGB_combined_data

# Define the columns of interest
columns_of_interest <- c("Height", "WT", "Alb", "CrCl", "Age", "PPI", "UM")

# Define the percentages of missing data
missing_percentages <- c("5% NA", "20% NA", "50% NA", "75% NA")

# Combine all datasets into a list
all_datasets <- list(
  sim_data,  # Original simulated data
  
  # MICE imputed datasets
  mice_imputed_data_list[[1]],
  mice_imputed_data_list[[2]],
  mice_imputed_data_list[[3]],
  mice_imputed_data_list[[4]],
  
  # AMELIA imputed datasets
  amelia_imputed_data_list[[1]],
  amelia_imputed_data_list[[2]],
  amelia_imputed_data_list[[3]],
  amelia_imputed_data_list[[4]],
  
  # MF imputed datasets
  MF_imputed_data_list[[1]],
  MF_imputed_data_list[[2]],
  MF_imputed_data_list[[3]],
  MF_imputed_data_list[[4]],
  
  # XGB imputed datasets
  XGB_imputed_data_list[[1]],
  XGB_imputed_data_list[[2]],
  XGB_imputed_data_list[[3]],
  XGB_imputed_data_list[[4]],
  
  # MIDAS imputed datasets
  MIDAS_imputed_data_list[[1]],
  MIDAS_imputed_data_list[[2]],
  MIDAS_imputed_data_list[[3]],
  MIDAS_imputed_data_list[[4]]
)

# Assign names to datasets
dataset_names <- c(
  "Original Data",
  
  # MICE datasets with missing percentages
  paste0("MICE ", missing_percentages),
  
  # AMELIA datasets
  paste0("Amelia ", missing_percentages),
  
  # MF datasets
  paste0("MF ", missing_percentages),
  
  # XGB datasets
  paste0("XGB ", missing_percentages),
  
  # MIDAS datasets
  paste0("MIDAS ", missing_percentages)
)

# Ensure that the number of datasets matches the number of names
if (length(all_datasets) != length(dataset_names)) {
  stop("The number of datasets and dataset names do not match.")
}

# Initialize an empty list to store plots
plot_list <- list()

# Loop over each dataset
for (i in seq_along(all_datasets)) {
  data <- all_datasets[[i]]
  
  # Check if all columns of interest are present in the dataset
  missing_cols <- setdiff(columns_of_interest, names(data))
  if (length(missing_cols) > 0) {
    warning(paste("Dataset", dataset_names[i], "is missing columns:", paste(missing_cols, collapse = ", ")))
    next
  }
  
  # Select columns of interest
  data_subset <- data[, columns_of_interest]
  
  # Convert all columns to numeric
  data_subset <- data_subset %>% mutate_all(~ as.numeric(as.character(.)))
  
  # Remove rows with NAs
  data_subset <- na.omit(data_subset)
  if (nrow(data_subset) == 0) {
    warning(paste("Dataset", dataset_names[i], "has no complete cases. Skipping."))
    next
  }
  
  # Compute correlation matrix
  corr_matrix <- cor(data_subset, use = "complete.obs", method = "pearson")
  
  # Generate the correlation plot using ggcorrplot with labels
  p <- ggcorrplot(corr_matrix,
                  type = "upper",
                  lab = TRUE,        # Include correlation coefficients
                  lab_size = 3,      # Adjust label size for readability
                  title = dataset_names[i],
                  show.legend = FALSE,
                  ggtheme = my_theme,
                  colors = c("blue", "white", "red"))  # Negative correlations in blue, positive in red
  
  # Adjust plot theme for readability
  p <- p + theme(
    plot.title = element_text(size = 10, hjust = 0.5),
    axis.text = element_text(size = 6)
  )
  
  # Add the plot to the list
  plot_list[[i]] <- p
}



plot_indices <- c(
  1, 2, 3, 4, 5,1, 6, 7, 8, 9,
  1, 10, 11, 12, 13, 1, 14, 15,
  16, 17, 1, 18, 19, 20, 21
)


# Arrange plots using ggarrange
# Since we now have 21 datasets, we'll arrange them in 7 columns and 3 rows
combined_plot <- ggarrange(
  plotlist = plot_list[plot_indices],
  ncol = 5, nrow = 5,
  labels = NULL
)




# Save the combined plot to a PNG file using png() and dev.off()
# Set the output image size and resolution
png("Correlation plot.png", width = 20, height = 16, units = 'in', res = 900)

# Draw the combined plot
print(combined_plot)

# Close the graphics device
dev.off()





# Assuming you have the correlation matrices stored or you can compute them again
corr_matrices <- list()

# Compute correlation matrices for all datasets
for (i in seq_along(all_datasets)) {
  data <- all_datasets[[i]]
  data_subset <- data[, columns_of_interest]
  
  # Convert all columns to numeric
  data_subset <- data_subset %>% mutate_all(~ as.numeric(as.character(.)))
  
  # Remove rows with NAs
  data_subset <- na.omit(data_subset)
  
  # Compute correlation matrix
  corr_matrix <- cor(data_subset, use = "complete.obs", method = "pearson")
  
  # Store the correlation matrix
  corr_matrices[[i]] <- corr_matrix
}

# Assign names to the correlation matrices
names(corr_matrices) <- dataset_names




# Extract the original correlation matrix
corr_original <- corr_matrices[["Original Data"]]

# Initialize a data frame to store the results
frobenius_results <- data.frame(
  Dataset = character(),
  Frobenius_Norm = numeric(),
  stringsAsFactors = FALSE
)

# Loop through the imputed datasets
for (i in seq_along(corr_matrices)) {
  dataset_name <- names(corr_matrices)[i]
  
  # Skip the original dataset
  if (dataset_name == "Final Data") next
  
  corr_matrix <- corr_matrices[[i]]
  
  # Compute the Frobenius norm of the difference
  diff_matrix <- corr_matrix - corr_original
  frob_norm <- norm(diff_matrix, type = "F")
  
  # Store the result
  frobenius_results <- rbind(frobenius_results, data.frame(
    Dataset = dataset_name,
    Frobenius_Norm = frob_norm
  ))
}

# Display the results sorted by Frobenius Norm
frobenius_results <- frobenius_results %>% arrange(Frobenius_Norm)
print(frobenius_results)




# Initialize a data frame to store the results
rv_results <- data.frame(
    Dataset = character(),
    RV_Coefficient = numeric(),
    stringsAsFactors = FALSE
)

# Loop through the imputed datasets
for (i in seq_along(corr_matrices)) {
    dataset_name <- names(corr_matrices)[i]
    
    # Skip the original dataset
    if (dataset_name == "Final Data") next
    
    corr_matrix <- corr_matrices[[i]]
    
    # Compute the RV coefficient
    rv_obj <- coeffRV(corr_original, corr_matrix)
    
    # Extract the RV coefficient value
    rv_value <- rv_obj$rv
    
    # Store the result
    rv_results <- rbind(rv_results, data.frame(
        Dataset = dataset_name,
        RV_Coefficient = rv_value
    ))
}

# Display the results sorted by RV Coefficient (descending)
rv_results <- rv_results %>% arrange(desc(RV_Coefficient))
print(rv_results)


# Frobenius Norm Plot
plot_frobenius <- ggplot(frobenius_results, aes(x = reorder(Dataset, Frobenius_Norm), y = Frobenius_Norm)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(title = "Frobenius Norm of Correlation Matrix Differences",
       x = "Dataset",
       y = "Frobenius Norm") +
  my_theme


# RV Coefficient Plot
plot_rv <- ggplot(rv_results, aes(x = reorder(Dataset, -RV_Coefficient), y = RV_Coefficient)) +
  geom_bar(stat = "identity", fill = "salmon") +
  coord_flip() +
  labs(title = "RV Coefficient between Correlation Matrices",
       x = "Dataset",
       y = "RV Coefficient") +
  my_theme


combined_plot <- ggarrange(
  plot_frobenius,
  plot_rv,
  ncol = 1, nrow = 2,
  labels = c("A", "B")
)


png(filename = "Combined_Metrics_Corr_Plot.png",
    width = 10,    # Adjust width as needed
    height = 12,   # Adjust height as needed
    units = "in",
    res = 300)

# Draw the combined plot
print(combined_plot)

# Close the graphics device
dev.off()








# Ensure the column names are consistent (optional, depending on your specific case)
# If necessary, rename columns in one or more data frames to ensure consistency.

# Merging the data frames by rows
combined_data <- bind_rows(
  MIDAS_combined_data %>% mutate(Imputation_Method = "MIDAS"),
  MF_combined_data %>% mutate(Imputation_Method = "MF"),
  mice_combined_data %>% mutate(Imputation_Method = "Mice"),
  amelia_combined_data %>% mutate(Imputation_Method = "amelia"),
  XGB_combined_data %>% mutate(Imputation_Method = "XGB"),
)



# Compute summary statistics
summary_stats <- combined_data %>%
  group_by(Missing_Values, Imputation_Method) %>%
  summarise(
    # Absolute Error with Standard Deviation
    AE_WT = sprintf("%.2f ± %.2f", mean(AE_WT, na.rm = TRUE), sd(AE_WT, na.rm = TRUE)),
    AE_CrCl = sprintf("%.2f ± %.2f", mean(AE_CrCl, na.rm = TRUE), sd(AE_CrCl, na.rm = TRUE)),
    
    # Percentage Error with Standard Deviation
    PE_WT = sprintf("%.2f ± %.2f", mean(PE_WT, na.rm = TRUE), sd(PE_WT, na.rm = TRUE)),
    PE_CrCl = sprintf("%.2f ± %.2f", mean(PE_CrCl, na.rm = TRUE), sd(PE_CrCl, na.rm = TRUE)),
    
    # Percentage of Concordance
    Concordance_PPI = sprintf("%.2f%%", mean(PPI, na.rm = TRUE) * 100),
    Concordance_UM = sprintf("%.2f%%", mean(UM, na.rm = TRUE) * 100)
  ) %>%
  ungroup()

# Ensure Missing_Values is ordered correctly and Imputation_Method sorted alphabetically
summary_stats <- summary_stats %>%
  mutate(
    Missing_Values = factor(
      Missing_Values,
      levels = c("5%", "20%", "50%", "75%")  # Ensure Missing_Values order
    ),
    Imputation_Method = factor(Imputation_Method, levels = sort(unique(Imputation_Method)))  # Alphabetical order
  ) %>%
  arrange(Missing_Values, Imputation_Method)  # Sort by Missing_Values and then Imputation_Method



# Create the flextable
ft <- flextable(summary_stats) %>%
  theme_box() %>%
  set_header_labels(
    Missing_Values = "Missing Percentage",
    Imputation_Method = "Imputation Method",
    AE_WT = "AE (WT) [Mean ± SD]",
    PE_WT = "PE (WT) [Mean ± SD]",
    AE_CrCl = "AE (CrCl) [Mean ± SD]",
    PE_CrCl = "PE (CrCl) [Mean ± SD]",
    Concordance_PPI = "% Concordance (PPI)",
    Concordance_UM = "% Concordance (UM)"
  ) %>%
  merge_v(j = c("Missing_Values", "Imputation_Method")) %>%  # Merge rows first by Missing_Values, then Imputation_Method
  fontsize(size = 10, part = "all") %>%
  font(part = "all", fontname = "Times New Roman") %>%
  bold(part = "header") %>%
  align(align = "center", part = "all") %>%
  width(j = c("Missing_Values", "Imputation_Method"), width = 1.5) %>%
  width(j = c("AE_WT", "PE_WT", "AE_CrCl", "PE_CrCl", "Concordance_PPI", "Concordance_UM"), width = 1.8)


# Create a Word document and add the flextable
doc <- read_docx() %>%
  body_add_par("Summary of AE, PE (WT and CrCl), and Concordance (PPI and UM)", style = "heading 1") %>%
  body_add_flextable(value = ft)

# Save the document
print(doc, target = "Summary_Statistics_with_Concordance.docx")


# View the combined data frame
print(combined_data)
long_combined_data <- combined_data %>%
  pivot_longer(
    cols = c(AE_WT, PE_WT, AE_CrCl, PE_CrCl), 
    names_to = c("ErrorType", "Variable"), 
    names_sep = "_"
  ) %>%
  mutate(
    ErrorType = factor(
      ErrorType, 
      levels = c("AE", "PE"), 
      labels = c("Absolute Error", "Percentage Error")  # New labels
    )
  )


AEPE_plot<-ggplot(long_combined_data, aes(x = Missing_Values, y = value, fill = Imputation_Method)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA, color = "black", linewidth = 1.2) +  # Thicker boxplot borders
  geom_point(color = "black", position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.75), alpha = 1, size = 0.6, show.legend = FALSE) +  # Black points without legend
  facet_grid(ErrorType ~ Variable, scales = "free_y") +  # Facet by ErrorType in rows and Variable in columns
  labs(title = "Comparison of Absolute and Percentage Error for WT and CrCl by Percentage of Missing Data",
       x = "Percentage of Missing Data",
       y = "Error Value",
       fill = "Imputation Method") +
  scale_color_jco() +  # Color scheme for lines
  scale_fill_jco() +   # Fill color scheme for ribbons
  my_theme +  # Apply your custom theme
  theme(strip.background = element_rect(fill = "grey90", color = "grey50"),  # Grey background and border
        strip.text = element_text(face = "bold", color = "black"))  # Bold text for facet labels


png("AE_PE_comparison.png", width = 12, height = 9, units = 'in', res = 900)  # Specify width, height, and resolution

AEPE_plot

# Close the device
dev.off()







# Merge the five imputed dataset lists into a single list


# Assuming 'combined_data' already contains both PPI and UM variables

combined_data <- combined_data %>%
  rename(PPI = ConcordancePPI, UM = ConcordanceUM)

# Count the number of TRUE/FALSE for PPI, grouped by Imputation_Method and Missing_Values
concordance_summary_PPI <- combined_data %>%
  group_by(Imputation_Method, Missing_Values, PPI) %>%
  summarise(Count = n()) %>%
  arrange(Imputation_Method, Missing_Values)

# Count the number of TRUE/FALSE for UM, grouped by Imputation_Method and Missing_Values
concordance_summary_UM <- combined_data %>%
  group_by(Imputation_Method, Missing_Values, UM) %>%
  summarise(Count = n()) %>%
  arrange(Imputation_Method, Missing_Values)

# Filter out rows with NA in PPI and UM
concordance_summary_PPI_filtered <- concordance_summary_PPI %>% filter(!is.na(PPI))
concordance_summary_UM_filtered <- concordance_summary_UM %>% filter(!is.na(UM))

# Calculate percentages for TRUE/FALSE concordance within each combination of Imputation_Method and Missing_Values
concordance_summary_PPI_pct <- concordance_summary_PPI_filtered %>%
  group_by(Imputation_Method, Missing_Values) %>%
  mutate(Percent = 100 * Count / sum(Count))  # Calculate percentage within each group

concordance_summary_UM_pct <- concordance_summary_UM_filtered %>%
  group_by(Imputation_Method, Missing_Values) %>%
  mutate(Percent = 100 * Count / sum(Count))  # Calculate percentage within each group

# Ensure both TRUE and FALSE categories are present for consistent bar widths
concordance_summary_PPI_pct <- concordance_summary_PPI_pct %>%
  complete(PPI = c(TRUE, FALSE), fill = list(Count = 0, Percent = 0))

concordance_summary_UM_pct <- concordance_summary_UM_pct %>%
  complete(UM = c(TRUE, FALSE), fill = list(Count = 0, Percent = 0))

# Create the bar plot for PPI
PPI_Plot <- ggplot(concordance_summary_PPI_pct, aes(x = Missing_Values, y = Percent, fill = PPI)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), color = "black", linewidth = 1.2) +
  geom_text(aes(label = Count, y = 28), position = position_dodge(width = 0.8), 
            vjust = 0, size = 3.5, fontface = "bold") +
  facet_wrap(~ Imputation_Method, scales = "free", ncol = 2) +
  labs(title = "Concordance for PPI (TRUE/FALSE) by Imputation Method and Missing Data",
       x = "Percentage of Missing Data",
       y = "Frequency (%) of TRUE/FALSE Concordance",
       fill = "Concordance (PPI)") +
  scale_fill_jco() +
  my_theme +
  scale_x_discrete(drop = FALSE) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold"),
    panel.border = element_rect(color = "black", fill = NA, size = 1),
    strip.placement = "outside",
    plot.margin = margin(2, 2, 2, 2, "cm")
  ) +
  my_theme  # Apply your custom theme

# Create the bar plot for UM
UM_Plot <- ggplot(concordance_summary_UM_pct, aes(x = Missing_Values, y = Percent, fill = UM)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.8), color = "black", linewidth = 1.2) +
  geom_text(aes(label = Count, y = 28), position = position_dodge(width = 0.8), 
            vjust = 0, size = 3.5, fontface = "bold") +
  facet_wrap(~ Imputation_Method, scales = "free", ncol = 2) +
  labs(title = "Concordance for UM (TRUE/FALSE) by Imputation Method and Missing Data",
       x = "Percentage of Missing Data",
       y = "Frequency (%) of TRUE/FALSE Concordance",
       fill = "Concordance (UM)") +
  scale_fill_jco() +
  my_theme +
  scale_x_discrete(drop = FALSE) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold"),
    panel.border = element_rect(color = "black", fill = NA, size = 1),
    strip.placement = "outside",
    plot.margin = margin(2, 2, 2, 2, "cm")
  ) +
  my_theme  # Apply your custom theme

# Save the PPI Plot
png("PPI_comparison.png", width = 12, height = 9, units = 'in', res = 900)
print(PPI_Plot)
dev.off()

# Save the UM Plot
png("UM_comparison.png", width = 12, height = 9, units = 'in', res = 900)
print(UM_Plot)
dev.off()


png("PPI_UM_comparison.png", width = 12, height = 16, units = 'in', res = 900)
ggarrange(print(PPI_Plot),print(UM_Plot),common.legend=TRUE,nrow=2,labels="AUTO")
dev.off()





merged_imputed_data_list <- c(
  mice_imputed_data_list,
  MF_imputed_data_list,
  MIDAS_imputed_data_list,
  amelia_imputed_data_list,
  XGB_imputed_data_list
)

# Append final_data to the merged list
merged_imputed_data_list$final_data <- final_data

# Check the structure of the merged list (including final_data)
str(merged_imputed_data_list)

# Optionally, print the names of the datasets for verification
names(merged_imputed_data_list) <- c(
  paste0("mice_", seq_along(mice_imputed_data_list)),
  paste0("MF_", seq_along(MF_imputed_data_list)),
  paste0("MIDAS_", seq_along(MIDAS_imputed_data_list)),
  paste0("amelia_", seq_along(amelia_imputed_data_list)),
  paste0("XGB_", seq_along(XGB_imputed_data_list)),
  "final_data"
)

# Check the first few dataset names in the merged list
head(names(merged_imputed_data_list))





# Define the time points where new rows should be added (dosing times)
dosing_times <- c(0, 8, 16, 24, 32, 40)

# Define the allowed times for rows with amt = 0, evid = 0
allowed_times <- c(0.25,0.5, 4,36, 40)

# Function to add columns and rows for each dataset and remove unwanted rows
modify_dataset_add_rows <- function(data) {
  # Get unique IDs
  unique_ids <- unique(data$ID)
  
  # Initialize an empty data frame to store the modified data
  expanded_data <- data.frame()
  
  # Loop through each ID and add new rows at dosing times
  for (id in unique_ids) {
    # Subset the data for the current ID
    id_data <- data[data$ID == id, ]
    
    # Create new rows for dosing times by copying the first row of id_data
    new_rows <- id_data[1:length(dosing_times), ]
    
    # Set the time for the new rows
    new_rows$time <- dosing_times
    
    # Assign amt, evid, and cmt for the new rows
    new_rows$amt <- 100
    new_rows$evid <- 1
    new_rows$cmt <- 1
    
    # Set DV to NA for the new rows (as DV doesn't exist at these times)
    new_rows$DV <- NA
    
    # Set amt, evid, and cmt for existing rows in id_data to 0
    id_data$amt <- 0
    id_data$evid <- 0
    id_data$cmt <- 0
    
    # Combine the original rows with the new dosing rows
    combined_data <- rbind(id_data, new_rows)
    
    # Sort the combined data by time
    combined_data <- combined_data[order(combined_data$time), ]
    
    # Remove rows where EVID = 0 and AMT = 0 outside the allowed times
    combined_data <- combined_data[!(combined_data$evid == 0 & combined_data$amt == 0 & !combined_data$time %in% allowed_times), ]
    
    # Handle duplicates at times 8 and 40 where EVID = 0, keeping only the first instance
    for (t in c(8, 40)) {
      duplicated_condition <- which(combined_data$time == t & combined_data$evid == 0)
      
      # If there are multiple rows matching the condition, remove all but the first
      if (length(duplicated_condition) > 1) {
        combined_data <- combined_data[-duplicated_condition[-1], ]  # Keep only the first duplicate
      }
    }
    
    # Append to the expanded_data
    expanded_data <- rbind(expanded_data, combined_data)
  }
  
  return(expanded_data)
}

# Apply the function to each dataset in the merged list
for (i in seq_along(merged_imputed_data_list)) {
  merged_imputed_data_list[[i]] <- modify_dataset_add_rows(merged_imputed_data_list[[i]])
}

# Check the result for the first dataset in the merged list
head(merged_imputed_data_list[[1]], 40)


# Identify the last dataframe (which has 18 columns)
last_df <- merged_imputed_data_list[[length(merged_imputed_data_list)]]

# Extract the last 5 columns from the last dataframe
last_5_columns <- last_df[, (ncol(last_df) - 4):ncol(last_df)]

# Loop through the first 20 dataframes and add the last 5 columns
for (i in 1:20) {
  # Add the last 5 columns to each dataframe in the list
  merged_imputed_data_list[[i]] <- cbind(merged_imputed_data_list[[i]], last_5_columns)
}

# Now check the result for one of the first 20 dataframes to ensure the columns have been added
head(merged_imputed_data_list[[1]], 10)


# Define the column order from final_data
final_data_columns <- c("ID", "time", "Height", "WT", "Alb", "CrCl", "Age", "PPI",
                        "UM", "amt", "evid", "cmt", "GUT", "CENTRAL", "DV", "CL", 
                        "VC", "KA")

# Reorder columns in each dataframe in merged_imputed_data_list
for (i in seq_along(merged_imputed_data_list)) {
  # Only reorder columns if all required columns are present in the dataframe
  common_columns <- intersect(final_data_columns, colnames(merged_imputed_data_list[[i]]))
  merged_imputed_data_list[[i]] <- merged_imputed_data_list[[i]][, common_columns, drop = FALSE]
  
  # If there are any missing columns in the current dataframe, add them as NA
  missing_columns <- setdiff(final_data_columns, colnames(merged_imputed_data_list[[i]]))
  for (col in missing_columns) {
    merged_imputed_data_list[[i]][, col] <- NA
  }
  
  # Ensure the final column order matches final_data
  merged_imputed_data_list[[i]] <- merged_imputed_data_list[[i]][, final_data_columns]
}

# Now all dataframes in merged_imputed_data_list should have the same column order as final_data

















# Define the base directory by removing the "Figures" part
base_directory <- "D:/MOBILITE/1er papier - Simulation Covariables"

# Define the new folder to store the imputed datasets
imputed_datasets_folder <- file.path(base_directory, "imputed_datasets_merged")

# Create the new folder if it doesn't exist
if (!dir.exists(imputed_datasets_folder)) {
  dir.create(imputed_datasets_folder)
}

# Save each dataframe in merged_imputed_data_list to a CSV file using its name
for (i in seq_along(merged_imputed_data_list)) {
  # Get the name of the current dataframe
  df_name <- names(merged_imputed_data_list)[i]
  
  # Define the file name and path based on the dataframe name
  file_name <- paste0(df_name, ".csv")
  file_path <- file.path(imputed_datasets_folder, file_name)
  
  # Write the dataframe to CSV
  write.csv(merged_imputed_data_list[[i]], file = file_path, row.names = FALSE)
}

# Print a message indicating completion
print("Merged imputed dataframes have been successfully exported into 'imputed_datasets_merged'.")



# Rename percentages for missing data
percentages <- c("0% NA", "5% NA", "20% NA", "50% NA", "75% NA")

# Map of imputation methods to the names used in the dataframe list
method_map <- c("mice", "MF", "MIDAS", "amelia", "XGB")

# Initialize an empty list to store the subsetted data
subsetted_data_list <- list()

# Add final_data with 0% missing data, keeping the ID column
final_data <- merged_imputed_data_list[["final_data"]] %>%
  select(ID, WT, CrCl) %>%
  mutate(Imputation_Method = "final", Missing_Percentage = "0% NA") %>%
  distinct(ID, .keep_all = TRUE)  # Keep only the first row of each ID
subsetted_data_list[["final_data"]] <- final_data

# Loop through the imputation methods and percentages of missing data
for (method in method_map) {
    for (i in 1:4) {
        df_name <- paste0(method, "_", i)
        
        # Check if the dataframe exists before processing
        if (!is.null(merged_imputed_data_list[[df_name]])) {
          data <- merged_imputed_data_list[[df_name]] %>%
            select(ID, WT, CrCl) %>%
            mutate(
              Imputation_Method = method,                     # String before "_"
              Missing_Percentage = percentages[i + 1]  # String after "_", skipping "0% NA"
            ) %>%
            distinct(ID, .keep_all = TRUE)  # Keep only the first row of each ID
        
          # Store the result in the list
          subsetted_data_list[[df_name]] <- data
        } else {
          message(paste("Dataframe", df_name, "not found, skipping."))
        }
    }
}

# Combine all subsetted dataframes into a single dataframe
combined_subset_data <- bind_rows(subsetted_data_list)

# View the combined dataframe structure
str(combined_subset_data)

# If you want to save the combined data:
write.csv(combined_subset_data, "combined_subset_data.csv", row.names = FALSE)





# Exclude final_data
filtered_combined_data <- combined_subset_data %>%
  filter(Imputation_Method != "final")  # Exclude final data

# Reorder Missing_Percentage and Imputation_Method as factors
filtered_combined_data$Missing_Percentage <- factor(filtered_combined_data$Missing_Percentage,
                                                  levels = c("5% NA", "20% NA", "50% NA", "75% NA"))

filtered_combined_data$Imputation_Method <- factor(filtered_combined_data$Imputation_Method,
                                                 levels = c("amelia", "MF", "mice", "MIDAS", "XGB"))







# Corrected function to match density with the histogram by dividing by the binwidth and add custom fill, spacing, and proper legend
plot_histogram_with_gaussian_and_density_fixed <- function(data, variable, variable_name) {
    binwidth_value <- ifelse(variable == "WT", 2, 2)  # Define binwidth
    
    ggplot(data, aes_string(x = variable)) +
        # Histogram with appropriate binwidth
        geom_histogram(aes(y = ..count../2.5, fill = Imputation_Method),  
                       binwidth = binwidth_value,  
                       color = "black", alpha = 0.7, show.legend = TRUE) +
        
        # Gaussian curve for each subset
        stat_function(data = data,
                      fun = function(x) {
                          mean_val <- mean(data[[variable]], na.rm = TRUE)
                          sd_val <- sd(data[[variable]], na.rm = TRUE)
                          dnorm(x, mean = mean_val, sd = sd_val)*100
                      }, 
                      aes(linetype = "Gaussian Distribution", color = "Gaussian Distribution"), 
                      size = 1, show.legend = TRUE) +
        
        # Density curve properly scaled by dividing by the binwidth
        geom_density(aes(y = ..density..*100 , 
                         linetype = "Density Estimate", color = "Density Estimate"),  
                     size = 1, alpha = 0.5, show.legend = TRUE) +
        
        # Facet by Missing_Percentage and Imputation_Method with custom spacing
        facet_grid(Imputation_Method ~ Missing_Percentage, scales = "free_y") +
        
        # Add the color palette from ggsci
        scale_fill_jco() +
        
        # Add manual legends for Gaussian and Density Estimate, both with solid lines and color differentiation
        scale_linetype_manual(name = "Legend", values = c("Density Estimate" = "solid", "Gaussian Distribution" = "solid")) +  
        scale_color_manual(name = "Legend", values = c("Gaussian Distribution" = "blue", "Density Estimate" = "red")) + 
        
        labs(title = paste("Distribution of", variable_name),
             x = variable_name,
             y = "Count") +
        
        # Custom theme with spacing and legend
        my_theme +
        theme(
            plot.title = element_text(hjust = 0.5, face = "bold"),
            axis.title = element_text(size = 16),
            axis.text = element_text(size = 14),
            strip.text = element_text(face = "bold"),
            
            # Add spacing between facets for better separation
            panel.spacing = unit(1, "lines"),  
            
            # Adjust legend
            legend.position = "top",
            legend.key = element_rect(fill = "white", color = "black"), 
            legend.title = element_text(face = "bold"),
            legend.text = element_text(size = 12),
            
            # Add grey borders to each facet panel
            panel.border = element_rect(color = "grey", fill = NA, size = 1),  
            strip.background = element_rect(fill = "grey", color = "grey", size = 1)
        ) +
        
        # Custom guides for the legend
        guides(
            fill = guide_legend(title = "Imputation Method"),
            linetype = guide_legend(override.aes = list(color = c("red", "blue"), fill = "white")),  
            color = "none"
        )
}

# Plot for WT
WT_Plot_fixed <- plot_histogram_with_gaussian_and_density_fixed(filtered_combined_data, "WT", "Weight (WT)")

# Save the plot for WT
png("WT_Distribution_with_Gaussian_Density_Proper_Scaled_Spaced_with_Legend.png", width = 16, height = 9, units = 'in', res = 900)
print(WT_Plot_fixed)
dev.off()



# Plot for CrCl
CrCl_Plot_fixed <- plot_histogram_with_gaussian_and_density_fixed(filtered_combined_data, "CrCl", "Creatinine Clearance (CrCl)")


# Save the plot for CrCl
png("CrCl_Distribution_with_Gaussian_Density_Proper_Scaled_Spaced_with_Legend.png", width = 16, height = 9, units = 'in', res = 900)
print(CrCl_Plot_fixed)
dev.off()


combined_plot <- ggarrange(
  WT_Plot_fixed, 
  CrCl_Plot_fixed,
  nrow = 2,
  common.legend = TRUE,  # Combine legends
  labels = "AUTO"        # Add labels "A", "B", etc.
)

# Display the combined plot
print(combined_plot)



png("WT_CrCl_Distrib.png", width = 12, height = 16, units = 'in', res = 900)
print(combined_plot)
dev.off()

OFVval <- read.csv2("D:/MOBILITE/1er papier - Simulation Covariables/OBJ/OBJ.csv")




# Define the custom palette using the first 10 colors of `jco` and colors 3 to 8 of `nejm`
custom_palette <- c(
  pal_jco("default")(10),  # First 10 colors from jco palette
  pal_nejm("default")(8)[3:8]   # Colors 3 to 8 from nejm palette
)

# Add a new column for the combined label with BICc and OFV values
OFVval <- OFVval %>%
  mutate(Label = paste0(Model, "\nOFV: ", OFV, "   BICc: ", BICc))  # Add values to labels

# Reshape data to have OFV and BICc in long format for faceting
plot_data <- OFVval %>%
  pivot_longer(cols = c(OFV, BICc), names_to = "Metric", values_to = "Value") %>%
  group_by(Metric) %>%
  arrange(desc(Value)) %>%
  mutate(
    Label = factor(Label, levels = unique(Label)),  # Set Label factor levels for sorted legend
    Model = factor(Model)  # Keep Model as factor without reordering x-axis
  ) %>%
  ungroup()  # Ungroup after setting factor levels

# Create the bar plot using Label as the fill
OFV_plot <- ggplot(plot_data, aes(x = reorder(Model, -Value), y = Value, fill = Label)) +
  geom_col(position = position_dodge(width = 0.3), color = "black") +  # Reduce width for more space between bars
  coord_cartesian(ylim = c(1450, 1750)) +  # Zoom in on Y-axis from 1450 to 1750
  facet_wrap(~ Metric, scales = "free_y") +  # Facet by Metric (OFV and BICc)
  labs(title = "Comparison of OFV and BICc by Model",
       x = "Model",
       y = "Value") +
  scale_fill_manual(values = custom_palette) +  # Apply the custom palette
  my_theme +  # Apply your custom theme
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for readability
    legend.spacing.y = unit(1, "cm"),  # Add space between legend items
    legend.key.height = unit(1, "cm"),  # Increase legend key height for additional spacing
    panel.spacing = unit(1, "cm")  # Increase space between facets
  )

# Save the plot with a wider width to accommodate the layout
png("D:/MOBILITE/1er papier - Simulation Covariables/Figures/OFV.png", width = 20, height = 9, units = 'in', res = 900)
print(OFV_plot)
dev.off()






# Define the base directory where the Monolix folder is located
base_dir <- "D:/MOBILITE/1er papier - Simulation Covariables/Monolix"

# List all directories inside the Monolix folder (amelia_1, MF_1, etc.)
subfolders <- list.dirs(base_dir, full.names = TRUE, recursive = FALSE)



# Modify the function to import data as dataframes and keep them in a list
import_monolix_data <- function(subfolder) {
  # Paths to additional subfolders
  log_likelihood_folder <- file.path(subfolder, "LogLikelihood")
  tests_folder <- file.path(subfolder, "Tests")
  
  # Paths to main folder files
  population_parameters_file <- file.path(subfolder, "populationParameters.txt")
  predictions_file <- file.path(subfolder, "predictions.txt")
  
  # Paths to files in subfolders
  log_likelihood_file <- file.path(log_likelihood_folder, "logLikelihood.txt")
  correlation_file <- file.path(tests_folder, "correlationIndividualParametersCovariates.txt")
  
  # Path to the IndividualParameters folder
  ind_param_folder <- file.path(subfolder, "IndividualParameters")
  
  # Initialize an empty list to store dataframes for each file
  combined_data <- list()
  
  # Define the separator used in your .txt files (e.g., "\t" for tab-separated, "," for CSV)
  separator <- ","  # Adjust this based on the actual file structure
  
  # Check if the folders and files exist and import them
  if (dir.exists(ind_param_folder)) {
    param_file <- file.path(ind_param_folder, "estimatedIndividualParameters.txt")
    random_effects_file <- file.path(ind_param_folder, "estimatedRandomEffects.txt")
    shrinkage_file <- file.path(ind_param_folder, "shrinkage.txt")
    
    if (file.exists(param_file)) {
      param_data <- read.table(param_file, header = TRUE, sep = separator)
      combined_data[["parameters"]] <- param_data
    }
    if (file.exists(random_effects_file)) {
      random_effects_data <- read.table(random_effects_file, header = TRUE, sep = separator)
      combined_data[["random_effects"]] <- random_effects_data
    }
    if (file.exists(shrinkage_file)) {
      shrinkage_data <- read.table(shrinkage_file, header = TRUE, sep = separator)
      combined_data[["shrinkage"]] <- shrinkage_data
    }
    if (file.exists(log_likelihood_file)) {
      log_likelihood_data <- read.table(log_likelihood_file, header = TRUE, sep = separator)
      combined_data[["log_likelihood"]] <- log_likelihood_data
    }
    if (file.exists(correlation_file)) {
      correlation_data <- read.table(correlation_file, header = TRUE, sep = separator)
      combined_data[["correlation"]] <- correlation_data
    }
    if (file.exists(population_parameters_file)) {
      population_parameters_data <- read.table(population_parameters_file, header = TRUE, sep = separator)
      combined_data[["population_parameters"]] <- population_parameters_data
    }
    if (file.exists(predictions_file)) {
      predictions_data <- read.table(predictions_file, header = TRUE, sep = separator)
      combined_data[["predictions"]] <- predictions_data
    }
    
    return(combined_data)  # Return the list of dataframes for the subfolder
  } else {
    message("Folder not found: ", ind_param_folder)
    return(NULL)
  }
}

# Apply the function to each subfolder and store results in a list
all_data <- lapply(subfolders, import_monolix_data)

# Name the list elements with the subfolder names
names(all_data) <- basename(subfolders)






# Create an empty list to store the merged shrinkage data
shrinkage_combined <- list()

# Define the percentages corresponding to the suffixes
missing_data_percentages <- c("_1" = "5%", "_2" = "20%", "_3" = "50%", "_4" = "75%")

# Loop through each element in the all_data list
for (folder_name in names(all_data)) {
  # Extract the shrinkage dataframe
  shrinkage_df <- all_data[[folder_name]]$shrinkage
  
  # Add a column for the folder name (e.g., amelia_1)
  shrinkage_df$File <- folder_name
  
  # Extract the name before the underscore
  base_name <- sub("_[0-9]+", "", folder_name)
  shrinkage_df$Base_Name <- base_name
  
  # Extract the missing data percentage from the folder name
  missing_suffix <- sub(".*(_[0-9]+)", "\\1", folder_name)
  shrinkage_df$Missing_Percentage <- missing_data_percentages[missing_suffix]
  
  # Append the dataframe to the list
  shrinkage_combined[[folder_name]] <- shrinkage_df
}

# Combine all shrinkage dataframes into one dataframe using rbind
final_shrinkage_df <- do.call(rbind, shrinkage_combined)

# View the combined dataframe
head(final_shrinkage_df)


# Use pivot_wider to spread the 'parameters' values into separate columns
final_shrinkage_wide <- final_shrinkage_df %>%
  tidyr::pivot_wider(
    names_from = parameters,         # Use the 'parameters' column to create new column names
    values_from = c(shrinkage_mode, shrinkage_mean, shrinkage_condDist) # Values to spread into the new columns
  )

# View the transformed wide dataframe
head(final_shrinkage_wide)


final_shrinkage_wide$Missing_Percentage[is.na(final_shrinkage_wide$Missing_Percentage)] <- "0%"


# Replace NA in Missing_Percentage with '0%'
final_shrinkage_wide$Missing_Percentage[is.na(final_shrinkage_wide$Missing_Percentage)] <- "0%"

# Reorder the Missing_Percentage as a factor with the specified order
final_shrinkage_wide$Missing_Percentage <- factor(
  final_shrinkage_wide$Missing_Percentage,
  levels = c("0%", "5%", "20%", "50%", "75%")
)

# Reshape the data to long format (split into shrinkage type and parameter)
final_shrinkage_long <- final_shrinkage_wide %>%
  pivot_longer(
    cols = starts_with("shrinkage"),  # Select shrinkage-related columns
    names_to = c("Shrinkage_Type", "Parameter"),  # Split into Shrinkage_Type and Parameter (ka, V, Cl)
    names_pattern = "shrinkage_(.*)_(.*)",       # Extract the shrinkage type and parameter
    values_to = "Value"                          # Column for the values
  )


# Update the Base_Name for consistent capitalization
final_shrinkage_long$Base_Name <- final_shrinkage_long$Base_Name %>%
  dplyr::recode(
    `amelia` = "Amelia",
    `mice` = "Mice",
    `midas` = "MIDAS",
    `xgb` = "XGB",
    `final_data` = "Original Data"  # Keep the name for Original Data consistent
  )

# Ensure Base_Name has the desired factor order for consistent coloring and labeling
final_shrinkage_long$Base_Name <- factor(
  final_shrinkage_long$Base_Name,
  levels = c("Amelia", "MF", "Mice", "MIDAS", "XGB", "Original Data")
)

# Create separate plots for each Shrinkage_Type using barplots with conditional width adjustment
plot_list <- list()

for (shrinkage_type in unique(final_shrinkage_long$Shrinkage_Type)) {
  
  # Filter data for the current Shrinkage_Type
  plot_data <- final_shrinkage_long %>% filter(Shrinkage_Type == shrinkage_type)
  
  # Create the plot using barplots with conditional width
  p <- ggplot(plot_data, aes(x = Missing_Percentage, y = Value, fill = Base_Name)) +
    geom_col(position = position_dodge(width = 0.75), 
             color = "black", 
             aes(width = ifelse(Missing_Percentage == "0%", 0.14, 0.7))) +  # Adjust width for 0% bars
    facet_wrap(~ Parameter, scales = "free") +  # Facet by ka, V, Cl
    labs(title = paste("Shrinkage", shrinkage_type, "by Missing Percentage"),
         fill = "Imputation Method",
         x = "Missing Percentage",
         y = "Shrinkage Value") +
    scale_fill_jco() +  # Use jco color palette for consistent coloring
    my_theme  # Apply your custom theme
  
  # Store the plot in the list
  plot_list[[shrinkage_type]] <- p
}

# Arrange the 3 plots (mode, mean, condDist) in a single figure using ggarrange
combined_plot <- ggarrange(
  plot_list[["mode"]], plot_list[["mean"]], plot_list[["condDist"]],
  ncol = 1, nrow = 3,  # Arrange in one column and three rows
  labels = c("A", "B", "C"),  # Add labels to the plots
  common.legend = TRUE,       # Share a common legend across all plots
  legend = "right"            # Position the legend on the right
)







# Save the plot as a PNG file with specified dimensions and resolution
png("D:/MOBILITE/1er papier - Simulation Covariables/Figures/Shrinkage_custom_adjusted.png", width = 16, height = 9, units = 'in', res = 900)
print(combined_plot)
dev.off()

# Compute the delta (difference) for each criterion and replace 0 with NA
log_likelihood_long <- log_likelihood_long %>%
  group_by(Criteria) %>%
  mutate(Delta = Value - Value[Base_Name == "Original Data" & Missing_Percentage == "0%"],
         Delta = ifelse(Delta == 0, NA, Delta))  # Replace 0 with NA

# Define the custom color mapping for each Base_Name
jco_colors <- pal_jco("default")(6)  # Extract the first 6 jco colors
custom_colors <- c(
  "Amelia" = jco_colors[1],
  "MF" = jco_colors[2],
  "Mice" = jco_colors[3],
  "MIDAS" = jco_colors[4],
  "XGB" = jco_colors[5],
  "Original Data" = jco_colors[6]
)

# Create separate plots for OFV and BICc
plot_list <- list()

for (criteria in log_likelihood_criteria) {
  
  # Filter data for the current criteria
  plot_data <- log_likelihood_long %>% filter(Criteria == criteria)
  
  # Separate data for Missing_Percentage == "0%" and the rest
  plot_data_0 <- plot_data %>% filter(Missing_Percentage == "0%")
  plot_data_rest <- plot_data %>% filter(Missing_Percentage != "0%")
  
  # Get the reference value for this criteria
  final_value <- plot_data_0$Value[1]  # Reference value for 0%
  
  # Adjust Y-axis limit based on criteria
  ylim_max <- ifelse(criteria == "BICc", 1775, 1750)  # Increase BICc to 1775, OFV remains 1750
  
  # Create the plot with delta values displayed at the top of each bar
  p <- ggplot() +
    geom_col(data = plot_data_0, aes(x = Missing_Percentage, y = Value, fill = Base_Name), 
             position = position_dodge(width = 0.75), color = "black", width = 0.14) +  # Narrow bars for 0%
    geom_col(data = plot_data_rest, aes(x = Missing_Percentage, y = Value, fill = Base_Name), 
             position = position_dodge(width = 0.75), color = "black", width = 0.7) +  # Regular bars for the rest
    geom_text(
      data = plot_data, 
      aes(x = Missing_Percentage, y = Value, label = ifelse(is.na(Delta), "", sprintf("%.1f", Delta)), group = Base_Name), 
      position = position_dodge(width = 0.75), vjust = -0.5, size = 3  # Adjust position and size of text
    ) +
    geom_hline(yintercept = final_value, linetype = "dashed", color = "black", size = 1) +  # Add horizontal line
    coord_cartesian(ylim = c(1450, ylim_max)) +  # Adjust Y-axis limits
    labs(
      title = paste(criteria, "by Missing Percentage"),
      x = "Missing Percentage",
      y = criteria
    ) +
    scale_fill_manual(values = custom_colors, breaks = c("Amelia", "MF", "Mice", "MIDAS", "XGB", "Original Data")) +  # Set legend order manually
    my_theme +  # Apply your custom theme
    guides(fill = guide_legend(title = "Imputation Method"))  # Set legend title
  
  # Store the plot in the list
  plot_list[[criteria]] <- p
}

# Arrange the two plots (OFV and BICc) in a single figure using ggarrange
combined_log_likelihood_plot <- ggarrange(
  plot_list[["OFV"]], plot_list[["BICc"]],
  ncol = 1, nrow = 2,  # Arrange in one column and two rows
  labels = c("A", "B"),  # Add labels to the plots
  common.legend = TRUE,       # Share a common legend across all plots
  legend = "right"            # Position the legend on the right
)

# Save the plot as a PNG file with specified dimensions and resolution
png("D:/MOBILITE/1er papier - Simulation Covariables/Figures/LogLikelihood_OFV_BICc_Updated.png", 
    width = 16, height = 9, units = 'in', res = 900)
print(combined_log_likelihood_plot)
dev.off()















# Rename Base_Name for consistent capitalization
population_param_combined$Base_Name <- population_param_combined$Base_Name %>%
  dplyr::recode(
    `amelia` = "Amelia",
    `MF` = "MF",
    `mice` = "Mice",
    `midas` = "MIDAS",
    `xgb` = "XGB",
    `final_data` = "Original Data"
  )

# Ensure Base_Name has the desired factor order for consistent coloring and labeling
population_param_combined$Base_Name <- factor(
  population_param_combined$Base_Name,
  levels = c("Amelia", "MF", "Mice", "MIDAS", "XGB", "Original Data")
)

# Define the custom color mapping for each Base_Name
jco_colors <- pal_jco("default")(6)  # Extract the first 6 jco colors
custom_colors <- c(
  "Amelia" = jco_colors[1],
  "MF" = jco_colors[2],
  "Mice" = jco_colors[3],
  "MIDAS" = jco_colors[4],
  "XGB" = jco_colors[5],
  "Original Data" = jco_colors[6]
)

# Create plots for each population parameter
plot_list <- list()

# Loop through unique parameters to generate plots for each
for (param in unique(population_param_combined$parameter)) {
  
  # Filter data for the current parameter
  plot_data <- population_param_combined %>% filter(parameter == param)
  
  # Separate the data for Missing_Percentage = 0% and the rest
  plot_data_0 <- plot_data %>% filter(Missing_Percentage == "0%")
  plot_data_rest <- plot_data %>% filter(Missing_Percentage != "0%")
  
  # Get the final_data value for this parameter
  final_value <- plot_data %>% filter(Base_Name == "Original Data") %>% pull(Value) %>% mean(na.rm = TRUE)
  
  # Create the plot using barplots with regular and narrow bar widths and error bars for percentiles
  p <- ggplot() +
    geom_col(data = plot_data_0, aes(x = Missing_Percentage, y = Value, fill = Base_Name), 
             position = position_dodge(width = 0.75), color = "black", width = 0.14) +  # Narrow bars for 0%
    geom_col(data = plot_data_rest, aes(x = Missing_Percentage, y = Value, fill = Base_Name), 
             position = position_dodge(width = 0.75), color = "black", width = 0.7) +  # Regular bars for the rest
    geom_errorbar(data = plot_data, aes(x = Missing_Percentage, ymin = P2.5, ymax = P97.5, group = Base_Name), 
                  position = position_dodge(width = 0.75), width = 0.25, color = "black") +  # Error bars for percentiles
    geom_hline(yintercept = final_value, linetype = "dashed", color = "black", size = 1) +  # Add black horizontal line
    labs(title = param,  # Show the parameter name
         x = "Missing Percentage",
         y = "Value") +
    scale_fill_manual(values = custom_colors, breaks = c("Amelia", "MF", "Mice", "MIDAS", "XGB", "Original Data")) +  # Set legend order manually
    my_theme +  # Apply your custom theme
    guides(fill = guide_legend(title = "Imputation Method"))  # Set legend title
  
  # Store the plot in the list
  plot_list[[param]] <- p
}

# Arrange the plots for all parameters in a grid with 3 columns
combined_population_plot <- ggarrange(plotlist = plot_list, ncol = 3, nrow = ceiling(length(plot_list) / 3), 
                                      common.legend = TRUE, legend = "right")

# Save the plot as a PNG file with a higher height dimension for better spacing
png("D:/MOBILITE/1er papier - Simulation Covariables/Figures/Population_Parameters_Plot_with_errorbars_Updated.png", 
    width = 16, height = 9, units = 'in', res = 900)  # Increase height
print(combined_population_plot)
dev.off()






# Prepare list to store p-values from correlation files
correlation_data_list <- list()

# Loop through each folder in your data
for (subfolder in names(all_data)) {
  if ("correlation" %in% names(all_data[[subfolder]])) {
    correlation_df <- all_data[[subfolder]]$correlation
    
    # Extract Missing_Percentage from folder names
    missing_percentage <- ifelse(grepl("final_data", subfolder), "0%", 
                                 ifelse(grepl("_1$", subfolder), "5%",
                                        ifelse(grepl("_2$", subfolder), "20%",
                                               ifelse(grepl("_3$", subfolder), "50%", "75%"))))
    
    # Create a new dataframe with relevant columns
    correlation_values <- data.frame(
      Base_Name = sub("_[0-9]+$", "", subfolder),  # Extract base name before the number
      Missing_Percentage = missing_percentage,     # Use extracted Missing_Percentage
      covariate = correlation_df$covariate,        # Covariate column (instead of parameter)
      p_value = correlation_df$p.value             # P-value column
    )
    
    # Append to list
    correlation_data_list[[subfolder]] <- correlation_values
  }
}

# Combine all data into one dataframe
correlation_combined <- do.call(rbind, correlation_data_list)

# Reorder the Missing_Percentage to start with 0%
correlation_combined$Missing_Percentage <- factor(
  correlation_combined$Missing_Percentage, 
  levels = c("0%", "5%", "20%", "50%", "75%")
)


# Convert p_value to numeric and then filter
correlation_combined <- correlation_combined %>%
  mutate(p_value = as.numeric(p_value)) %>%  # Convert p_value column to numeric
  filter(!is.na(p_value) & p_value > 0.05)  # Remove rows with NA or p-values < 0.05

# View the resulting dataframe
head(correlation_combined)



# Reorder columns as: Covariate, Missing Percentage, Base Name, and P-value
correlation_combined <- correlation_combined %>%
  dplyr::select(covariate, Missing_Percentage, Base_Name, p_value) %>%
  dplyr::arrange(covariate, Missing_Percentage, Base_Name)

# View the resulting dataframe
correlation_combined





# Create the flextable from the reordered dataframe
flextable_table <- flextable(correlation_combined)

# Merge vertically where values are the same for covariate, Missing_Percentage, and Base_Name
flextable_table <- flextable_table %>%
  merge_v(j = "covariate") %>%  # Merge covariate column when values are the same
  merge_v(j = "Missing_Percentage") %>%  # Merge Missing_Percentage column when values are the same
  merge_v(j = "Base_Name") %>%  # Merge Base_Name column when values are the same
  theme_box() %>%  # Apply a simple box theme to the table
  set_header_labels(
    covariate = "Covariate",
    Missing_Percentage = "Missing Percentage",
    Base_Name = "Imputation Method",
    p_value = "P-Value"
  ) %>% 
  fontsize(size = 10, part = "all") %>%  # Set font size to 10 for all parts of the table
  bold(part = "header") %>%  # Make the headers bold
  align(align = "center", part = "all") %>%  # Center-align all text
  width(j = c("covariate"), width = 2.5) %>%  # Set custom column widths for better readability
  width(j = c("Missing_Percentage", "Base_Name", "p_value"), width = 1.5) %>%  # Adjust width of other columns
  border_inner_h(border = fp_border(color = "gray", width = 1)) %>%  # Set inner horizontal borders
  border_outer(border = fp_border(color = "black", width = 1.5)) %>%  # Set outer borders
  padding(padding = 5, part = "all") %>%  # Add some padding for better readability
  add_footer_lines(values = "This table summarizes the non-significant p-values for each covariate.") %>%  # Add a custom footer
  font(part = "all", fontname = "Times New Roman")  # Set font to Times New Roman for publication

# Display the flextable
flextable_table

# Export the flextable as a Word document with improved formatting
doc <- officer::read_docx()
doc <- flextable::body_add_flextable(doc, flextable_table)
print(doc, target = "Improved_Merged_Correlation_Table.docx")








# Define the directory where your mlxtran files are stored
mlxtran_directory <- "D:/MOBILITE/1er papier - Simulation Covariables/Monolix"



# Helper function to extract the project name and percentage of NA
extract_project_info <- function(project_name) {
  name_parts <- unlist(strsplit(project_name, "_"))
  project_name <- name_parts[1]
  if (length(name_parts) > 1) {
    na_percentage <- switch(name_parts[2],
                            "1" = "5% NA",
                            "2" = "20% NA",
                            "3" = "50% NA",
                            "4" = "75% NA",
                            "data" = "0% NA")
  } else {
    na_percentage <- "0% NA"
  }
  return(list(project_name = project_name, na_percentage = na_percentage))
}

# Get a list of all .mlxtran files in the directory
mlxtran_files <- list.files(path = mlxtran_directory, pattern = "*.mlxtran", full.names = TRUE)

# Define fixed x and y axis limits
x_limits <- c(0, 40)
y_limits <- c(0, 20)

# Initialize an empty list to store the plots
vpc_plots <- list()

# Loop through each .mlxtran file to create the plots
for (i in seq_along(mlxtran_files)) {
  
  # Load the Monolix project
  project_file <- mlxtran_files[i]
  loadProject(project_file)
  
  # Extract project name and percentage of NA
  project_info <- extract_project_info(tools::file_path_sans_ext(basename(project_file)))
  
  # Draw the VPC without stratification
  vpc_plot <- plotVpc(obsName = "DV",
                      settings = list(outlierDots = FALSE, useCorrpred = TRUE, grid = FALSE,
                                      ylab = "Concentration", empPercentiles=TRUE, predPercentiles=TRUE, empirical=TRUE, theoretical=TRUE, xlab = "Time (in hour)"))
  
  # Add consistent x and y axis limits, title, and custom theme to the plot
  vpc_plot <- vpc_plot +
    ggtitle(paste(project_info$project_name, "with", project_info$na_percentage)) +
    xlim(x_limits) +
    ylim(y_limits) +
    my_theme
  
  # Store the plot in the list
  vpc_plots[[i]] <- vpc_plot
}
# Modify the title of plot 5 to "Original 0% NA"
vpc_plots[[5]] <- vpc_plots[[5]] + ggtitle("Original with 0% NA")




plot_indices <- c(
  5, 1, 2, 3, 4, 5, 6, 7, 8, 9,
  5, 10, 11, 12, 13, 5, 14, 15,
  16, 17, 5, 18, 19, 20, 21
)

combined_plot <- ggarrange(
  plotlist = vpc_plots[plot_indices],
  ncol = 5,
  nrow = 5,
  common.legend = TRUE,
  legend = "bottom"
)


png("D:/MOBILITE/1er papier - Simulation Covariables/Figures/pcVPC.png", width = 12, height = 12, units = 'in', res = 900)
print(combined_plot)
dev.off()


directory_path <- "D:/MOBILITE/1er papier - Simulation Covariables/imputed_datasets_merged"
# Get full paths of all CSV files in the directory
csv_files <- list.files(path = directory_path, pattern = "\\.csv$", full.names = TRUE)
# Read all CSV files into a list
merged_imputed_data_list <- lapply(csv_files, read.csv)
# Extract file names without directory and extension
file_names <- basename(csv_files)
file_names <- tools::file_path_sans_ext(file_names)

# Assign names to the list elements
names(merged_imputed_data_list) <- file_names


mrgsolv_val <- merged_imputed_data_list$final_data %>%
  group_by(ID) %>%
  dplyr::slice(1) %>%
  select(ID, CL, VC, KA) %>%
  ungroup()

mlx_val <-   all_data$final_data$parameters %>%
group_by(id) %>%
  dplyr::slice(1) %>%
  select(id, Cl_mode, V_mode, ka_mode) %>%
  ungroup()

# Step 1: Extract and rename the required columns for each dataframe in the list
mlx_val_list <- map(all_data, ~ .x$parameters %>%
  group_by(id) %>%
  dplyr::slice(1) %>%
  select(ID = id, CL = Cl_mode, VC = V_mode, KA = ka_mode) %>%
  ungroup())

# Step 2: Combine all extracted data into a single dataframe
combined_mlx_val <- bind_rows(mlx_val_list, .id = "source")  # Adds a column source indicating the source dataframe

# Step 3: Update source to readable missing percentage levels and imputation methods
combined_mlx_val <- combined_mlx_val %>%
  mutate(
    Missing_Percentage = case_when(
      source == "final_data" ~ "0% NA",
      grepl("_1$", source) ~ "5% NA",
      grepl("_2$", source) ~ "20% NA",
      grepl("_3$", source) ~ "50% NA",
      grepl("_4$", source) ~ "75% NA"
    ),
    Imputation_Method = case_when(
      grepl("amelia", source) ~ "Amelia",
      grepl("mice", source) ~ "MICE",
      grepl("MF", source) ~ "MF",
      grepl("midas", source) ~ "MIDAS",
      grepl("xgb", source) ~ "XGB",
      source == "final_data" ~ "Final Data"
    )
  )

# Step 4: Duplicate "0% NA" data for each imputation method and remove the original "Final Data" rows
final_data_rows <- combined_mlx_val %>%
  filter(Missing_Percentage == "0% NA") %>%
  select(-Imputation_Method) %>%
  expand_grid(Imputation_Method = c("Amelia", "MF", "MIDAS", "MICE", "XGB"))

combined_mlx_val <- combined_mlx_val %>%
  filter(Imputation_Method != "Final Data") %>%
  bind_rows(final_data_rows)

combined_mlx_val <- combined_mlx_val %>%
  mutate(
    Imputation_Method = factor(Imputation_Method, levels = c("Amelia", "MF", "MIDAS", "MICE", "XGB")),
    Missing_Percentage = factor(Missing_Percentage, levels = c("0% NA", "5% NA", "20% NA", "50% NA", "75% NA"))
  )


plot_histogram_with_lognormal_and_density_fixed <- function(data, variable, variable_name) {

    
    # Set binwidth based on the variable
    binwidth_value <- if (variable == "CL") 0.2 else if (variable == "VC") 1 else 0.1
    
    # Compute per-group parameters for log-normal distribution
    subset_params <- data %>%
        filter(!!sym(variable) > 0) %>%
        group_by(Missing_Percentage, Imputation_Method) %>%
        summarize(
            meanlog = mean(log(!!sym(variable)), na.rm = TRUE),
            sdlog = sd(log(!!sym(variable)), na.rm = TRUE),
            count = n(),
            .groups = "keep"  # Retain grouping
        )
    
    # Create a grid of x-values
    x_grid <- seq(min(data[[variable]], na.rm = TRUE), max(data[[variable]], na.rm = TRUE), length.out = 100)
    
    # Compute log-normal density values per group
    lognormal_data <- subset_params %>%
        group_modify(~ {
            meanlog = .x$meanlog[1]
            sdlog = .x$sdlog[1]
            count = .x$count[1]
            # Handle cases where sdlog is NA or zero
            if (is.na(sdlog) || sdlog == 0 || count < 2) {
                return(data.frame(
                    x = numeric(0),
                    y = numeric(0)
                ))
            }
            data.frame(
                x = x_grid,
                y = dlnorm(x_grid, meanlog, sdlog) * count * binwidth_value
            )
        })
    
    # Compute density estimates per group
    density_data <- data %>%
        group_by(Missing_Percentage, Imputation_Method) %>%
        group_modify(~ {
            if(nrow(.x) < 2){
                return(data.frame(
                    x = numeric(0),
                    y = numeric(0)
                ))
            }
            dens <- density(.x[[variable]], na.rm = TRUE)
            data.frame(
                x = dens$x,
                y = dens$y * length(.x[[variable]]) * binwidth_value
            )
        })
    
    # Create the base plot
    plot <- ggplot(data, aes_string(x = variable)) +
        # Histogram
        geom_histogram(aes(y = ..count.., fill = Imputation_Method),
                       binwidth = binwidth_value, color = "black", alpha = 0.7, show.legend = TRUE) +
        # Log-normal curve per group
        geom_line(data = lognormal_data,
                  aes(x = x, y = y, color = "Log-normal Distribution", linetype = "Log-normal Distribution"),
                  size = 1, show.legend = TRUE) +
        # Density curve per group
        geom_line(data = density_data,
                  aes(x = x, y = y, color = "Density Estimate", linetype = "Density Estimate"),
                  size = 1, alpha = 0.5, show.legend = TRUE) +
        # Facet by Missing_Percentage and Imputation_Method
        facet_grid(Imputation_Method ~ Missing_Percentage, scales = "free_y") +
        # Add the color palette from ggsci
        scale_fill_jco() +
        # Add manual legends for Log-normal and Density Estimate
        scale_linetype_manual(name = "Legend", values = c("Density Estimate" = "solid", "Log-normal Distribution" = "solid")) +
        scale_color_manual(name = "Legend", values = c("Log-normal Distribution" = "blue", "Density Estimate" = "red")) +
        labs(title = paste("Distribution of", variable_name),
             x = variable_name,
             y = "Count") +
        # Custom theme with spacing and legend
        my_theme +
        theme(
            plot.title = element_text(hjust = 0.5, face = "bold"),
            axis.title = element_text(size = 16),
            axis.text = element_text(size = 14),
            strip.text = element_text(face = "bold"),
            panel.spacing = unit(1, "lines"),
            legend.position = "top",
            legend.key = element_rect(fill = "white", color = "black"),
            legend.title = element_text(face = "bold"),
            legend.text = element_text(size = 12),
            panel.border = element_rect(color = "grey", fill = NA, size = 1),
            strip.background = element_rect(fill = "grey", color = "grey", size = 1)
        ) +
        # Custom guides for the legend
        guides(
            fill = guide_legend(title = "Imputation Method"),
            linetype = guide_legend(override.aes = list(color = c("red", "blue"), fill = "white")),
            color = "none"
        )
    
    return(list(
        plot = plot,
        lognormal_data = lognormal_data,
        density_data = density_data
    ))
    
    return(plot)
}


# Example usage for CL
CL_Plot_fixed <- plot_histogram_with_lognormal_and_density_fixed(combined_mlx_val, "CL", "CL")
png("CL_Distribution.png", width = 16, height = 9, units = 'in', res = 900)
print(CL_Plot_fixed)
dev.off()



V_Plot_fixed <- plot_histogram_with_lognormal_and_density_fixed(combined_mlx_val, "VC", "VC")
png("V_Distribution.png", width = 16, height = 9, units = 'in', res = 900)
print(V_Plot_fixed)
dev.off()

KA_Plot_fixed <- plot_histogram_with_lognormal_and_density_fixed(combined_mlx_val, "KA", "KA")
png("KA_Distribution.png", width = 16, height = 9, units = 'in', res = 900)
print(KA_Plot_fixed)
dev.off








# Define the desired order of imputation methods
desired_method_order <- c("Amelia", "MF", "MICE", "MIDAS", "XGB", "Final Data")

# Step 1: Extract PK parameters from all datasets and label them

# Initialize a list to store the parameter data frames
param_dfs <- list()

# Process the final_data
final_data_params <- all_data$final_data$parameters %>%
  group_by(id) %>%
  summarise(
    CL = first(Cl_mode),
    VC = first(V_mode),
    KA = first(ka_mode)
  ) %>%
  ungroup() %>%
  rename(ID = id) %>%
  mutate(
    Missing_Percentage = factor("0% NA", levels = c("0% NA", "5% NA", "20% NA", "50% NA", "75% NA")),
    Imputation_Method = factor("Final Data", levels = desired_method_order)
  )

# Add to the list
param_dfs[["Final Data"]] <- final_data_params

# Get the names of the datasets (excluding 'final_data')
dataset_names <- setdiff(names(all_data), "final_data")

# Process each imputed dataset
for (dataset_name in dataset_names) {
  # Get the dataset
  dataset <- all_data[[dataset_name]]
  
  # Extract PK parameters
  params_df <- dataset$parameters %>%
    group_by(id) %>%
    summarise(
      CL = first(Cl_mode),
      VC = first(V_mode),
      KA = first(ka_mode)
    ) %>%
    ungroup() %>%
    rename(ID = id)
  
  # Determine the Imputation_Method
  if (grepl("^amelia", dataset_name, ignore.case = TRUE)) {
    method <- "Amelia"
  } else if (grepl("^MF", dataset_name, ignore.case = TRUE)) {
    method <- "MF"
  } else if (grepl("^mice", dataset_name, ignore.case = TRUE)) {
    method <- "MICE"
  } else if (grepl("^MIDAS", dataset_name, ignore.case = TRUE)) {
    method <- "MIDAS"
  } else if (grepl("^XGB", dataset_name, ignore.case = TRUE)) {
    method <- "XGB"
  } else {
    stop(paste("Unknown method in dataset name:", dataset_name))
  }
  
  # Extract the missing percentage from the dataset name
  # Assuming the dataset names end with '_1', '_2', '_3', '_4'
  if (grepl("_1$", dataset_name)) {
    percentage <- "5% NA"
  } else if (grepl("_2$", dataset_name)) {
    percentage <- "20% NA"
  } else if (grepl("_3$", dataset_name)) {
    percentage <- "50% NA"
  } else if (grepl("_4$", dataset_name)) {
    percentage <- "75% NA"
  } else {
    stop(paste("Could not determine Missing_Percentage for dataset:", dataset_name))
  }
  
  # Add the columns to params_df with the correct factor levels
  params_df <- params_df %>%
    mutate(
      Missing_Percentage = factor(percentage, levels = c("0% NA", "5% NA", "20% NA", "50% NA", "75% NA")),
      Imputation_Method = factor(method, levels = desired_method_order)
    )
  
  # Add to the list with the dataset_name as key
  param_dfs[[dataset_name]] <- params_df
}

# Step 2: Extract IDs with missing data for each missing percentage

# Initialize a list to store IDs with missing data
missing_ids_list <- list()

# Loop over each dataset in 'missing_data_list'
for (dataset_name in names(missing_data_list)) {
  # Extract the percentage from the dataset name
  # Assuming dataset names are like "missing_5perc", "missing_20perc", etc.
  percentage_match <- str_extract(dataset_name, "\\d+")
  
  if (!is.na(percentage_match)) {
    percentage_value <- as.integer(percentage_match)
    percentage <- paste0(percentage_value, "% NA")
  } else {
    stop(paste("Could not extract percentage from dataset name:", dataset_name))
  }
  
  # Get the dataset with missing data
  data_with_missing <- missing_data_list[[dataset_name]]
  
  # Identify IDs with any missing data
  ids_with_missing <- data_with_missing %>%
    filter(is.na(WT) | is.na(CrCl) | is.na(PPI) | is.na(UM)) %>%
    select(ID) %>%
    distinct()
  
  # Store the IDs in the list
  missing_ids_list[[percentage]] <- ids_with_missing$ID
}

# Step 3: Perform comparisons for IDs with missing data

# Define the comparison function
compare_params <- function(imputed_df, final_df, ids_with_missing) {
  # Filter both data frames to include only IDs with missing data
  imputed_df_filtered <- imputed_df %>% filter(ID %in% ids_with_missing)
  final_df_filtered <- final_df %>% filter(ID %in% ids_with_missing)
  
  # Ensure IDs are of the same type
  imputed_df_filtered$ID <- as.character(imputed_df_filtered$ID)
  final_df_filtered$ID <- as.character(final_df_filtered$ID)
  
  # Proceed with the comparison
  comparison_df <- imputed_df_filtered %>%
    inner_join(final_df_filtered, by = "ID", suffix = c("_imputed", "_original")) %>%
    mutate(
      CL_diff = CL_imputed - CL_original,
      VC_diff = VC_imputed - VC_original,
      KA_diff = KA_imputed - KA_original,
      CL_PE = (CL_imputed - CL_original) / CL_original * 100,
      VC_PE = (VC_imputed - VC_original) / VC_original * 100,
      KA_PE = (KA_imputed - KA_original) / KA_original * 100
    )
  return(comparison_df)
}

# Initialize a list to store comparison results
comparison_results <- list()

# Loop over the imputed datasets (excluding 'Final Data')
for (name in names(param_dfs)) {
  if (name != "Final Data") {
    imputed_df <- param_dfs[[name]]
    final_df <- param_dfs[["Final Data"]]
    
    # Get the Imputation_Method and Missing_Percentage
    method <- as.character(unique(imputed_df$Imputation_Method))
    percentage <- as.character(unique(imputed_df$Missing_Percentage))
    
    # Get the IDs with missing data for this percentage
    ids_with_missing <- missing_ids_list[[percentage]]
    
    # Check if ids_with_missing is NULL or empty
    if (is.null(ids_with_missing)) {
      message(paste("No IDs with missing data found for", percentage))
      next  # Skip to the next iteration
    } else if (length(ids_with_missing) == 0) {
      message(paste("IDs with missing data list is empty for", percentage))
      next  # Skip to the next iteration
    }
    
    # Ensure IDs are of the same type
    ids_with_missing <- as.character(ids_with_missing)
    imputed_df$ID <- as.character(imputed_df$ID)
    final_df$ID <- as.character(final_df$ID)
    
    # Perform the comparison using the filtered IDs
    comparison_df <- compare_params(imputed_df, final_df, ids_with_missing)
    
    # Add metadata
    comparison_df <- comparison_df %>%
      mutate(
        Imputation_Method = factor(method, levels = desired_method_order),
        Missing_Percentage = factor(percentage, levels = c("5% NA", "20% NA", "50% NA", "75% NA"))
      )
    
    # Store in the list
    comparison_results[[name]] <- comparison_df
  }
}

# Combine all comparison results into one data frame
pk_param_comparison_combined <- bind_rows(comparison_results)

# Ensure Imputation_Method and Missing_Percentage are factors with desired levels
pk_param_comparison_combined <- pk_param_comparison_combined %>%
  mutate(
    Imputation_Method = factor(Imputation_Method, levels = desired_method_order),
    Missing_Percentage = factor(Missing_Percentage, levels = c("5% NA", "20% NA", "50% NA", "75% NA"))
  )

# Step 4: Reshape the data for plotting

# Reshape the data to have both Absolute Error and Percentage Error
pk_params_long <- pk_param_comparison_combined %>%
  pivot_longer(
    cols = c(CL_diff, VC_diff, KA_diff, CL_PE, VC_PE, KA_PE),
    names_to = c("Parameter", "ErrorType"),
    names_pattern = "([A-Z]{2})_(.*)",
    values_to = "ErrorValue"
  ) %>%
  mutate(
    ErrorType = case_when(
      ErrorType == "diff" ~ "Absolute Error",
      ErrorType == "PE" ~ "Percentage Error"
    ),
    Parameter = factor(Parameter, levels = c("CL", "VC", "KA")),
    ErrorType = factor(ErrorType, levels = c("Absolute Error", "Percentage Error")),
    Imputation_Method = factor(Imputation_Method, levels = desired_method_order)  # Ensure levels are set
  )

# Step 5: Create and save plots for each parameter


# Function to create plot for a given parameter
create_parameter_plot <- function(param_name) {
  # Filter the data for the specific parameter
  data_param <- pk_params_long %>% filter(Parameter == param_name)
  
  # Create the plot
  plot_param <- ggplot(data_param, aes(x = Missing_Percentage, y = ErrorValue, fill = Imputation_Method)) +
    geom_boxplot(alpha = 0.7, outlier.shape = NA, color = "black", linewidth = 1.2) +
    geom_point(
      color = "black",
      position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.75),
      alpha = 1,
      size = 0.6,
      show.legend = FALSE
    ) +
    facet_grid2(ErrorType ~ ., scales = "free", independent = "y") +  # Facet only by ErrorType
    labs(
      title = paste("Comparison of Absolute and Percentage Errors for", param_name),
      x = "Percentage of Missing Data",
      y = "Error Value",
      fill = "Imputation Method"
    ) +
    scale_color_jco() +
    scale_fill_jco() +
    my_theme +
    theme(
      strip.background = element_rect(fill = "grey90", color = "grey50"),
      strip.text = element_text(face = "bold", color = "black"),
      plot.title = element_text(hjust = 0.5),
      axis.text.x = element_text(angle = 45, hjust = 1),
      panel.spacing.y = unit(1, "cm"),
      panel.border = element_rect(
        colour = "black",
        fill = NA,
        size = 0.5
      )
    )
  
  return(plot_param)
}

# Get the unique parameters
parameters <- unique(pk_params_long$Parameter)

# Loop over each parameter to generate and save the plots
for (param in parameters) {
  # Generate the plot
  plot_param <- create_parameter_plot(param)
  
  # Display the plot (optional)
  print(plot_param)
  
  # Save the plot to a PNG file
  file_name <- paste0("PK_Error_Comparison_", param, ".png")
  png(file_name, width = 16, height =9, units = 'in', res = 900)
  print(plot_param)
  dev.off()
}











# Define the desired order of imputation methods
imputation_methods <- c("Amelia", "MF", "MICE", "MIDAS", "XGB")

# Define the missing data percentages
missing_percentages <- c("0% NA", "5% NA", "20% NA", "50% NA", "75% NA")

# Initialize a list to store the data frames
predictions_list <- list()

# Process the final_data
final_predictions <- all_data[["final_data"]][["predictions"]]

# Check if 'final_predictions' exists and has the required columns
if (!is.null(final_predictions) && all(c("DV", "popPred", "indivPred_mode") %in% names(final_predictions))) {
  
  # Assign Missing_Percentage as "0% NA"
  final_predictions <- final_predictions %>%
    mutate(
      Missing_Percentage = factor("0% NA", levels = missing_percentages)
    )
  
  # For each imputation method, create a copy of final_predictions
  for (method in imputation_methods) {
    method_predictions <- final_predictions %>%
      mutate(
        Imputation_Method = factor(method, levels = imputation_methods)
      )
    
    # Store in the list with a key combining method and percentage
    key <- paste(method, "0% NA", sep = "_")
    predictions_list[[key]] <- method_predictions
  }
}

# Get the names of the datasets (excluding 'final_data')
dataset_names <- setdiff(names(all_data), "final_data")

# Process each imputed dataset
for (dataset_name in dataset_names) {
  
  # Extract the dataset
  dataset <- all_data[[dataset_name]]
  
  # Extract the 'predictions' data frame
  predictions_df <- dataset[["predictions"]]
  
  # Check if 'predictions' data frame exists and has the required columns
  if (!is.null(predictions_df) && all(c("DV", "popPred", "indivPred_mode") %in% names(predictions_df))) {
    
    # Determine the Imputation_Method based on dataset name
    if (grepl("^amelia", dataset_name, ignore.case = TRUE)) {
      method <- "Amelia"
    } else if (grepl("^MF", dataset_name, ignore.case = TRUE)) {
      method <- "MF"
    } else if (grepl("^mice", dataset_name, ignore.case = TRUE)) {
      method <- "MICE"
    } else if (grepl("^MIDAS", dataset_name, ignore.case = TRUE)) {
      method <- "MIDAS"
    } else if (grepl("^XGB", dataset_name, ignore.case = TRUE)) {
      method <- "XGB"
    } else {
      stop(paste("Unknown method in dataset name:", dataset_name))
    }
    
    # Extract the missing percentage from the dataset name
    if (grepl("_1$", dataset_name)) {
      percentage <- "5% NA"
    } else if (grepl("_2$", dataset_name)) {
      percentage <- "20% NA"
    } else if (grepl("_3$", dataset_name)) {
      percentage <- "50% NA"
    } else if (grepl("_4$", dataset_name)) {
      percentage <- "75% NA"
    } else {
      stop(paste("Could not determine Missing_Percentage for dataset:", dataset_name))
    }
    
    # Assign Missing_Percentage and Imputation_Method
    predictions_df <- predictions_df %>%
      mutate(
        Missing_Percentage = factor(percentage, levels = missing_percentages),
        Imputation_Method = factor(method, levels = imputation_methods)
      )
    
    # Store in the list with a key combining method and missing percentage
    key <- paste(method, percentage, sep = "_")
    predictions_list[[key]] <- predictions_df
  } else {
    warning(paste("Dataset", dataset_name, "does not have the required columns for plotting."))
  }
}

# Combine all predictions into one data frame
all_predictions <- bind_rows(predictions_list, .id = "Dataset")

# Reshape the data to long format for plotting
all_predictions_long <- all_predictions %>%
  select(id, time, DV, popPred, indivPred_mode, Imputation_Method, Missing_Percentage) %>%
  pivot_longer(
    cols = c(popPred, indivPred_mode),
    names_to = "Prediction_Type",
    values_to = "Predicted_Value"
  ) %>%
  mutate(
    Prediction_Type = factor(
      Prediction_Type,
      levels = c("popPred", "indivPred_mode"),
      labels = c("Population Predictions", "Individual Predictions")
    ),
    Imputation_Method = factor(Imputation_Method, levels = imputation_methods),
    Missing_Percentage = factor(Missing_Percentage, levels = missing_percentages)
  )

# Split data into Population and Individual Predictions
population_data <- all_predictions_long %>%
  filter(Prediction_Type == "Population Predictions")

individual_data <- all_predictions_long %>%
  filter(Prediction_Type == "Individual Predictions")


# Create Population Predictions plot
population_plot <- ggplot(population_data, aes(x = Predicted_Value, y = DV)) +
  geom_point(alpha = 0.6, size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  facet_grid(Imputation_Method ~ Missing_Percentage, scales = "free") +
  labs(
    title = "Observations vs. Population Predictions",
    x = "Predicted Values",
    y = "Observed Values (DV)"
  ) +
  my_theme + 
    theme(
      strip.background = element_rect(fill = "grey90", color = "grey50"),
      strip.text = element_text(face = "bold", color = "black"),
      plot.title = element_text(hjust = 0.5),
      axis.text.x = element_text(angle = 45, hjust = 1),
      panel.spacing.y = unit(1, "cm"),
      panel.border = element_rect(
        colour = "black",
        fill = NA,
        size = 0.5
      ))

# Create Individual Predictions plot
individual_plot <- ggplot(individual_data, aes(x = Predicted_Value, y = DV)) +
  geom_point(alpha = 0.6, size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  facet_grid(Imputation_Method ~ Missing_Percentage, scales = "free") +
  labs(
    title = "Observations vs. Individual Predictions",
    x = "Predicted Values",
    y = "Observed Values (DV)"
  ) +
  my_theme + 
    theme(
      strip.background = element_rect(fill = "grey90", color = "grey50"),
      strip.text = element_text(face = "bold", color = "black"),
      plot.title = element_text(hjust = 0.5),
      axis.text.x = element_text(angle = 45, hjust = 1),
      panel.spacing.y = unit(1, "cm"),
      panel.border = element_rect(
        colour = "black",
        fill = NA,
        size = 0.5
      ))

# Arrange the two plots side by side using ggarrange
combined_plot <- ggarrange(
  population_plot,
  individual_plot,
  ncol = 2,
  labels = c("A", "B"),
  align = "hv"
)

# Add an overall title (optional)
combined_plot <- annotate_figure(
  combined_plot,
  top = text_grob("Observations vs. Predictions Across Imputation Methods and Missing Data Percentages", 
                 face = "bold", size = 18)
)


# Save the combined plot as a PNG
png("D:/MOBILITE/1er papier - Simulation Covariables/Figures/ObsPred.png", width = 18, height = 12, units = 'in', res = 900)
print(combined_plot)
dev.off()


# Compute statistics for population_data
population_stats <- population_data %>%
  group_by(Imputation_Method, Missing_Percentage) %>%
  summarise(
    MAE = mean(abs(DV - Predicted_Value), na.rm = TRUE),
    MPE = mean(((DV - Predicted_Value) / DV) * 100, na.rm = TRUE),
    RMSE = sqrt(mean((DV - Predicted_Value)^2, na.rm = TRUE)),
    RMSE_percent = (RMSE / mean(DV, na.rm = TRUE)) * 100,
    R_squared = cor(DV, Predicted_Value, use = "complete.obs")^2
  ) %>%
  ungroup()


# Compute statistics for individual_data
individual_stats <- individual_data %>%
  group_by(Imputation_Method, Missing_Percentage) %>%
  summarise(
    MAE = mean(abs(DV - Predicted_Value), na.rm = TRUE),
    MPE = mean(((DV - Predicted_Value) / DV) * 100, na.rm = TRUE),
    RMSE = sqrt(mean((DV - Predicted_Value)^2, na.rm = TRUE)),
    RMSE_percent = (RMSE / mean(DV, na.rm = TRUE)) * 100,
    R_squared = cor(DV, Predicted_Value, use = "complete.obs")^2
  ) %>%
  ungroup()



# Round the statistics for population_data
population_stats <- population_stats %>%
  mutate(
    MAE = round(MAE, 4),
    MPE = round(MPE, 2),
    RMSE = round(RMSE, 4),
    RMSE_percent = round(RMSE_percent, 2),
    R_squared = round(R_squared, 4)
  )

# Round the statistics for individual_data
individual_stats <- individual_stats %>%
  mutate(
    MAE = round(MAE, 4),
    MPE = round(MPE, 2),
    RMSE = round(RMSE, 4),
    RMSE_percent = round(RMSE_percent, 2),
    R_squared = round(R_squared, 4)
  )

# Add Prediction_Type to each data frame
population_stats <- population_stats %>%
  mutate(Prediction_Type = "Population")

individual_stats <- individual_stats %>%
  mutate(Prediction_Type = "Individual")

# Combine the statistics
combined_stats <- bind_rows(population_stats, individual_stats)

# Arrange columns in desired order
combined_stats <- combined_stats %>%
  select(Imputation_Method, Missing_Percentage, Prediction_Type,
         MAE, MPE, RMSE, RMSE_percent, R_squared)

# View combined statistics
print(combined_stats)


flextable_table <- flextable(combined_stats)

# Merge vertically where values are the same for Prediction_Type, Missing_Percentage, and Imputation_Method
flextable_table <- flextable_table %>%
  merge_v(j = c("Missing_Percentage", "Imputation_Method"))
  merge_h(j = c("Prediction_Type"))

# Apply a simple box theme to the table
flextable_table <- flextable_table %>%
  theme_box()

# Set custom header labels for better readability
flextable_table <- flextable_table %>%
  set_header_labels(
    Imputation_Method = "Imputation Method",
    Missing_Percentage = "Missing Percentage",
    Prediction_Type = "Prediction Type",
    MAE = "MAE",
    MPE = "MPE",
    RMSE = "RMSE",
    RMSE_percent = "RMSE%",
    R_squared = "R²"
  )

# Set font size to 10 and font to Times New Roman for all parts of the table
flextable_table <- flextable_table %>%
  fontsize(size = 10, part = "all") %>%
  font(part = "all", fontname = "Times New Roman")

# Make the headers bold
flextable_table <- flextable_table %>%
  bold(part = "header")

# Center-align all text in the table
flextable_table <- flextable_table %>%
  align(align = "center", part = "all")

# Set custom column widths for better readability
flextable_table <- flextable_table %>%
  width(j = c("Prediction_Type"), width = 2) %>%
  width(j = c("Imputation_Method", "Missing_Percentage"), width = 1.5) %>%
  width(j = c("MAE", "MPE", "RMSE", "RMSE_percent", "R_squared"), width = 1)

# Set inner horizontal borders
flextable_table <- flextable_table %>%
  border_inner_h(border = fp_border(color = "gray", width = 1))

# Set outer borders
flextable_table <- flextable_table %>%
  border_outer(border = fp_border(color = "black", width = 1.5))

# Add padding for better readability
flextable_table <- flextable_table %>%
  padding(padding = 5, part = "all")

# Add a custom footer line
flextable_table <- flextable_table %>%
  add_footer_lines(values = "This table summarizes the statistical metrics for each combination of Prediction Type, Imputation Method, and Missing Percentage.")

# Create a new Word document
doc <- read_docx()

# Add the flextable to the Word document
doc <- body_add_flextable(doc, value = flextable_table)

# Save the Word document to a file
print(doc, target = "Statistical_Metrics_Table.docx")
